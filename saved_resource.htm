 <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- saved from url=(0022)https://wlouyang.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Wanli Ouyang - Homepage</title>

<!--- <link rel="shortcut icon" type="image/ico" href="http://www.interdigital.com/idsystem/favicon.ico"><link rel="stylesheet" type="text/css" href="style.css" media="screen"> -->
 <script type="text/javascript" src="jsript.js"></script>  

<style type="text/css">@import url( style.css );
DIV.b-mobile {
	DISPLAY: none
}
</style>

<meta name="GENERATOR" content="MSHTML 8.00.6001.18852">
</head>

<body style="VERTICAL-ALIGN: middle">
<table border="0" cellspacing="0" cellpadding="0" width="100%">
  <tbody>
  <tr>
    <td><a name="Top"></a>
      <center>
      <!--================================Begin of Header of Windows Body====================================-->
      <table style="BACKGROUND-IMAGE: url(wave.jpg); BACKGROUND-REPEAT: no-repeat;BACKGROUND-POSITION:center" border="0" cellspacing="0" cellpadding="0" width="100%">
        <tbody>
        <tr>
          <td>
            <center>
            <table style="TEXT-ALIGN: left; WIDTH: 900px" border="0" cellspacing="0" cellpadding="0">
              <tbody>
              <tr>
                <td class="item"><a href="#Top">Home</a></td>
                <td class="item" style="WIDTH: 40px"></td>
                <td class="item"><a href="#Biography">Biography</a></td>
                <td class="item" style="WIDTH: 40px"></td>
                <td class="item"><a href="#Education">Education</a></td>
                <td class="item" style="WIDTH: 40px"></td>
                <td class="item"><a href="#Research">Research</a></td>
                <td class="item" style="WIDTH: 40px"></td>
<!--                <td class="item"><a href="http://www.h265.net/" target="_blank">Blog</a></td>  -->
                <td style="WIDTH: 540px"></td></tr>
             <tr><td style="HEIGHT: 30px;WIDTH: 900px" colspan="12"></td></tr></tbody></table></center>
          </td></tr></tbody></table>
<!--================================End of Header of Windows Body====================================-->

<!--================================Begin of Body of Windows Body====================================-->
            <table id="main" border="0" cellspacing="0" cellpadding="0" width="900">
              <tbody>
              <tr>
                <td style="BACKGROUND-COLOR: #fcfcfc; WIDTH: 100%; OVERFLOW: hidden" valign="top">
                
                  <div id="divContentHome" class="content_div_block">
                  <table class="garde_table_photo" align="center" border="0">
                    <tbody>
                    <tr>
                      <td style="width:412px;height:200px;background-color: #FFFFFF; font-family:garamond; font-size:13px; padding: 10px 20px 10px 20px ; border: 1px solid #D2D2D2;">
                      	<table>
							<tbody><tr>
								<td colspan="2" style="width:165px; height:44px;"><a href="https://sydney.edu.au" target="_blank"><img src="Resources/Icons/usyd-icon.png" height="120px"></a></td>								
<!--								<td colspan="2" style="width:412px; height:44px;"><img src="cu-hk-university.jpg" alt="CUHK" width="308px" height="70px"></td> -->
							</tr>
							<tr>
								<td colspan="2" style="width:412px; height:20px;"></td>
							</tr>
							<tr>
								<td colspan="2" style="width:412px; height:22px;"><b style="font-family:garamond; font-size: 19px;font-weight: bold;">Wanli Ouyang, Ph.D, IEEE Senior Member.</b></td>
							</tr>
							<tr>
								<td colspan="2" style="width:412px; height:22px;"> <b style="font-family:garamond; font-size: 15px;">Senior Lecturer at the University of Sydney</b></td>
							</tr>
							<tr>
								<td colspan="2" style="width:412px; height:22px;"> <b style="font-family:garamond; font-size: 15px;">I'm with <a class="aLink" href="http://mmlab.ie.cuhk.edu.hk/" target="_blank"><i> MMlab </i></a> and <a class="aLink" href="https://sigmalab-usyd.github.io/" target="_blank"><i> SIGMA lab</i></a> </b></td>
							</tr>
						</tbody></table>
						<table cellspacing="0" cellpadding="0" border="0" width="100%">
							<tbody><tr>
								<td bgcolor="#0099FF"><img src="transparent.gif" alt="" width="1" height="1" border="0"></td>
							</tr>
						</tbody></table>
						<table>
							<tbody><tr>
								<td style="width:210px; height:44px; line-height:125%; ">
								<a class="aLink" href="https://sigmalab-usyd.github.io/" target="_blank"> SIGMA lab</a>, <a class="aLink" href="https://sydney.edu.au/engineering/about/school-of-electrical-and-information-engineering.html" target="_blank"> School of Electrical and Information Engineering, </a> <br>
								<a class="aLink" href="https://sydney.edu.au/" target="_blank"> The University of Sydney, </a> <br>
								Sydney, Australia<br>
								</td>
								<td style="width:182px; height:44px; line-height:125%; text-align:left;">
								<br>
								<br>
								<img src="mainpa1.gif" alt="CUHK" width="80px" height="12px">sydney.edu.au<br>
								</td>
							</tr>
						</tbody></table>
<!--						<table>
							<tbody><tr>
								<td colspan="2" style=" line-height:50%; text-align:center;VERTICAL-ALIGN:middle; width:356px; height:10px;">www.interdigital.com</td>
							</tr> 
						</tbody>
						</table>-->
                      </td>
                      <td width="150px"></td>
                      <td style="VERTICAL-ALIGN: middle; width:120px"><img alt="Photo" src="Wanli-New.jpg" width="180px" height="200px"></td>
                      <td width="167px"></td>
                     </tr>
                    </tbody>
                  </table>
				  </div>
                  <div id="divContentBiography" class="content_div_block"><a name="Biography"></a>
                  <h1>Biography</h1>
                  <p class="garde_p">
                  	Wanli Ouyang obtained Ph.D from <a class="aLink" href="http://www.ee.cuhk.edu.hk/" target="_blank">  the Dept. of Electronic Engineering </a>, 
                  	the Chinese University of Hong Kong. 
                  	He is now a Senior Lecturer (equivalent to associate professor in US university systems) at the University of Sydney.
                  	His research interests include deep learning and its application to computer vision and pattern recognition, image and video processing.  <br><br> 

<!-- Dr. Dong received the B.Eng. and M.Eng. degrees, both in Information Engineering from 
                  <a class="aLink" href="http://www.zju.edu.cn/english/" target="_blank">Zhejiang University</a>, Hangzhou, China, 
                  in 2002 and 2005, respectively, and received the Ph.D. degree in Electronic Engineering in 2009, from 
                  <a class="aLink" href="http://www.cuhk.edu.hk/english/" target="_blank">the Chinese University of Hong Kong</a>, 
                  Hong Kong, China, where she worked as a Postdoctoral Fellow in the following year. In 2011,
                  she joined the CTO Office of 
                  <a class="aLink" href="http://www.interdigital.com/" target="_blank">InterDigital Communications</a>, U.S.A.,
                  as Staff Engineer. From 2003 to 2009, she was an active participant in Chinese standardization for 
                  multimedia with successful submissions to 
                  <a class="aLink" href="http://www.avs.org.cn/english/" target="_blank">AVS workgroup</a>. She has been engaged in 
                  HEVC standardization effort since 2011. 
                  Her research interests include high efficiency video coding and real-time video processing.<br><br> -->
				  <a target="_blank" class="aLink" href="./CV_WanliOuyang_CUHK.pdf"><img src="Resources/Icons/pdf-icon.png" width="15px" height="15px" alt="CV PDF" border="0"> Download Wanli Ouyang's Full CV</a>
				  &nbsp;&nbsp;&nbsp;&nbsp;
				  <a target="_blank" class="aLink" href="http://hk.linkedin.com/pub/wanli-ouyang/24/61b/293">
				  <img src="Resources/Icons/linkedin-icon.png" width="15px" height="15px" alt="View Wani Ouyang&#39;s LinkedIn Profile" border="0">View Wanli Ouyang's LinkedIn Profile</a> 
				  &nbsp;&nbsp;&nbsp;&nbsp;
				  <a target="_blank" class="aLink" href="http://scholar.google.com/citations?user=pw_0Z_UAAAAJ&amp hl=en">
				  <img src="Resources/Icons/google-icon.png" width="15px" height="15px" alt="View Wani Ouyang&#39;s Google Scholar Citations" border="0"> View Wanli Ouyang's Google Scholar Citations</a><br><br>
                  <a class="aLink" href="./#Top">Back To Top</a> <br><br> </p>
</div>
                  <h1>Information for potential Postdoctoral Fellow, Master and Ph.D. students and Final Year Program students </h1>                   
                  I moved  <a href="http://sydney.edu.au/engineering/electrical/"> the School of Electrical and Information Engineering, University of Sydney </a> as senior lecturer on 2017. 
                  If you are interested in my research topic and this university, please feel free to contact me <strong> after reading the information available <a href="https://sigmalab-usyd.github.io/recruitment/"> here</a> </strong>.
                  <h1>News </h1>                   
									International Journal of Computer Vision (IJCV) <strong><a href="http://www.ee.oulu.fi/~lili/IJCVSIEVR2018.htm">Special Issue on Efficient Visual Recognition </a></strong>. Due date for submission of full papers: February 15, 2019.
									<br>	<br>
									
                  <h1>Talks </h1>
<a href="./talk/Wanli_AutoML.pdf"> My recent talk on 'From Manual Design to Automatic Deep Learning'</a>
<br> <br>
<a href="./talk/Tutorial_2019_China_PRCV_out.pdf"> My recent tutorial on 'Deep learning in object detection' at PRCV 2019 </a>
<br> <br>
<a href="./talk/Talk2019_China_Jan_4_out.pdf">  My recent talk on ‘Structured deep learning for visual localization and recognition’  </a>
<br> <br>

<a href="./talk/ACCV18_3D_scene_understanding.pdf">  My talk ‘Modeling deep structures for 3D scene understanding’ at ACCV 2018 workshop </a>
									<br>	<br>

<a href="./talk/ACCV18_High_Performance.pdf">  My talk ‘Modeling deep structures for using high performance images’ at ACCV 2018 workshop </a>
<br> <br>




<!----																	<a href="./projects/GBD/index.html">Our team rank as #1 for object detection with provided data and external data and #1 for video object detection/tracking in the ImageNet Large Scale Visual Recognition Challenge 2016. Project page with source code </a> <br /> -->

                  <h1>Good resources on Paper Writing </h1>         
                  <a href="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649439633&idx=1&sn=f9a6b894e4266a6e5ec4b6ec673255a2&chksm=82c0d415b5b75d03faa511eb691630d2e5d3ed2fbe3598629ab20a037b8d95bfa5c795ad1c66&mpshare=1&scene=1&srcid=1205JJGMMw3BmvYdPXJBrNyq##"> How to do good research on computer vision (Chinese) </a> <br />                            
                  <a href="http://ec.europa.eu/eurostat/documents/64157/4374310/33-UNECE-making-data-meaningful-Part2-EN.pdf/d5b954e2-b110-469b-a2b5-78aa7c12ab62"> Making Data meaningful </a> <br />
                  <a href="https://www.dropbox.com/s/wgrdpmxcmb4nhl0/How%20to%20Get%20Your%20CVPR%20Paper%20Rejected.pptx?dl=0"> Slides on "How to get your paper rejected." By Prof. Ming-Hsuan Yang from UC Merced </a> <br />
                  <a href="http://blog.csdn.net/daiyuchao/article/details/6419543"> Chinese blog on how to publish a top journal </a> <br />

                  <h1>Good advices for Research Students</h1>         
                  <a href="https://mp.weixin.qq.com/s?__biz=MzI0NTY5ODMzMQ==&mid=2247485969&idx=1&sn=0766ff7f65828486725c722c88101a07&chksm=e94bd065de3c59735e2e3876c74e646d2c87e6acdf17157cddcc7fca02afe6261fcbf88b75db&mpshare=1&scene=1&srcid=0207ALoWRslZQnzDDYaKeUNw#rd"> 研究生导师：这种学生，才是我眼中的科研好苗子！(Advice in Chinese)</a> <br />


<!--                   <a href="./projects/ImageNet/index.html">Our team rank as #1 for video object detection and #2 for still image object detection in the ImageNet Large Scale Visual Recognition Challenge 2015. Project page</a> <br />
                  <a href="./projects/ImageNet/index.html">Our team rank as #2 in the ImageNet Large Scale Visual Recognition Challenge 2014. Project page</a> <br /> -->


<!--========================  Recommendation on Papers  ==========================--> 
<div id="divContentResearch" class="content_div_block"><a name="Research"></a>
                  <h1>Chef's Recommendation on Papers </h1>

                  <em> Our recent survey on object detection: </em>   <br>
                  
                  Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang, <strong>Wanli Ouyang</strong>, “EcoNAS: Finding Proxies for Economical Neural Architecture Search”, Proc. <em>CVPR</em>, 2020.
                        <br>  <br>


									Liu, Li, <strong>Wanli Ouyang</strong>, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietikäinen, "Deep learning for generic object detection: A survey,"  <em>IJCV, accepted</em>, 2019.
												[<a class="aLink" href="https://arxiv.org/pdf/1809.02165" target="_blank">Full Text</a>]
                        <br>  <br>

								
									<em> A new back-bone deep model design (performs better than ResNet and DenseNet): </em>   <br>
									Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, <strong>Wanli Ouyang</strong>, "FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction,"  <em>NuerIPS. (Previously called NIPS)</em>, 2018.
												[<a class="aLink" href="http://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" href="https://github.com/kevin-ssy/FishNet" target="_blank"> Source code </a>]
                        <br>  <br>

									<em> The first end-to-end deep video compression model: </em> <br>
									Guo Lu, <strong>Wanli Ouyang</strong>, Dong Xu, Xiaoyun Zhang, Chunlei Cai, Zhiyong Gao, "DVC: An End-to-end Deep Video Compression Framework," In Proc. CVPR 2019. 												[<a class="aLink" href="https://arxiv.org/pdf/1812.00101" target="_blank">Full Text</a>]  [<a class="aLink" href="https://github.com/GuoLusjtu/DVC" target="_blank">Source code</a>]
                        <br>  <br>

									<em> Details on our wining entry in ImageNet 2016 challenge on object detection: </em>   <br>
                      	Xingyu Zeng (equal contribution), <strong>Wanli Ouyang</strong> (equal contribution), Junjie Yan, Hongsheng Li, Tong Xiao, Kun Wang, Yu Liu, Yucong Zhou, Bin
Yang, Zhe Wang, Hui Zhou, Xiaogang Wang,
                      "Crafting GBD-Net for Object Detection," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, accepted, 2017.
												[<a class="aLink" href="./Papers/Zeng2017_GBD_PAMI.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" href="./projects/GBD/index.html" target="_blank">Project page & code </a>]												
												[<a class="aLink" href="https://github.com/craftGBD/craftGBD" target="_blank"> Source code </a>]
                        <br>  <br>


<em> The first work modeling deformation in deep CNN, used for pedestrian detection: </em>   <br>
                      	<strong>Wanli Ouyang</strong>, Hui Zhou, Hongsheng Li, Quanquan Li, Junjie Yan, Xiaogang Wang,
                      "Jointly learning deep features, deformable parts, occlusion and classification for pedestrian detection," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, 40(8):1874-1887, 2018.
												[<a class="aLink" href="./Papers/Ouyang2017JoingCNNPed.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" href="https://github.com/xiaohuige1/udn_extend" target="_blank">Source code</a>]                        <br>  <br>

<em> Extend our work on modeling deformation for generic object detection. This new deformation handling layer can be placed anywhere. </em>   <br>
									<strong>Wanli Ouyang</strong>, Xingyu Zeng,  Xiaogang Wang, et al,
                      "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, accepted, 2016.
												[<a class="aLink" href="./Papers/DeepID-Net.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Project" href="./projects/ImageNet/index.html" target="_blank">Project</a>]																								<br>  <br>
                          
												
									<em> The first cascade network for generic object detection. </em>   <br>
                      	<strong>Wanli Ouyang</strong>, Kun Wang, Xin Zhu, Xiaogang Wang. "Chained Cascade Network
for Object Detection", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ouyang_Chained_Cascade_Network_ICCV_2017_paper.pdf" target="_blank">Full Text</a>]  
[<a class="aLink" title="Download source code" href="https://github.com/wk910930/ccnn" target="_blank">Source code</a>]  	
                        <br>  
												<br>
												
<em> A simple and effective multi-scale feature operation. Showing and solving the initialization problem in existing multi-branch networks, e.g. Inception V2-V5, Hourglass, ResNxt, etc. </em>   <br>

												Wei Yang, Shuang Li, <strong>Wanli Ouyang</strong>, Hongsheng Li, XiaogangWang. "Learning Feature Pyramids for Human Pose Estimation", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yang_Learning_Feature_Pyramids_ICCV_2017_paper.pdf" target="_blank">Full Text</a>] [<a class="aLink" title="Code on Github" href="https://github.com/bearpaw/PyraNet" target="_blank">Source code</a>]
                        <br>  <br>



									<em> The first work on structured feature learning. </em>   <br>
                      	X. Chu,  <strong>Wanli Ouyang</strong> , H. Li, and X. Wang. 
                      	"Structured feature learning for pose estimation",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="http://arxiv.org/pdf/1603.09065.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Project" href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_structured_feature_pose.html" target="_blank">Project and dataset </a>]                          
                        [<a class="aLink" title="Project" href="https://www.youtube.com/watch?v=SMFt6TJ-ntA" target="_blank">Spotlight talk</a>]                          
                        [<a class="aLink" title="Code" href="https://github.com/chuxiaoselena/StructuredFeature" target="_blank">Source code </a>]                          
                        [<a class="aLink" title="Supplementary" href="http://www.ee.cuhk.edu.hk/~xgwang/StructureFeature/supp.pdf" target="_blank">Supplementary </a>]                          
                          <br>
                        <br>

									<em> The first Fully Convolutional Network for visual tracking. </em> <br>
                      	Lijun Wang,  <strong>Wanli Ouyang</strong>, Xiaogang Wang, and Huchuan Lu,
                      	"Visual Tracking with Fully Convolutional Networks",  In <em>Proc. ICCV </em> 2015. 
                        [<a class="aLink" title="Download Full Text" href="Papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf" target="_blank">Full Text</a>]  
                      [<a class="aLink" title="Project" href="http://scott89.github.io/FCNT/" target="_blank">Project and source code </a>]                

                  <h1>Journal Papers</h1>
                  
                  <table border="0" cellspacing="0" cellpadding="5" width="900">
                    <tbody>

                      <tr class="pub_tr_2">
                        <td width="20px" class="pub_td_number">  </td>
                        <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/progressive_cross-stream_cooperation_in_spatial_and_temporal_domain_for_action_localization.png" width="336px" height="200px"> </td>
                        <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                          Rui Su, Dong Xu, Luping Zhou, and <strong>Ouyang Wanli</strong>, "Progressive Cross-stream Cooperation in Spatial and Temporal Domain for Action Localization"  IEEE Trans. Pattern Anal. Mach. Intell. (<em>PAMI</em>), accepted, May, 2020
                          [<a class="aLink" href="#" target="_blank">Full Text</a>]
                          <br>
                        </td></tr>

                      <tr class="pub_tr_2">
                        <td width="20px" class="pub_td_number">  </td>
                        <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/An_End-to-End Learning_Framework_for_Video_Compression.png" width="336px" height="200px"> </td>
                        <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                          Guo Lu, Xiaoyun Zhang, <strong>W. Ouyang</strong>, Li Chen, Zhiyong Gao, Dong Xu, "An End-to-End Learning Framework for Video Compression"  IEEE Trans. Pattern Anal. Mach. Intell. (<em>PAMI</em>), accepted, Apr, 2020
                          [<a class="aLink" href="#" target="_blank">Full Text</a>]
                          <br>
                        </td></tr>

                      <tr class="pub_tr_2">
                        <td width="20px" class="pub_td_number">  </td>
                        <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/Image_Captioning_with_End-to-end_Attribute_Detection_and_Subsequent_Attributes_Prediction.png" width="336px" height="200px"> </td>
                        <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                          Yiqing Huang, Jiansheng Chen, <strong>W. Ouyang</strong>, Weitao Wan, Youze Xue, "Image Captioning with End-to-end Attribute Detection and Subsequent Attributes Prediction"  IEEE Trans. Image Process. (<em>TIP</em>) , accepted, Jan, 2020
                          [<a class="aLink" href="#" target="_blank">Full Text</a>]
                          <br>
                        </td></tr>

                      <tr class="pub_tr_2">
                        <td width="20px" class="pub_td_number">  </td>
                        <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/Person_Search_by_Separated_Modeling_and_A_Mask-Guided_Two-Stream_CNN_Model.png" width="336px" height="200px"> </td>
                        <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                          D. Chen, S. Zhang, <strong>W. Ouyang</strong>, J. Yang, Y. Tai, "Person Search by Separated Modeling and A Mask-Guided Two-Stream CNN Model"  IEEE Trans. Image Process. (<em>TIP</em>) , 29: 4669-4682, 2020
                          [<a class="aLink" href="#" target="_blank">Full Text</a>]
                          <br>
                        </td></tr>
                      
                        <tr class="pub_tr_2">
                          <td width="20px" class="pub_td_number">  </td>
                          <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/Self-Paced_Collaborative_and_Adversarial_Network_for_Unsupervised_Domain_Adaptation.png" width="336px" height="200px"> </td>
                          <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                            Weichen Zhang, Dong Xu, <strong>Wanli Ouyang</strong>, W. Li, "Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation"  IEEE Trans. Pattern Anal. Mach. Intell. (<em>PAMI</em>), accepted, Sept. 2019
                            [<a class="aLink" href="#" target="_blank">Full Text</a>]
                            <br>
                          </td></tr>
										
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/Det_Survey_2019.JPG" width="336px" height="200px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	Liu, Li, <strong>Wanli Ouyang</strong>, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietikäinen, "Deep learning for generic object detection: A survey,"  <em>IJCV</em>, accepted, Sept. 2019.
												[<a class="aLink" href="https://arxiv.org/pdf/1809.02165" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/DeepNonLocal.png" width="336px" height="200px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                        Guo Lu, Xiaoyun Zhang, <strong>Wanli Ouyang</strong>, Dong Xu, Li Chen, and Zhiyong Gao, “Deep Non-local Kalman Network for Video Compression Artifact Reduction”  <em>TIP</em>, accepted, Sept. 2019.
                        [<a class="aLink" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Guo_Lu_Deep_Kalman_Filtering_ECCV_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/ContextualSpatial.png" width="336px" height="200px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                        Lingbo Liu, Zhilin Qiu, Guanbin Li, Qing Wang, <strong>Wanli Ouyang</strong>, Liang Lin, “Contextualized Spatial-Temporal Network for Taxi Origin-Destination Demand Prediction”, IEEE Transactions on Intelligent Transportation Systems (TITS), accepted Apr., 2019.
                        [<a class="aLink" href="https://arxiv.org/abs/1905.06335" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>  										
									
												

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/ZoomNet.jpg" width="336px" height="300px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	Hongyang Li, Yu Liu, <strong>Wanli Ouyang</strong>, Xiaogang Wang, "Zoom out-and-in network with map attention decision for region proposal and object detection," <em>International Journal of Computer Vision, (IJCV)</em>, Accepted Jun., 2018.                
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2018_IJCV1&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2018_IJCV1&#39;)">BibTeX</a>]
												[<a class="aLink" href="https://arxiv.org/abs/1709.04347" target="_blank">Full Text</a>]
                        <br>
                        <div id="divAbstract2018_IJCV1" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2018_IJCV1" class="pub_div_abstract"> <br><b>Abstract:</b>  -->
In this paper, we propose a zoom-out-and-in network
for generating object proposals. A key observation is
that it is difficult to classify anchors of different sizes with
the same set of features. Anchors of different sizes should
be placed accordingly based on different depth within a network:
smaller boxes on high-resolution layers with a smaller
stride while larger boxes on low-resolution counterparts with
a larger stride. Inspired by the conv/deconv structure, we
fully leverage the low-level local details and high-level regional
semantics from two feature map streams, which are
complimentary to each other, to identify the objectness in an
image. A map attention decision (MAD) unit is further proposed
to aggressively search for neuron activations among
two streams and attend the most contributive ones on the feature
learning of the final loss. The unit serves as a decisionmaker
to adaptively activate maps along certain channels
with the solely purpose of optimizing the overall training
loss. One advantage of MAD is that the learned weights enforced
on each feature channel is predicted on-the-fly based
on the input context, which is more suitable than the fixed
enforcement of a convolutional kernel. Experimental results
on three datasets, including PASCAL VOC 2007, ImageNet
DET, MS COCO, demonstrate the effectiveness of our proposed
algorithm over other state-of-the-arts, in terms of average
recall (AR) for region proposal and average precision
(AP) for object detection.
                        </div>
                        <div id="divBibTex2018_IJCV1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{li2017zoom,
  title={Zoom out-and-in network with map attention decision for region proposal and object detection},
  author={Li, Hongyang and Liu, Yu and Ouyang, Wanli and Wang, Xiaogang},
  journal={International Journal of Computer Vision},
  year={2017},
  publisher={Springer}
}
<br>}
			  </code></div>
                      </td></tr>  										
										
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/TPAMI18_Depth.jpg" width="336px" height="180px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	Dan Xu, Elisa Ricci, <strong>Wanli Ouyang</strong>, Xiaogang Wang, Nicu Sebe, "Monocular Depth Estimation using Multi-Scale Continuous CRFs as Sequential Deep Networks," <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, Accepted Apr., 2018. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2018_PAMI1&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2018_PAMI1&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/pami_depth.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" href="https://github.com/danxuhk/ContinuousCRF-CNN" target="_blank">Source code</a>]
												
												
                        <br>
                        <div id="divAbstract2018_PAMI1" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2018_PAMI1" class="pub_div_abstract"> <br><b>Abstract:</b>  -->
Depth cues have been proved very useful in various computer vision and robotic tasks. This paper addresses the problem
of monocular depth estimation from a single still image. Inspired by the effectiveness of recent works on multi-scale convolutional
neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs.
Different from previous methods using concatenation or weighted average schemes, the integration is obtained by means of
continuous Conditional Random Fields (CRFs). In particular, we propose two different variations, one based on a cascade of multiple
CRFs, the other on a unified graphical model. By designing a novel CNN implementation of mean-field updates for continuous CRFs,
we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end.
Through extensive experimental evaluation we demonstrate the effectiveness of the proposed approach and establish new state of the
art results on three publicly available depth estimation datasets, i.e. NYUD-V2, Make3D and KITTI.
                        </div>
                        <div id="divBibTex2018_PAMI1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{xu2018monocular,
  title={Monocular Depth Estimation using Multi-Scale Continuous CRFs as Sequential Deep Networks},
  author={Xu, Dan and Ricci, Elisa and Ouyang, Wanli and Wang, Xiaogang and Sebe, Nicu},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2018}
}
<br>}
			  </code></div>
                      </td></tr>  
											

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/DCCRF.jpg" width="336px" height="150px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	Hui Zhou, <strong>Wanli Ouyang</strong>, Jian Cheng, Xiaogang Wang, and Hongsheng Li, "Deep Continuous Conditional Random Fields with Asymmetric Inter-object Constraints for Online Multi-object Tracking," <em>IEEE Trans. Circuits and System for Video Technology (CSVT)</em>, Accepted Apr., 2018.                
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2018_CSVT1&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2018_CSVT1&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/DCCRF_CSVT.pdf" target="_blank">Full Text</a>]
                        <br>
                        <div id="divAbstract2018_CSVT1" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2018_CSVT1" class="pub_div_abstract"> <br><b>Abstract:</b>  -->
Online Multi-Object Tracking (MOT) is a challenging
problem and has many important applications including intelligence
surveillance, robot navigation and autonomous driving.
In existing MOT methods, individual object’s movements and
inter-object relations are mostly modeled separately and relations
between them are still manually tuned. In addition, inter-object
relations are mostly modeled in a symmetric way, which we
argue is not an optimal setting. To tackle those difficulties, in this
paper, we propose a Deep Continuous Conditional Random Field
(DCCRF) for solving the online MOT problem in a track-bydetection
framework. The DCCRF consists of unary and pairwise
terms. The unary terms estimate tracked objects’ displacements
across time based on visual appearance information. They are
modeled as deep Convolution Neural Networks, which are able
to learn discriminative visual features for tracklet association.
The asymmetric pairwise terms model inter-object relations in
an asymmetric way, which encourages high-confidence tracklets
to help correct errors of low-confidence tracklets and not to be
affected by low-confidence ones much. The DCCRF is trained
in an end-to-end manner for better adapting the influences of
visual information as well as inter-object relations. Extensive
experimental comparisons with state-of-the-arts as well as detailed
component analysis of our proposed DCCRF on two public
benchmarks demonstrate the effectiveness of our proposed MOT
framework
                        </div>
                        <div id="divBibTex2018_CSVT1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{zhou2018Deep,
  title={Deep Continuous Conditional Random Fields with Asymmetric Inter-object Constraints for Online Multi-object Tracking},
  author={Hui Zhou and Wanli Ouyang and Jian Cheng and Xiaogang Wang and Hongsheng Li},
  journal={IEEE TSCVT},
  year={2018},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   

											
<!--   
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/Begans.jpg" width="336px" height="155px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	Yanchun Li, Nanfeng Xiao, <strong>Wanli Ouyang</strong>, "Improved Boundary Equilibrium Generative Adversarial Networks," <em>IEEE Access</em>, Accepted Feb., 2018.                
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2018_acess1&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2018_acess1&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/iBegan.pdf" target="_blank">Full Text</a>]
                        <br>
                        <div id="divAbstract2018_acess1" class="pub_div_abstract"><br><b>Abstract:</b> 
Boundary equilibrium generative adversarial networks (BEGANs) can generate impressively
realistic face images, but there is a trade-off between the quality and the diversity of generated images.
Based on BEGANs, we propose an effective approach to generate images with higher quality and better
diversity. By adding a second loss function (a denoising loss) to the discriminator, the discriminator can
learn more useful informations about the distribution of real images. Naturally, the ability of discriminator
in distinguishing between real and generated images is improved, which further guides the generator to
produce more realistic images to confuse the discriminator. We also find that using technique of batch
normalization in BEGANs architecture can improve the diversity of generated images. By using batch
normalization and adding a denoising loss to the objective of discriminator, we achieve comparative
generations on CIFAR-10 and CelebA datasets. In addition, we evaluate the effect of several techniques
on BEGANs framework through “Inception-Score", a measure which has been found to correlate well with
human assessment of generated samples.
                        </div>
                        <div id="divBibTex2018_acess1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016Improved,
  title={Improved boundary equilibrium Generative Adversarial Networks},
  author={Wanli Ouyang and Hui Zhou and 
Hongsheng Li and Quanquan Li and Junjie Yan and Xiaogang Wang},
  journal={IEEE Acess},
  year={2018},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   
---> 

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/PedCNN_TPAMI.jpg" width="336px" height="155px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	<strong>Wanli Ouyang</strong>, Hui Zhou, Hongsheng Li, Quanquan Li, Junjie Yan, Xiaogang Wang,
                      "Jointly learning deep features, deformable parts, occlusion and classification for pedestrian detection," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, 40(8):1874-1887, 2018.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2017_PAMI1&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2017_PAMI1&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/Ouyang2017JoingCNNPed.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" href="https://github.com/xiaohuige1/udn_extend" target="_blank">Source code</a>]												
                        <br>
                        <div id="divAbstract2017_PAMI1" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian
detection. Existing methods learn or design these components either individually or sequentially. The interaction among these
components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths
through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network
architecture ( Code available on www.ee.cuhk.edu.hk/~wlouyang/projects/ouyangWiccv13Joint/index.html). By establishing automatic,
mutual interaction among components, the deep model has average miss rate 8.57%/11.71% on the Caltech benchmark dataset with
new/original annotations.

                        </div>
                        <div id="divBibTex2017_PAMI1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016Jointly,
  title={Jointly learning deep features, deformable parts,
occlusion and classification for pedestrian detection},
  author={Wanli Ouyang and Hui Zhou and 
Hongsheng Li and Quanquan Li and Junjie Yan and Xiaogang Wang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell},
  pages={1874-1887},
  year={2018},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   


                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/GBD_TPAMI.jpg" width="336px" height="155px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	Xingyu Zeng (equal contribution), <strong>Wanli Ouyang</strong> (equal contribution), Junjie Yan, Hongsheng Li, Tong Xiao, Kun Wang, Yu Liu, Yucong Zhou, Bin
Yang, Zhe Wang, Hui Zhou, Xiaogang Wang,
                      "Crafting GBD-Net for Object Detection," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, accepted, 2017.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2017_PAMI2&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2017_PAMI2&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/Zeng2017_GBD_PAMI.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" href="https://github.com/craftGBD/craftGBD" target="_blank"> Code </a>]
												[<a class="aLink" href="./projects/GBD/index.html" target="_blank">Project page & code </a>]												
                        <br>
                        <div id="divAbstract2017_PAMI2" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

The visual cues from multiple support regions of
different sizes and resolutions are complementary in classifying
a candidate box in object detection. Effective integration of
local and contextual visual cues from these regions has become
a fundamental problem in object detection. In this paper, we
propose a gated bi-directional CNN (GBD-Net) to pass messages
among features from different support regions during both
feature learning and feature extraction. Such message passing
can be implemented through convolution between neighboring
support regions in two directions and can be conducted in
various layers. Therefore, local and contextual visual patterns can
validate the existence of each other by learning their nonlinear
relationships and their close interactions are modeled in a more
complex way. It is also shown that message passing is not always
helpful but dependent on individual samples. Gated functions are
therefore needed to control message transmission, whose on-oroffs
are controlled by extra visual evidence from the input sample.
The effectiveness of GBD-Net is shown through experiments
on three object detection datasets, ImageNet, Pascal VOC2007
and Microsoft COCO. Besides the GBD-Net, this paper also
shows the details of our approach in winning the ImageNet
object detection challenge of 2016, with source code provided on
https://github.com/craftGBD/craftGBD. In this winning system,
the modified GBD-Net, new pretraining scheme and better region
proposal designs are provided. We also show the effectiveness of
different network structures and existing techniques for object
detection, such as multi-scale testing, left-right flip, bounding box
voting, NMS, and context.

                        </div>
                        <div id="divBibTex2017_PAMI2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016Jointly,
  title={Crafting GBD-Net for Object Detection},
  author={Zeng, Xingyu and Ouyang, Wanli and Yan, Junjie and Li, Hongsheng and Xiao, Tong and Wang, Kun and Liu, Yu and Zhou, Yucong and Yang, Bin and Wang, Zhe and Hui Zhou and Xiaogang Wang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell},
  pages={1--14},
  year={2017},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   



                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/TCNN.png" width="336px" height="200px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                        K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang, R. Wang, X. Wang, <strong>W. Ouyang</strong>, T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos, IEEE Transactions on Circuits and Systems for Video Technology (CSVT), accepted, 2017.
                        [<a class="aLink" href="https://arxiv.org/abs/1604.02532" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>


                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <!--<img style="width: 336Px;" alt="Wanli" src="imgs/Deep_IDNet_PAMI.jpg" width="336px" height="155px">--> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	<strong>Wanli Ouyang</strong>, Xingyu Zeng,  Xiaogang Wang, et al,
                      "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, accepted, 2016.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2016_PAMI2&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2016_PAMI2&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/DeepID-Net.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Project" href="./projects/ImageNet/index.html" target="_blank">Project</a>]                          
                        <br>
                        <div id="divAbstract2016_PAMI2" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep
learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation
constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training
strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability.
By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of
models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach
improves the mean averaged precision obtained by RCNN [16], which was the state-of-the-art, from 31% to 50.3% on the ILSVRC2014
detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1%. Detailed component-wise analysis is also
provided through extensive experimental evaluation, which provides a global view for people to understand the deep learning object
detection pipeline.

Wanli Ouyang and Xingyu Zeng and Xiaogang Wang  and Shi Qiu and Ping Luo and Yonglong Tian and
Hongsheng Li and Shuo Yang and Zhe Wang and Hongyang Li and Kun Wang and Junjie Yan and Chen-Change Loy and Xiaoou Tang
                        </div>
                        <div id="divBibTex2016_PAMI2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016DeepID-Net,
  title={DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks},
  author={Wanli Ouyang and Xingyu Zeng and Xiaogang Wang  and Shi Qiu and Ping Luo and Yonglong Tian and Hongsheng Li and Shuo Yang and Zhe Wang and Hongyang Li and Kun Wang and Junjie Yan and Chen-Change Loy and Xiaoou Tang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell},
  pages={1--14},
  year={2016},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <!--<img style="width: 336Px;" alt="Wanli" src="images/OHT_J.jpg" width="336px" height="205px">--><img style="width: 336Px;" alt="Wanli" src="imgs/FastFull.png" width="336px" height="205px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	<strong>Wanli Ouyang</strong>, Tianle Zhao, Wai-Kuen Cham,  Liying Wei
                      "Fast Full-Search Equivalent Pattern Matching Using Asymmetric Haar Wavelet Packets," 
                      <em>IEEE Trans. Circuits and System for Video Technology (CSVT)</em>, accepted, 2016.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2016_IJCV&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2016_IJCV&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/AHT_CSVT16.pdf" target="_blank">Full Text</a>]
			                  [<a class="aLink" href="./OHT.htm" target="_blank">Project & source code</a>]
                        [<a class="aLink" title="Download Slides" href="./PPT/OHT1.ppt" target="_blank">Slides</a>]
                        <br>
                        <div id="divAbstract2016_CSVT" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Pattern matching is widely used in signal processing,
computer vision, image and video processing. One efficient
approach is to perform pattern matching in a transform domain
that has good energy packing ability and so allows early rejection
of most mismatched candidates. Calculating the transforms of
pixels in sliding windows requires much computation, and so
fast algorithms are employed. Existing methods require O(u)
additions per pixel for projecting input pixels onto u 2D basis
vectors. In this paper, we propose a new 2D transform, called
asymmetric 2D Haar transform (A2DHT), and extend it to
wavelet packets that contain exponentially large number of bases.
A basis selection algorithm is then proposed to search for the
optimal basis in the wavelet packets. A fast algorithm is also
developed which can compute u projection coefficients with
only O(log u) additions per pixel. Results of experiments show
that the proposed fast algorithm and the proposed transform
can significantly accelerate the full-search equivalent pattern
matching process and outperform state-of-the-art methods.
                        </div>
                        <div id="divAbstract2016_CSVT" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016fast,
  title={Fast Full-Search Equivalent Pattern Matching Using Asymmetric Haar Wavelet Packets},
  author={Ouyang, Wanli and Zhao, Tianle and Cham, Wai-kuen and Wei, Liying},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2016},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   

                      
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <!--<img style="width: 336Px;" alt="Wanli" src="imgs/LearningMutual.png" width="336px" height="205px">--> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	<strong>Wanli Ouyang</strong>, Xingyu Zeng and Xiaogang Wang, 
                      "Learning Mutual Visibility Relationship for Pedestrian Detection with a Deep Model," 
                      <em>International Journal of Computer Vision (IJCV)</em>, accepted, 2016.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2016_IJCV&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2016_IJCV&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/Mutual-DBN-IJCV16.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" href="./projects/ouyangZWcvpr13MutVisibility/index.html" target="_blank">  Project and code</a>]
                        <br>
                        <div id="divAbstract2016_IJCV" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. 
The difficulty is added when several pedestrians overlap in images and occlude each other. 
We observe, however, that the occlusion/visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation - the visibility estimation of one pedestrian facilitates the visibility estimation of another. 
In this paper, we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. 
The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. 
Then the evidence of co-existing pedestrians is used for improving the single pedestrian detection results. 
Compared with existing image-based pedestrian detection approaches, our approach has the lowest average miss rate on the Caltech-Train dataset and the ETH dataset. 
Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. 
The mutual visibility deep model leads to 6% - 15% improvements on multiple benchmark datasets.
                        </div>
                        <div id="divBibTex2016_IJCV" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016learning,
  title={Learning Mutual Visibility Relationship for Pedestrian Detection with a Deep Model},
  author={Ouyang, Wanli and Zeng, Xingyu and Wang, Xiaogang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  pages={1--14},
  year={2016},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <!--<img style="width: 336Px;" alt="Wanli" src="imgs/PersonReid.png" width="336px" height="120px">--> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> Rui Zhao, <strong>Wanli Ouyang</strong> and Xiaogang Wang, 
                      "Person Re-identification by saliency Learning," 
                      <em>IEEE Trans. Pattern Analysis and machine Intelligence(TPAMI)</em>, accepted, 2016.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2016_tpami&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2016_TPAMI&#39;)">BibTeX</a>]
												[<a class="aLink" href=".\projects\Person_reid_pami\TPAMI2544310_proof2.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="" target="_blank">  Project and dataset(N/A)</a>]
                        <br>
                        <div id="divAbstract2015_tpami" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Human eyes can recognize person identities based on small salient regions, i.e. person saliency is distinctive and reliable
in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities
of pedestrian images with existing approaches. Inspired by our user study result of human perception on person saliency, we propose
a novel perspective for person re-identification based on learning person saliency and matching saliency distribution. The proposed
saliency learning and matching framework consists of four steps: (1) To handle misalignment caused by drastic viewpoint change
and pose variations, we apply adjacency constrained patch matching to build dense correspondence between image pairs. (2) We
propose two alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a saliency score for each image patch,
through which distinctive features stand out without using identity labels in the training procedure. (3) saliency matching is proposed
based on patch matching. Matching patches with inconsistent saliency brings penalty, and images of the same identity are recognized
by minimizing the saliency matching cost. (4) Furthermore, saliency matching is tightly integrated with patch matching in a unified
structural RankSVM learning framework. The effectiveness of our approach is validated on the four public datasets. Our approach
outperforms the state-of-the-art person re-identification methods on all these datasets.
                        </div>
                        <div id="divBibTex2016_TPAMI" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{zhao2014person,
  title={Person Re-identification by Saliency Learning},
  author={Zhao, Rui and Ouyang, Wanli and Wang, Xiaogang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2016}
}
<br>}
			  </code></div>
                      </td></tr>   
                      
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px" style="text-align: center"> <!--<img style="width: 336Px;" alt="Wanli" src="HumanDBNDetRes.jpg" width="336px" height="205px">--> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> <strong>Wanli Ouyang</strong>, Xingyu Zeng and Xiaogang Wang, 
                      "Partial Occlusion Handling in Pedestrian Detection with a Deep Model," 
                      <em>IEEE Trans. Circuits and System for Video Technology (TCSVT)</em>, accepted, 2015.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_tpami&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_TPAMI&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/TCSVT_ouyang_xyZeng_xgwang.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="http://www.ee.cuhk.edu.hk/~xgwang/CUHK_pedestrian.html" target="_blank">  Project</a>]
                        <br>
                        <div id="divAbstract2015_tpami" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Part-based models have demonstrated their merit	in object detection. However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions, abnormal deformations, appearances or illuminations. To handle the imperfection of part detectors, this paper presents a probabilistic pedestrian detection framework. 
In this framework, a deformable part-based model is used to obtain the scores of part detectors and the visibilities of parts are modeled as hidden variables. 
Once the occluded parts are identified, their effects are properly removed from the final detection score.
Unlike previous occlusion handling approaches that assumed independence among the visibility probabilities of parts or manually defined rules for the visibility relationship, a deep model is proposed in this paper for learning the visibility relationship among overlapping parts at multiple layers. 
The proposed approach can be viewed as a general post-processing of part-detection results and can take detection scores of existing part-based models as input. 
Experimental results on three public datasets (Caltech, ETH and Daimler) and a new CUHK occlusion dataset, which is specially designed for the evaluation of occlusion handling approaches,  show the effectiveness of the proposed approach.

                        </div>
                        <div id="divBibTex2015_TPAMI" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyangPartial2015,
  <br>title={Partial Occlusion Handling in Pedestrian Detection with a Deep Model},
  <br>author={Ouyang, Wanli and Zeng, Xingyu and Wang, Xiaogang},
  <br>publisher={IEEE Trans. Circuits and System for Video Technology}
<br>}
			  </code></div>
                      </td></tr>   


                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <!--<img style="width: 336Px;" alt="Wanli" src="imgs/SinglePedestrian.png" width="336px" height="205px">--> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> <strong>Wanli Ouyang</strong>, Xingyu Zeng and Xiaogang Wang, 
                      "Single-Pedestrian Detection Aided by Two-Pedestrian Detection," 
                      <em>IEEE Trans. Pattern Analysis and machine Intelligence(TPAMI)</em>, 37(9), 1875-1889, 2015.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_tpami&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_tip&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/tpami15-ouyang_xgWang_xyZeng.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="./projects/ouyangWcvpr13MultiPed/index.html" target="_blank">  Project, source code and dataset</a>]
                        <br>
                        <div id="divAbstract2015_tpami" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

In this paper, we address the challenging problem of detecting pedestrians who appear in groups. A new approach is
proposed for single-pedestrian detection aided by two-pedestrian detection. A mixture model of two-pedestrian detectors is
designed to capture the unique visual cues which are formed by nearby pedestrians but cannot be captured by single-pedestrian
detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and
two-pedestrian detectors, and to refine the single-pedestrian detection result using two-pedestrian detection. The two-pedestrian
detector can integrate with any single-pedestrian detector. Twenty-five state-of-the-art single-pedestrian detection approaches are
combined with the two-pedestrian detector on three widely used public datasets: Caltech, TUD-Brussels, and ETH. Experimental
results show that our framework improves all these approaches. The average improvement is 9 percent on the Caltech-Test
dataset, 11 percent on the TUD-Brussels dataset and 17 percent on the ETH dataset in terms of average miss rate. The lowest
average miss rate is reduced from 37 to 32 percent on the Caltech-Test dataset, from 55 to 50 percent on the TUD-Brussels dataset
and from 43 to 38 percent on the ETH dataset.
                        </div>
                        <div id="divBibTex2013_tip" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2015single,
  title={Single-Pedestrian Detection Aided by Two-Pedestrian Detection},
  author={Ouyang, Wanli and Zeng, Xingyu and Wang, Xiaogang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={37},
  number={9},
  pages={1875--1889},
  year={2015},
  publisher={IEEE}
}
			  </code></div>
                      </td></tr>   
                      
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="mainpa1.jpg" width="336px" height="205px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> <strong>Wanli Ouyang</strong>, Renqi Zhang and Wai-Kuen Cham, 
                      "Segmented Gray-Code Kernels for Fast Pattern Matching," 
                      <em>IEEE Trans. Image Processing(TIP)</em>, 22(4):1512-1525, Apr. 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_tip&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_tip&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/SegGCK.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="./SegGCK.htm" target="_blank">  Project, source code and dataset</a>]
                        <br>
                        <div id="divAbstract2013_tip" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->
                        The gray-code kernels (GCK) family, which has
												Walsh Hadamard transform on sliding windows as a member, is a
												family of kernels that can perform image analysis efficiently using
												a fast algorithm, such as the GCK algorithm. The GCK has been
												successfully used for pattern matching. In this paper, we propose
												that the G4-GCK algorithm is more efficient than the previous
												algorithm in computing GCK. The G4-GCK algorithm requires
												four additions per pixel for three basis vectors independent of
												transform size and dimension. Based on the G4-GCK algorithm,
												we then propose the segmented GCK. By segmenting input data
												into Ls parts, the SegGCK requires only four additions per
												pixel for 3Ls basis vectors. Experimental results show that the
												proposed algorithm can significantly accelerate the full-search
												equivalent pattern matching process and outperforms state-ofthe-
												art methods.
                        </div>
                        <div id="divBibTex2013_tip" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @ARTICLE{Ouyang2013SegGCK,                        
			  <br>author = {Wanli Ouyang and Federico Tombari and Stefano Mattoccia and Luigi
			  <br>Di Stefano and Wai-Kuen Cham},
			  <br>title = {Segmented Gray-Code Kernels for Fast Pattern Matching},
			  <br>journal = {IEEE Trans. Image Processing},
			  <br>year = {2013},
			  <br>volume = {22},
			  <br>pages = {1512-1525},
			  <br>number = {4},
			  <br>month = {Apr.},
			  <br>}</code></div>
                      </td></tr>   

                    
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                     <td width="304px" style="text-align: center"> <!--  <img style="width: 336Px;" alt="Wanli" src="mainpa1.jpg" width="336px" height="205px"> </td>  -->                    
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> <strong>Wanli Ouyang</strong>, Federico Tombari, Stefano Mattoccia, Luigi Di Stefano and Wai-Kuen Cham, 
                      "Performance Evaluation of Full Search Equivalent Pattern Matching Algorithms," 
                      <em>IEEE Trans. Pattern Analysis and machine Intelligence(TPAMI)</em>, 34(1):127 - 143, Jan. 2012.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2012_tpami&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2012_tpami&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/PME_TPAMI.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="./PMEval/index.htm" target="_blank">  Project, source code, and dataset</a>]
                        <br>
                        <div id="divAbstract2012_tpami" class="pub_div_abstract"><br><b>Abstract:</b> 
                        Pattern matching is widely used in signal processing, computer vision, and image and video processing. 
                        Full search equivalent algorithms accelerate the pattern matching process and, in the meantime, 
                        yield exactly the same result as the full search. This paper proposes an analysis and comparison 
                        of state-of-the-art algorithms for full search equivalent pattern matching. Our intention is that 
                        the data sets and tests used in our evaluation will be a benchmark for testing future pattern 
                        matching algorithms, and that the analysis concerning state-of-the-art algorithms could inspire new fast algorithms. 
                        We also propose extensions of the evaluated algorithms and show that they outperform the original formulations.
                        </div>
                        <div id="divBibTex2012_tpami" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @ARTICLE{Ouyang:PME,                        
			  <br>author = {Wanli Ouyang and Federico Tombari and Stefano Mattoccia and Luigi
			  <br>Di Stefano and Wai-Kuen Cham},
			  <br>title = {Performance Evaluation of Full Search Equivalent Pattern Matching
			  <br>Algorithms},
			  <br>journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
			  <br>year = {2012},
			  <br>volume = {34},
			  <br>pages = {127 - 143},
			  <br>number = {1},
			  <br>month = {Jan.},
			  <br>}</code></div>
                      </td></tr>   
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                       <td width="304px"> </td> 
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      F. Tombari, <strong>Wanli Ouyang</strong>, L. Di Stefano, W.K. Cham, 
                      “Adaptive Low Resolution Pruning for Fast Full-Search Equivalent Pattern Matching,”
                      <em>Pattern Recognition Letters (JPRL)</em>, 32(15), pp 2119-2127, November 2011

                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2011_Jprl&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2011_Jprl&#39;)">BibTeX</a>]
												[<a class="aLink" href="./Papers/jprl11b.pdf" target="_blank">  Full Text</a>]
                        <br>
                        <div id="divAbstract2011_Jprl" class="pub_div_abstract"> <br><b>Abstract:</b> 
                        Several recent proposals have shown the feasibility of signiﬁcantly speeding-up pattern matching by means of Full Search-equivalent techniques, i.e. without approximating the outcome of the search with
                        respect to a brute force investigation. These techniques are generally heavily based on efﬁcient incremental calculation schemes aimed at avoiding unnecessary computations. In a very recent and extensive
                        experimental evaluation, Low Resolution Pruning turned out to be in most cases the best performing
                        approach. In this paper we propose a computational analysis of several incremental techniques speciﬁ-
                        cally designed to enhance the efﬁciency of LRP. In addition, we propose a novel LRP algorithm aimed
                        at minimizing the theoretical number of operations by adaptively exploiting different incremental
                        approaches. We demonstrate the effectiveness of our proposal by means of experimental evaluation
                        on a large dataset
                         <br></div>
                        <div id="divBibTex2011_Jprl" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @ARTICLE{Ouyang:PME,                        
			  <br>author = {Federico Tombari and Wanli Ouyang and Luigi	Di Stefano and Wai-Kuen Cham},
			  <br>title = {“Adaptive Low Resolution Pruning for Fast Full-Search Equivalent Pattern Matching},
			  <br>journal = {Pattern Recognition Letters (JPRL)},
			  <br>year = {2011},
			  <br>volume = {32},
			  <br>pages = {2119-2127},
			  <br>number = {15},
			  <br>month = {Nov.},
			  <br>}</code></div>
                      </td></tr>                             
                                     
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"><!-- <img alt="Photo" src="imgs/FastAlgorithm.png" width="336px" height="205px"> </td> -->
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"><strong> Wanli Ouyang </strong> and Wai-Kuen Cham, 
                      	"Fast algorithm for Walsh Hadamard transform on sliding windows", 
                      	<em> IEEE Trans. Pattern Analysis and machine Intelligence (TPAMI), </em>
                      	32(1):165-171, Jan. 2010.                      
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2010_PAMI&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2010_PAMI&#39;)">BibTeX</a>]
			[<a class="aLink" href="./Software/FWHT.m" target="_blank">Matlab Code for the proposed fast algorithm</a>]			
			[<a class="aLink" href="./FWHT.htm" target="_blank"> Project & source code</a>]
                        [<a class="aLink" title="Download Full Text" href="./Papers/Manu_FWHT.pdf" target="_blank">Full Text</a>]
                        <br>
                        <div id="divAbstract2010_PAMI" class="pub_div_abstract"><br><b>Abstract:</b> 
                        	This paper proposes a fast algorithm for Walsh Hadamard Transform on sliding windows 
                        	which can be used to implement pattern matching most efficiently. 
                        	The computational requirement of the proposed algorithm is about 1.5 additions per projection vector per sample, 
                        	which is the lowest among existing fast algorithms for Walsh Hadamard Transform on sliding windows.
                        <br></div>
                        <div id="divBibTex2010_PAMI" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@ARTICLE{Wanli:GCK,
			  <br>author = {W. Ouyang and W.K. Cham},
				  <br>title = {Fast Algorithm for {W}alsh {H}adamard transform on Sliding Windows},
				  <br>journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
				  <br>year = {2010},
				  <br>volume = {32},
				  <br>pages = {165-171},
				  <br>number = {1},
				  <br>month = {Jan.},
			<br>}</code></div>                        	
                      </td></tr>
                      

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <!--<img style="width: 336Px;" alt="Wanli" src="imgs/ImprovedBoundary.png" width="336px" height="200px">--> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                        Y. Li, N. Xiao, <strong>W. Ouyang</strong>, ”Improved Boundary Equilibrium Generative Adversarial Networks,” IEEE Access”, Accepted, Jan., 2018.
                        [<a class="aLink" href="https://ieeexplore.ieee.org/document/8288664" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/ImprovedGenerative.png" width="336px" height="200px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                        Y. Li, N. Xiao, <strong>W. Ouyang</strong>, ”Improved Generative Adversarial Networks with Reconstruction Loss,” Neurocomputing, Accepted, Oct., 2018.
                        [<a class="aLink" href="https://www.sciencedirect.com/science/article/pii/S0925231218311901" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/PartAligned.png" width="336px" height="200px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                        L. Huang, Y. Huang, <strong>Wanli Ouyang</strong>, L. Wang, ”Part-Aligned Pose-Guided Recurrent Network for Action Recognition,” Pattern Recognition (PR), accepted Mar., 2019.
                        [<a class="aLink" href="https://www.sciencedirect.com/science/article/pii/S0031320319301098" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/PerceptualImage.png" width="336px" height="200px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                        Yukai Shi, Jinghui Qin, Pengxu Wei, <strong>Wanli Ouyang</strong>, Liang Lin, “Perceptual Image Enhancement by Relativistic Discriminant Learning With Cross-Scale Aggregated Representation”, IEEE Access, accepted Mar., 2019.
                        [<a class="aLink" href="https://ieeexplore.ieee.org/document/8672893" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/ShowTell.png" width="336px" height="200px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                        Zhiwang Zhang, Dong Xu, <strong>Wanli Ouyang</strong>, Chuanqi Tan, “Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization”, IEEE Trans. Circuits Syst. Video Technol. (CSVT), accepted Aug., 2019.
                        [<a class="aLink" href="https://ieeexplore.ieee.org/document/8807239" target="_blank">Full Text</a>]
                        <br>
                      </td></tr>

                    <tr class="pub_tr_1">  
                      <td width="20%" align="left" colspan="2" style="HEIGHT:10px"></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="20%" align="left" colspan="2"><a class="aLink" href="./#Top">Back To Top</a></td></tr></tbody></table>

                    
                      	
<!--========================Conference Papers ========================-->                      	
                  <h1>Conference Papers</h1>
                  <table border="0" cellspacing="0" cellpadding="5" width="900">
                    <tbody>
                

                      <tr class="pub_tr_2">
                        <td width="20px" class="pub_td_number"> </td>
                        <td width="304px" style="vertical-align: middle"> <img alt="ecoNAS" border="0" src="imgs/ecoNAS.jpg" width="304px" height="200" >
                        </td>
                    
                        <td class="pub_td_text" width="500px" style="vertical-align: middle"> 
                          <br>
                          Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang, <strong>Wanli Ouyang</strong>, “EcoNAS: Finding Proxies for Economical Neural Architecture Search”, Proc. CVPR, 2020.                  
                          <br>     
                          <br>
                          Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, <strong>W. Ouyang</strong>, “Unifying Spatial-Temporal Graph Convolutions with Disentangled Multi-Scale Aggregators for Skeleton-Based Action Recognition”, CVPR, 2020, accepted (Oral).                
                          <br>
                          <br>
                          Jinyang Guo, <strong>W. Ouyang</strong>, Dong Xu, “Multi-Dimensional Pruning: A Uniﬁed Framework for Model Compression”, CVPR, 2020, accepted (Oral).                 
                          <br> 
                          <br>
                          Xiang Li, Chen Lin, Chuming Li, Ming SUn, Wei Wu, Junjie Yan, <strong>W. Ouyang</strong>, “Improving One-shot NAS by Suppressing the Posterior Fading”, CVPR, 2020, accepted.                 
                          <br> 
                          <br>
                          Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, <strong>W. Ouyang</strong>, Changqing Yin, Junjie Yan, “Equalization Loss for Long-Tailed Object Recognition, CVPR, 2020, accepted.               
                          <br> 
                          <br>
                          Wang Zeng, Wei Yang, <strong>W. Ouyang</strong>, Ping Luo, Wentao Liu, Xiaogang Wang, “3D Human Mesh Regression with Dense Correspondence”, CVPR, 2020, accepted.                 
                          <br>                         

                        </td>
                    </tr>


                        <tr class="pub_tr_2">
                            <td width="20px" class="pub_td_number"> </td>
                         <td width="304px" style="vertical-align: middle"> 
                        </td>
                        
                            <td class="pub_td_text" width="500px" style="vertical-align: middle"> 
                              <br>                  
                              Jinyang Guo, <strong>Wanli Ouyang</strong>, Dong Xu. ”Channel Pruning Guided by Classiﬁcation Loss and Feature Importance”, Proc. AAAI, 2020. [<a class="aLink" target="_blank">Full Text</a>]
                              <br>     
                              <br>                          
                              Linjiang Huang, Yan Huang, <strong>Wanli Ouyang</strong>, Liang Wang, “Relational Prototypical Network for Weakly Supervised Temporal Action Localization”, Proc. AAAI, 2020, (Oral). [<a class="aLink" target="_blank">Full Text</a>]
                              <br>
                              <br>
                              Linjiang Huang, Yan Huang, <strong>Wanli Ouyang</strong>, Liang Wang, “Part-Level Graph Convolutional Network for Skeleton-Based Action Recognition”, Proc. AAAI, 2020, (Oral).[<a class="aLink" target="_blank">Full Text</a>]
                              <br>
                              <br>
                              Di Chen, Shanshan Zhang, <strong>Wanli Ouyang</strong>, Jian Yang, Bernt Schiele, “Hierarchical Online Instance Matching for Person Search”, Proc. AAAI, 2020, accepted (Oral). [<a class="aLink" target="_blank">Full Text</a>]
                              <br>
                              <br>
                              Qi Chu, <strong>Wanli Ouyang</strong>, Bin Liu, Feng Zhu, Nenghai Yu, “DASOT: A Uniﬁed Framework Integrating Data Association and Single Object Tracking for Online Multi-Object Tracking”, Proc. AAAI, 2020, accepted. [<a class="aLink" target="_blank">Full Text</a>]
                              <br>
                              <br>

                            </td>
                        </tr>



                


                <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px" style="vertical-align: middle"> <img alt="OHT_Complexity" border="0" src="imgs/GradNet.png" width="304px" height="200" ><img alt="OHT_Complexity" border="0" src="imgs/CrowdCounting.png" width="304px" height="70" ><img alt="OHT_Complexity" border="0" src="imgs/AccurateMonocular.png" width="304px" height="70" >
                  </td>
                  
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> 
                        <br>                  
Peixia Li, Boyu Chen, <strong>W. Ouyang</strong>, Dong Wang, Xiaoyun Yang, Huchuan Lu. ”GradNet: Gradient-Guided Network for Visual Object Tracking”, Proc. ICCV, 2019. (Oral) [<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_GradNet_Gradient-Guided_Network_for_Visual_Object_Tracking_ICCV_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>     
                        <br>                          
Haodong Duan, Kwan-Yee Lin, Sheng Jin, Wentao Liu, Chen Qian, <strong>W. Ouyang</strong>. ”TRB: A Novel Triplet Representation for Understanding 2D Human Body”, Proc. ICCV, 2019. (Oral) [<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_TRB_A_Novel_Triplet_Representation_for_Understanding_2D_Human_Body_ICCV_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>    
                        <br>  
                        Lingbo Liu, Zhilin Qiu , Guanbin Li, Shufan Liu, <strong>W. Ouyang</strong>, Liang Lin. ”Crowd Counting with Deep Structured Scale Integration Network”, Proc. ICCV, 2019.  [<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Crowd_Counting_With_Deep_Structured_Scale_Integration_Network_ICCV_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>    
                        <br>                        
                        Lu Sheng, Dan Xu, <strong>W. Ouyang</strong>, Xiaogang Wang. ”Unsupervised Collaborative Learning of Keyframe Detection and Visual Odometry towards Monocular Deep SLAM”, Proc. ICCV, 2019. [<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Sheng_Unsupervised_Collaborative_Learning_of_Keyframe_Detection_and_Visual_Odometry_Towards_ICCV_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>    
                        <br>    
Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei Wu, Junjie Yan, Dahua Lin, <strong>W. Ouyang</strong>. ”Online Hyper-parameter Learning for Auto-Augmentation Strategy”, Proc. ICCV, 2019.[<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_Online_Hyper-Parameter_Learning_for_Auto-Augmentation_Strategy_ICCV_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>    
                        <br>    
Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, <strong>W. Ouyang</strong>, Xin Fan. ”Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving”, Proc. ICCV, 2019. [<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Accurate_Monocular_3D_Object_Detection_via_Color-Embedded_3D_Reconstruction_for_ICCV_2019_paper.pdf" target="_blank">Full Text</a>] 
                        <br>    
                        <br>    
Yunan Li, Qiguang Miao, <strong>W. Ouyang</strong>, Zhenxin Ma, Huijuan Fang, Chao Dong, Yining Quan. ”LAP-Net: Level-Aware Progressive Network for Image Dehazing”, Proc. ICCV, 2019. [<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Li_LAP-Net_Level-Aware_Progressive_ICCV_2019_supplemental.pdf" target="_blank">Full Text</a>]
                        <br>    
                        <br>    
Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, <strong>W. Ouyang</strong>. ”AM-LFS: AutoML for Loss Function Search”, Proc. ICCV, 2019. [<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_AM-LFS_AutoML_for_Loss_Function_Search_ICCV_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>    
                        <br>    
Yingyue Xu, Dan Xu, Xiaopeng Hong, W. Ouyang, Rongrong Ji, Min Xu, Guoying Zhao. ”Structured Modeling of Joint Deep Feature and Prediction Reﬁnement for Salient Object Detection”, Proc. ICCV, 2019. [<a class="aLink" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Structured_Modeling_of_Joint_Deep_Feature_and_Prediction_Refinement_for_ICCV_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>  
                      </td>
                  </tr>


                <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px" style="vertical-align: middle"> <img alt="OHT_Complexity" border="0" src="imgs/DaNet.png" width="304px" height="150" > <img alt="OHT_Complexity" border="0" src="imgs/IntersectGAN.png" width="304px" height="150" ></td>
 <td class="pub_td_text" width="500px" style="vertical-align: middle">                          <br>                  
                  H. Zhang, Jie Cao, Guo Lu, W. Ouyang, Zhenan Sun. ”DaNet: Decomposeand-aggregate Network for 3D Human Shape and Pose Estimation ”, Proc. ACM Multimedia, 2019. [<a class="aLink" href="https://hongwenzhang.github.io/pdf/acmmm19DaNet.pdf" target="_blank">Full Text</a>] 
                        <br>
                        <br>
                  Z. Yao, B. Zhang, Z. Wang W. Ouyang, D. Xu, D. Feng. ”IntersectGAN: Learning Domain Intersection for Generating Images with Multiple Attributes”, Proc. ACM Multimedia, 2019. [<a class="aLink" href="https://arxiv.org/abs/1909.09767" target="_blank">Full Text</a>] 
                      </td>
                  </tr>  



										
                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px" style="vertical-align: middle"> <img alt="OHT_Complexity" border="0" src="./imgs/DVC.JPG" width="304px" height="200" ><img alt="OHT_Complexity" border="0" src="./imgs/HybridTaskCascade.JPG" width="304px" height="70" ><img alt="OHT_Complexity" border="0" src="./imgs/GS3D.JPG" width="304px" height="70" >
									</td>
									
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">	
                        <br>									
Guo Lu, <strong>Wanli Ouyang</strong>, Dong Xu,  Chunlei Cai, Xiaoyun Zhang, Zhiyong Gao. "DVC: An End-to-end Deep Video Compression Framework", Proc. CVPR, 2019, Accepted. (Oral) [<a class="aLink" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Lu_DVC_An_End-To-End_Deep_Video_Compression_Framework_CVPR_2019_paper.pdf" target="_blank">Full Text</a>] [<a class="aLink" href="https://github.com/GuoLusjtu/DVC" target="_blank">Source code</a>]
                        <br>		 
												<br>													
Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, <strong>Wanli Ouyang</strong>,Chen Change Loy, Dahua Lin. "Hybrid Task Cascade for Instance Segmentation", Proc. CVPR, 2019, Accepted. [<a class="aLink" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Hybrid_Task_Cascade_for_Instance_Segmentation_CVPR_2019_paper.pdf" target="_blank">Full Text</a>]   [<a class="aLink" href="https://github.com/
open-mmlab/mmdetection" target="_blank">Source code (ranking 1st in the COCO 2018 Challenge Object Detection Task) </a>]
                        <br>		
												<br>	
												Buyu Li, <strong>Wanli Ouyang</strong>, Lu Sheng, et. al. "GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving", Proc. CVPR, 2019, Accepted.	[<a class="aLink" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_GS3D_An_Efficient_3D_Object_Detection_Framework_for_Autonomous_Driving_CVPR_2019_paper.pdf" target="_blank">Full Text</a>]
												<br>		
												<br>												
												Jiangmiao Pang, Kai. Chen, Jianping Shi, <strong>Wanli Ouyang</strong>, et. al. "Libra R-CNN: Balanced Learning for Object Detection", Proc. CVPR, 2019, Accepted.	[<a class="aLink" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Pang_Libra_R-CNN_Towards_Balanced_Learning_for_Object_Detection_CVPR_2019_paper.pdf" target="_blank">Full Text</a>]	[<a class="aLink" href="https://github.com/OceanPang/
Libra_R-CNN" target="_blank">Source code</a>]	
                        <br>		
												<br>		
Chunfeng Song, Yan Huang, <strong>Wanli Ouyang</strong>, Liang Wang. "Box-driven Class-wise Region Masking and Filling Rate Guided Loss for Weakly Supervised Semantic Segmentation", Proc. CVPR, 2019, Accepted.  [<a class="aLink" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Song_Box-Driven_Class-Wise_Region_Masking_and_Filling_Rate_Guided_Loss_for_CVPR_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>		
												<br>		
Sheng Jin, Wentao Liu, <strong>Wanli Ouyang</strong>, Chen Qian. "Multi-person Articulated Tracking with Spatial and Temporal Embeddings", Proc. CVPR, 2019, Accepted. [<a class="aLink" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Jin_Multi-Person_Articulated_Tracking_With_Spatial_and_Temporal_Embeddings_CVPR_2019_paper.pdf" target="_blank">Full Text</a>] 
                        <br>		
												<br>		
Rui Su, <strong>Wanli Ouyang</strong>, Luping Zhou, Dong Xu. "Improving Action Localization by Progressive Cross-stream Cooperation", Proc. CVPR, 2019, Accepted.  [<a class="aLink" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Su_Improving_Action_Localization_by_Progressive_Cross-Stream_Cooperation_CVPR_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>		
												<br>		
Pu Zhang, <strong>Wanli Ouyang</strong>, Pengfei Zhang, Jianru Xue, Nanning Zheng. "SR-LSTM: State Refinement for LSTM towards Pedestrian Trajectory Prediction", Proc. CVPR, 2019, Accepted. [<a class="aLink" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_SR-LSTM_State_Refinement_for_LSTM_Towards_Pedestrian_Trajectory_Prediction_CVPR_2019_paper.pdf" target="_blank">Full Text</a>]
                        <br>
											  <br>	
								      </td>
									</tr>       
									

                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px" style="vertical-align: middle"> <img alt="OHT_Complexity" border="0" src="./imgs/Intertwiner.JPG" width="304px" height="150" > </td>
 <td class="pub_td_text" width="500px" style="vertical-align: middle">	                        <br>									
									Li, Hongyang, Bo Dai, Shaoshuai Shi, <strong>Wanli Ouyang</strong>, and Xiaogang Wang. "Feature Intertwiner for Object Detection." ICLR, 2019. [<a class="aLink" href="https://openreview.net/pdf?id=SyxZJn05YX" target="_blank">Full Text</a>]   [<a class="aLink" href="https://github.com/hli2020/feature intertwiner" target="_blank">Source code</a>] [<a class="aLink" href="https://docs.google.com/presentation/d/12Syg5OXD6nGwtG_nwmoQ4kqX5GtJ-5pJ1OuVY53FqB0/edit?usp=sharing" target="_blank"> Slides (50-min talk presented at GTC 2019) </a>]  
                        <br>
											  <br>	
								      </td>
									</tr>       
									



                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px" style="vertical-align: middle"><img alt="OHT_Complexity" border="0" src="./imgs/FishNet.jpg" width="304px" height="160" >
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">
											
											  <br>
									Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, <strong>Wanli Ouyang</strong>, "FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction,"  <em>NuerIPS. (Previously called NIPS)</em>, 2018.
												[<a class="aLink" href="http://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" href="https://github.com/kevin-ssy/FishNet" target="_blank"> Source code </a>]  
                        <br>  <br>
								      </td>
									</tr>       


												
                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px" style="vertical-align: middle"> <img alt="OHT_Complexity" border="0" src="./imgs/Encapsule.jpg" width="304px" height="100" ><img alt="OHT_Complexity" border="0" src="./imgs/FactorizableNet.jpg" width="304px" height="200" >
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">												
Hongyang Li, Bo Dai, <strong>Wanli Ouyang</strong>, Xiaoyang Guo, Xiaogang Wang. "Neural
Network Encapsulation ", <em>Proc. ECCV</em>, 2018.[<a class="aLink" title="Download Code" href=" https://github.com/hli2020/nn_capsulation" target="_blank">Code link</a>] [<a class="aLink" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Hongyang_Li_Neural_Network_Encapsulation_ECCV_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>		
												<br>
Yi Wei, Xinyu Pan, Hongwei Qin, Junjie Yan, <strong>Wanli Ouyang</strong>, "Quantization
Mimic: Towards Very Tiny CNN for Object Detection". <em>Proc. ECCV</em>, 2018. [<a class="aLink" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yi_Wei_Quantization_Mimic_Towards_ECCV_2018_paper.pdf" target="_blank">Full Text</a>] 
                        <br>
											  <br>
Yikang Li, <strong>Wanli Ouyang</strong>, Bolei Zhou, Yanwen Cui, Jianping Shi, Chao Zhang, Xiaogang Wang. "Factorizable Net: An Efficient Subgraph-based Framework for
Scene Graph Generation" <em>Proc. ECCV</em>, 2018. [<a class="aLink" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yikang_LI_Factorizable_Net_An_ECCV_2018_paper.pdf" target="_blank">Full Text</a>]
[<a class="aLink" title="Download Code" href="https://github.com/yikang-li/FactorizableNet" target="_blank">Source code</a>]
                        <br>																				
                        <br>
Guo Lu, <strong>Wanli Ouyang</strong>, Dong Xu, Xiaoyun Zhang, Zhiyong Gao, Ming-Ting Sun, "Deep Kalman Filtering Network for Video Compression Artifact Reduction". <em>Proc. ECCV</em>, 2018.  [<a class="aLink" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Guo_Lu_Deep_Kalman_Filtering_ECCV_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>
											  <br>
Dongang Wang, <strong>Wanli Ouyang</strong>, Wen Li, Dong Xu, "Dividing and Aggregating
Network for Multi-view Action Recognition", <em>Proc. ECCV</em>, 2018.  [<a class="aLink" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Dongang_Wang_Dividing_and_Aggregating_ECCV_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>
											  <br>
Di Chen, Shanshan Zhang, <strong>Wanli Ouyang</strong>, Jian Yang, Ying Tai, "Person Search
via A Mask-guided Two-stream CNN Model". <em>Proc. ECCV</em>, 2018.  [<a class="aLink" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Di_Chen_Person_Search_via_ECCV_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>
																		
								      </td>
									</tr>       
									
											
                  <tr class="pub_tr_2">
                     <td width="20px" class="pub_td_number"> </td>
                   <td width="304px" style="vertical-align: middle"> <img alt="OHT_Complexity" border="0" src="./imgs/OFF.jpg" width="304px" height="200" ></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">  
											
                      	Xuanyi Dong, Yan Yan, <strong>Wanli Ouyang </strong>, Yi Yang. "Style Aggregated Network for Facial Landmark Detection", <em>Proc. CVPR</em>, 2018.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, <strong>Wanli Ouyang</strong>, Xiaogang Wang. "Visual Question Generation as Dual Task of Visual Question Answering", <em>Proc. CVPR</em>, 2018 (spotlight).
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Visual_Question_Generation_CVPR_2018_paper.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" title="Download Code" href="https://github.com/yikang-li/iQAN" target="_blank">Source code</a>]
                        <br>
                        <br>
                      	Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, <strong>Wanli Ouyang</strong>. "Attention-aware Compositional Network for Person Re-Identification", <em>Proc. CVPR</em>, 2018. [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Attention-Aware_Compositional_Network_CVPR_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, <strong>Wanli Ouyang</strong>, Yi Yang. "Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning", <em>Proc. CVPR</em>, 2018.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Wei Yang, <strong>Wanli Ouyang</strong>, Xiaolong Wang, Xiaogang Wang. "3D Human Pose Estimation in the Wild by Adversarial Learning", <em>Proc. CVPR</em>, 2018.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_3D_Human_Pose_CVPR_2018_paper.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Watch Video on Youtube" href="https://youtu.be/LAxVl4zQT-A" target="_blank">Video</a>]
												<br>
                        <br>
                      	Weichen Zhang, <strong>Wanli Ouyang</strong>, Dong Xu, Wen Li. "Collaborative and Adversarial Network for Unsupervised domain adaptation", <em>Proc. CVPR</em>, 2018. (Spotlight) [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Collaborative_and_Adversarial_CVPR_2018_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Shuyang Sun, Zhanghui Kuang, Lu Sheng, <strong>Wanli Ouyang</strong>, Wei Zhang. "Optical Flow Guided Feature: A Motion Representation for Video Action Recognition", <em>Proc. CVPR</em>, 2018.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Optical_Flow_Guided_CVPR_2018_paper.pdf" target="_blank">Full Text</a>]												[<a class="aLink" title="Download Source code" href="https://github.com/kevin-ssy/Optical-Flow-Guided-Feature" target="_blank">Source code</a>]                         
												<br>
                        <br>
                      	Dan Xu, <strong>Wanli Ouyang</strong>, Xiaogang Wang, Nicu Sebe. "PAD-Net: Multi-Tasks Guided Prediciton-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing", <em>Proc. CVPR</em>, 2018.
												[<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PAD-Net_Multi-Tasks_Guided_CVPR_2018_paper.pdf" target="_blank">Full Text</a>] 
                        <br>
                        <br>
                      	Chunfeng Song, Yan Huang, Liang Wang, <strong>Wanli Ouyang</strong>. "Mask-guided Contrastive Attention Model for Person Re-Identification ", <em>Proc. CVPR</em>, 2018. 
												[<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf" target="_blank">Full Text</a>] 
[<a class="aLink" title="Download Code" href="https://github.com/developfeng/MGCAM" target="_blank">Source Code</a>]
                        <br>
                        <br>
                      	Lingbo Liu, HongjunWang, Guanbin Li, <strong>Wanli Ouyang</strong>, Liang Lin, "Crowd Counting using Deep Recurrent Spatial-Aware Network", <em>Proc. IJCAI</em>, 2018. [<a class="aLink" title="Download Full Text" href="https://www.ijcai.org/proceedings/2018/0118.pdf" target="_blank">Full Text</a>] 
								    </td>       
                 </tr>       



                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px"> <img alt="OHT_Complexity" border="0" src="./images/ChainCNN.JPG" width="304px" height="240">
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	<strong>Wanli Ouyang</strong>, Kun Wang, Xin Zhu, Xiaogang Wang. "Chained Cascade Network
for Object Detection", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ouyang_Chained_Cascade_Network_ICCV_2017_paper.pdf" target="_blank">Full Text</a>] [<a class="aLink" title="Download source code" href="https://github.com/wk910930/ccnn" target="_blank">Source code</a>]  	
                        <br>
                        <br>
                      	Wei Yang, Shuang Li, <strong>Wanli Ouyang</strong>, Hongsheng Li, XiaogangWang. "Learning Feature Pyramids for Human Pose Estimation", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yang_Learning_Feature_Pyramids_ICCV_2017_paper.pdf" target="_blank">Full Text</a>] [<a class="aLink" title="Code on Github" href="https://github.com/bearpaw/PyraNet" target="_blank">Code</a>]
                        <br>
                        <br>
                      	Yikang Li, <strong>Wanli Ouyang</strong>, Bolei Zhou, Kun Wang, Xiaogang Wang. "Scene
Graph Generation from Objects, Phrases and Region Captions", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Scene_Graph_Generation_ICCV_2017_paper.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" title="Download Code" href="https://github.com/yikang-li/MSDN" target="_blank">Source code</a>]												
												<br>
                        <br>
                      	Qi Chu, <strong>Wanli Ouyang</strong>, Hongsheng Li, Xiaogang Wang, Bin Liu, Nenghai
Yu. "Online Multi-Object Tracking Using CNN-based Single Object Tracker with
Spatial-Temporal Attention Mechanism", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chu_Online_Multi-Object_Tracking_ICCV_2017_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Dan Xu, <strong>Wanli Ouyang</strong>, Xavier Alameda-Pineda, Elisa Ricci, Xiaogang Wang,
Nicu Sebe. "Learning Deep Structured Multi-Scale Features using Attention-Gated
CRFs for Contour Prediction", <em>Proc. NIPS</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://papers.nips.cc/paper/6985-learning-deep-structured-multi-scale-features-using-attention-gated-crfs-for-contour-prediction.pdf" target="_blank">Full Text</a>]
                       
                 </tr>       




                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px"> <img alt="OHT_Complexity" border="0" src="./images/vip-cnn.jpg" width="304px" height="350">
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	Kai Kang, Hongsheng Li, <strong> W. Ouyang </strong>, Junjie  Yan, Xihui  Liu, Tong  Xiao, Xiaogang  Wang. ”Object Detection in Vidoes with Tubelet Proposal Networks”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/abs/1702.06355" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Feng  Zhu, Hongsheng Li, <strong> W. Ouyang </strong>, Nenghai  Yu, Xiaogang  Wang. ”Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/pdf/1702.05891" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Yu Liu, Junjie  Yan, <strong> W. Ouyang </strong>. ”Quality Aware Network for Set to Set Recognition”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/pdf/1704.03373" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Yikang  LI , <strong> W. Ouyang </strong>, Xiaogang  Wang. ”ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/pdf/1702.07191" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Xiao  Chu, Wei  Yang, <strong> W. Ouyang </strong>, Xiaogang  Wang, Alan  Yuille. ”Multi-Context Attention for Human Pose Estimation”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/pdf/1702.07432" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Source code" href="https://github.com/bearpaw/pose-attention" target="_blank">Code</a>]                        
                        
                        <br>
                        <br>
                      	Dan  Xu, Elisa  Ricci, <strong> W. Ouyang </strong>, Xiaogang  Wang, Nicu Sebe. Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/abs/1704.02157" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Source code" href="https://github.com/danxuhk/ContinuousCRF-CNN" target="_blank">Code</a>]                        
                        [<a class="aLink" title="Presentation" href="https://youtu.be/4mdqh6YGhgE" target="_blank">Presentation</a>]                              <br>
                        <br>
                      	Dan  Xu,  <strong> W. Ouyang </strong>,  Elisa  Ricci, Xiaogang  Wang, Nicu Sebe. Learning Cross-Modal Deep Representations for Robust Pedestrian Detection”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/abs/1704.02431" target="_blank">Full Text</a>]

                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="./images/CRF_CNN.jpg" width="304px" height="65"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	X. Chu, <strong> W. Ouyang </strong>, H. Li, X. Wang. ”CRF-CNN: Modeling Structured Information in Human Pose Estimation”,<em> Advances In Neural Information Processing Systems (NIPS)</em>, 2016.
                        [<a class="aLink" title="Download Full Text" href="./Papers/CRF-CNN_NIPS16" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Demo Results" href="http://www.ee.cuhk.edu.hk/~xchu/files/nips_2016_supp.pdf" target="_blank">Demo Results</a>]
                      </td>
                 </tr>       


                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="./projects/GBD/images/motivation.jpg" width="304px" height="200"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	Xingyu Zeng, <strong>Wanli Ouyang</strong>, Bin Yang, Junjie Yan, Xiaogang
                      	"Gated Bidirectional CNN for Object Detection",  In <em>Proc. ECCV </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="./Papers/GBD.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Z. Wang, H. Li, <strong>W. Ouyang </strong>, X. Wang Wanli
                      	"Learnable Histogram: Statistical Context Features for Deep Neural Networks",  In <em>Proc. ECCV </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="./Papers/HistDNN.pdf" target="_blank">Full Text</a>]
                        
                    </td>
                 </tr>       
                        
                      	
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/ImageNetFactors.jpg" width="304px" height="200"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	<strong>Wanli Ouyang</strong>, X. Wang, C. Zhang, and X. Yang.
                      	"Factors in finetuning deep model for object detection with long-tail distribution",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="./Papers/OuyangFactors_CVPR16.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Wei Yang, <strong>Wanli Ouyang</strong>, Hongsheng Li and Xiaogang Wang
                      	"End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation",  In <em>Proc. CVPR </em> 2016 (Oral). 
                        [<a class="aLink" title=" Download Full Text" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/yangOLWcvpr16.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title=" Project" href="http://www.ee.cuhk.edu.hk/~wyang/Deep-Deformable-Mixture-of-Parts-for-Human-Pose-Estimation/" target="_blank">Project</a>]
												[<a class="aLink" title="Download Source Code" href="https://github.com/bearpaw/eval_pose" target="_blank">Source Code</a>]
												
												
                        <br>
                        <br>
                      	Lijun Wang, <strong>Wanli Ouyang</strong>, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu.
                      	"STCT: Sequentially Training Convolutional Networks for Visual Tracking",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="./Papers/WangLJ_CVPR16.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	K. Kang, <strong>Wanli Ouyang</strong>, H. Li, and X. Wang.
                      	"Object detection from video tubelets with convolutional neural networks",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="./Papers/KangVideoDet_CVPR16.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	X. Chu,  <strong>Wanli Ouyang</strong> , H. Li, and X. Wang. 
                      	"Structured feature learning for pose estimation",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="http://arxiv.org/pdf/1603.09065.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Project" href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_structured_feature_pose.html" target="_blank">Project and dataset </a>]                          
                        [<a class="aLink" title="Project" href="https://www.youtube.com/watch?v=SMFt6TJ-ntA" target="_blank">Spotlight talk</a>]                          
                        [<a class="aLink" title="Code" href="https://github.com/chuxiaoselena/StructuredFeature" target="_blank">Source code </a>]                          
                        [<a class="aLink" title="Supplementary" href="http://www.ee.cuhk.edu.hk/~xgwang/StructureFeature/supp.pdf" target="_blank">Supplementary </a>]                          
                          <br>
                        <br>
                      	Tong Xiao, Hongsheng Li, <strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Full Text on Arxiv" href="http://arxiv.org/abs/1604.07528" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Hongyang Li, <strong>Wanli Ouyang</strong>, Xiaogang Wang
                      	"Multiple Bias on Non-linearity Activation in Deep Neural Networks",  In <em>Proc. ICML </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="./MBA_icml16.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Slides" href="http://www.ee.cuhk.edu.hk/~yangli/icml16_bias.pdf" target="_blank">Slides </a>]                          
                        [<a class="aLink" title="Code" href="https://github.com/hli2020/caffe/tree/bias" target="_blank">Code on Github </a>]                          
                      	
                    </td>
                 </tr>       



                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="projects/ImageNetAttribute/dataset.png" width="304px" height="120"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	<strong>Wanli Ouyang</strong>, Hongyang Li, Xingyu Zeng, and Xiaogang Wang,
                      	"Learning Deep Representation with Large-scale Attributes",  In <em>Proc. ICCV </em> 2015. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_ICCV&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_ICCV&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="./Papers/ImageNetAttributes_ICCV15.pdf" target="_blank">Full Text</a>]  
                        [<a class="aLink" title="Project" href="./projects/ImageNetAttribute/iccv15.html" target="_blank">Project and dataset </a>]                          
                        <br>
                        <div id="divAbstract2015_ICCV" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        Learning strong feature representations from large scale supervision has achieved remarkable success in computer
vision as the emergence of deep learning techniques. It is
driven by big visual data with rich annotations. This paper
contributes a large-scale object attribute database that
contains rich attribute annotations (over 300 attributes)
for ∼180k samples and 494 object classes. Based on the
ImageNet object detection dataset, it annotates the rotation,
viewpoint, object part location, part occlusion, part
existence, common attributes, and class-specific attributes.
Then we use this dataset to train deep representations and
extensively evaluate how these attributes are useful on the
general object detection task. In order to make better use
of the attribute annotations, a deep learning scheme is proposed
by modeling the relationship of attributes and hierarchically
clustering them into semantically meaningful mixture
types. Experimental results show that the attributes are
helpful in learning better features and improving the object
detection accuracy by 2.6% in mAP on the ILSVRC 2014
object detection dataset and 2.4% in mAP on PASCAL VOC
2007 object detection dataset. Such improvement is well
generalized across datasets.
                            <br></div>
                        <div id="divBibTex2015_ICCV" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@CONFERENCE{ouyang2015learning,
   <br> title={Learning Deep Representation With Large-Scale Attributes},
   <br>author={Ouyang, Wanli and Li, Hongyang and Zeng, Xingyu and Wang, Xiaogang},
   <br>booktitle={Proceedings of the IEEE International Conference on Computer Vision},
   <br>pages={1895--1903},
   <br> year={2015}
                        <br>}</code></div>
                        </td></tr>                        
               
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/Immediacy.PNG" width="304px" height="150"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	Xiao Chu, <strong>Wanli Ouyang</strong>, Wei Yang, and Xiaogang Wang,
                      	"Multi-task Recurrent Neural Network for Immediacy Prediction",  In <em>Proc. ICCV </em> 2015.<strong> (Oral) </strong>
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_ICCVXchu&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_ICCVXchu&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="./Papers/Chu_Multi-Task_Recurrent_Neural_ICCV_2015_paper.pdf" target="_blank">Full Text</a>]  
                        [<a class="aLink" title="Project" href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_immediacy.html" target="_blank">Project and dataset </a>]                          
                        [<a class="aLink" title="Oral" href="http://videolectures.net/iccv2015_chu_neural_network/" target="_blank">Oral presentation on videolectures</a>]                          
                        [<a class="aLink" title="Poster" href="http://www.ee.cuhk.edu.hk/~xchu/files/iccv_2015_poster.pdf" target="_blank">Poster</a>]                          
                        <br>
                        <div id="divAbstract2015_ICCVXchu" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        In this paper, we propose to predict immediacy for interacting persons from still images. A complete immediacy
set includes interactions, relative distance, body leaning direction and standing orientation. These measures are found
to be related to the attitude, social relationship, social interaction, action, nationality, and religion of the communicators. A large-scale dataset with 10, 000 images is
constructed, in which all the immediacy cues and the human poses are annotated. We propose a rich set of immediacy representations that help to predict immediacy from
imperfect 1-person and 2-person pose estimation results. A
multi-task deep recurrent neural network is constructed to
take the proposed rich immediacy representations as the input and learn the complex relationship among immediacy
predictions through multiple steps of refinement. The effectiveness of the proposed approach is proved through extensive experiments on the large-scale dataset.

                            <br></div>
                        <div id="divBibTex2015_ICCVXchu" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@inproceedings{chu2015multi,
  <br>title={Multi-Task Recurrent Neural Network for Immediacy Prediction},
  <br>author={Chu, Xiao and Ouyang, Wanli and Yang, Wei and Wang, Xiaogang},
  <br>booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  <br>pages={3352--3360},
  <br>year={2015}
<br>}</code></div>
</td></tr>                        
             

                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/Tracking_ICCV15.png" width="304px" height="150"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	Lijun Wang,  <strong>Wanli Ouyang</strong>, Xiaogang Wang, and Huchuan Lu,
                      	"Visual Tracking with Fully Convolutional Networks",  In <em>Proc. ICCV </em> 2015. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_ICCVWang&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_ICCVWang&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="Papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf" target="_blank">Full Text</a>]  
                      [<a class="aLink" title="Project" href="http://scott89.github.io/FCNT/" target="_blank">Project and source code </a>]                
                        <br>
                        <div id="divAbstract2015_ICCVWang" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        We propose a new approach for general object tracking
with fully convolutional neural network. Instead of treating
convolutional neural network (CNN) as a black-box feature
extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and
classification task on ImageNet. The discoveries motivate
the design of our tracking system. It is found that convolutional layers in different levels characterize the target from
different perspectives. A top layer encodes more semantic
features and serves as a category detector, while a lower
layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism
during tracking. It is also found that for a tracking target,
only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy
and improve tracking accuracy. Extensive evaluation on the
widely used tracking benchmark [36] shows that the proposed tacker outperforms the state-of-the-art significantly.
                            <br></div>
                        <div id="divBibTex2015_ICCVWang" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@inproceedings{wang2015visual,
  <br>title={Visual Tracking With Fully Convolutional Networks},
  <br>author={Wang, Lijun and Ouyang, Wanli and Wang, Xiaogang and Lu, Huchuan},
  <br>booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  <br>pages={3119--3127},
  <br>year={2015}
<br>}</code></div>
</td></tr>                        

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/overview_DeepID.png" width="304px" height="120"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <strong>Details that show how our team achieve #2 in the ImageNet Large Scale Visual Recognition Challenge 2014: </strong> <br>
                      	<strong>Wanli Ouyang</strong>, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping Luo, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy and Xiaoou Tang,
                      	"DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection",  In <em>Proc. CVPR </em> 2015. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_CVPR1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_CVPR1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="./Papers/DeepIDNet_CVPR15.pdf" target="_blank">Full Text</a>]  
                        [<a class="aLink" title="Project" href="./projects/ImageNet/index.html" target="_blank">Project</a>]                          
                        <br>
                        <div id="divAbstract2015_CVPR1" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        In this paper, we propose deformable deep convolutional
neural networks for generic object detection. This new deep
learning object detection diagram has innovations in multiple
aspects. In the proposed new deep architecture, a
new deformation constrained pooling (def-pooling) layer
models the deformation of object parts with geometric constraint
and penalty. A new pre-training strategy is proposed
to learn feature representations more suitable for the object
detection task and with good generalization capability. By
changing the net structures, training strategies, adding and
removing some key components in the detection pipeline,
a set of models with large diversity are obtained, which
significantly improves the effectiveness of model averaging.
The proposed approach improves the mean averaged precision
obtained by RCNN [13], which is the state-of-the-art,
from 31% to 50.3% on the ILSVRC2014 detection dataset.
Detailed component-wise analysis is also provided through
extensive experimental evaluation, which provide a global
view for people to understand the deep learning object detection
pipeline.
                        <br></div>
                        <div id="divBibTex2015_CVPR1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
				@CONFERENCE{ouyang2015deepid,
				  <br>author = {Ouyang, Wanli and Wang, Xiaogang and Zeng, Xingyu and Qiu, Shi and Luo, Ping and Tian, Yonglong and Li, Hongsheng and Yang, Shuo and Wang, Zhe and Loy, Chen-Change, and Tang, Xiaoou},
				  <br>title = {DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection},
				  <br>booktitle = {CVPR},
				  <br>year = {2015},
                        <br>}</code></div>
                        </td></tr>                        
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/ZhaoCVPR15.JPG" width="304px" height="120"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">  <br>
                      	Rui Zhao, <strong>Wanli Ouyang</strong>,  Hongsheng Li, and Xiaogang Wang,
                      	"Saliency Detection by Multi-context Deep Learning",  In <em>Proc. CVPR </em> 2015. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_CVPR2&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_CVPR2&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~rzhao/project/deepsal_cvpr15/zhaoOLWcvpr15.pdf" target="_blank">Full Text</a>]  
                        [<a class="aLink" title="Code" href="https://github.com/Robert0812/deepsaldet" target="_blank">Code</a>]                          
                        [<a class="aLink" title="Supplementary Material" href="http://www.ee.cuhk.edu.hk/~rzhao/project/deepsal_cvpr15/zhaoOLWcvpr15.html" target="_blank">Supplementary Material</a>]                          
                        <br>
                        <div id="divAbstract2015_CVPR2" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        Low-level saliency cues or priors do not produce good
enough saliency detection results especially when the
salient object presents in a low-contrast background with
confusing visual appearance. This issue raises a serious
problem for conventional approaches. In this paper, we
tackle this problem by proposing a multi-context deep learning
framework for salient object detection. We employ deep
Convolutional Neural Networks to model saliency of objects
in images. Global context and local context are both taken
into account, and are jointly modeled in a unified multicontext
deep learning framework.

To provide a better initialization for training the deep
neural networks, we investigate different pre-training
strategies, and a task-specific pre-training scheme is designed
to make the multi-context modeling suited for
saliency detection. Furthermore, recently proposed contemporary
deep models in the ImageNet Image Classification
Challenge are tested, and their effectiveness in saliency detection
are investigated. Our approach is extensively evaluated
on five public datasets, and experimental results show
significant and consistent improvements over the state-ofthe-art
methods

                        <br></div>                        
                         <div id="divBibTex2015_CVPR2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
				@CONFERENCE{ouyang2015deepid,
  <br>title={Saliency Detection by Multi-Context Deep Learning},
  <br>author={Zhao, Rui and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
	<br>booktitle = {CVPR},
  <br>year={2015}
                        <br>}</code></div>
                        </td></tr>                        
                        
                                               
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <!--<img alt="OHT_Complexity" border="0" src="imgs/ZengECCV14.jpg" width="304px" height="256px">--></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">Xinyu Zeng, <strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Deep Learning of Scene-Specific Classifier for Pedestrian Detection", In <em>Proc. ECCV </em> 2014.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://link.springer.com/chapter/10.1007/978-3-319-10578-9_31" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV1" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        	The performance of a detector depends much on its training
dataset and drops significantly when the detector is applied to a new scene due to the large variations between the source training dataset
and the target scene. In order to bridge this appearance gap, we propose a deep model to automatically learn scene-specific features and
visual patterns in static video surveillance without any manual labels from the target scene. It jointly learns a scene-specific classifier and the
distribution of the target samples. Both tasks share multi-scale feature
representations with both discriminative and representative power. We also propose a cluster layer in the deep model that utilizes the scene-specific visual patterns for pedestrian detection. Our specifically designed
objective function not only incorporates the condence scores of target training samples but also automatically weights the importance of source
training samples by fitting the marginal distributions of target samples.
It significantly improves the detection rates at 1 FPPI by 10% compared with the state-of-the-art domain adaptation methods on MIT TrafficDataset and CUHK Square Dataset.                        	
                        <br></div>
                        <div id="divBibTex2013_ICCV1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Zeng2014Deep,
			  <br>author = {X. Zeng and W. Ouyang and X. Wang},
			  <br>title = {Deep Learning of Scene-Specific Classifier for Pedestrian Detection},
			  <br>booktitle = {ECCV},
			  <br>year = {2014},
                        <br>}</code></div>
                        </td></tr>
                        
                                            	
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/PoseDBN.PNG" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Xiao Chu, Xiaogang Wang, 
                      	"Multi-source Deep Learning for Human Pose Estimation", In <em>Proc. IEEE CVPR </em> 2014.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/ouyangCWcvpr14.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV1" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        	Visual appearance score, appearance mixture type and deformation are three important information sources for human pose estimation. This paper proposes to build a multi-source deep model  in order to extract non-linear representation from these different aspects of information sources. With the deep model, the global, high-order human body articulation patterns in these information sources are extracted for pose estimation. The task for estimating body locations and the task for human detection are jointly learned using a unified deep model. The proposed approach can be viewed as a post-processing of pose estimation results and can flexibly integrate with existing methods by taking their information sources as input.
By extracting the non-linear representation from multiple information sources, the deep model outperforms  state-of-the-art by up to 8.6 percent on three public benchmark datasets.
                        	
                        <br></div>
                        <div id="divBibTex2013_ICCV1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang2014Multi,
			  <br>author = {W. Ouyang and X. Chu and X. Wang},
			  <br>title = {Multi-source Deep Learning for Human Pose Estimation},
			  <br>booktitle = {CVPR},
			  <br>year = {2014},
                        <br>}</code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/ZhaoCVPR14.JPG" width="304px" height="180px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> Rui Zhao,  <strong>Wanli Ouyang</strong>, and Xiaogang Wang, 
                      	"Learning Mid-level Filters for Person Re-Identfiation", In <em>Proc. IEEE CVPR </em> 2014.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2014_CVPR2&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2014_CVPR2&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project page" href="http://www.ee.cuhk.edu.hk/~rzhao/project/midfilter_cvpr14/zhaoOWcvpr14.html" target="_blank">Project</a>]  
                        [<a class="aLink" title="Download Code" href="https://github.com/Robert0812/midfilter_reid" target="_blank">Code</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~rzhao/project/midfilter_cvpr14/zhaoOWcvpr14.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2014_CVPR2" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        	In this paper, we propose a novel approach of learning
mid-level filters from automatically discovered patch clusters
for person re-identification. It is well motivated by our
study on what are good filters for person re-identification.
Our mid-level filters are discriminatively learned for identifying
specific visual patterns and distinguishing persons,
and have good cross-view invariance. First, local patches
are qualitatively measured and classified with their discriminative
power. Discriminative and representative patches
are collected for filter learning. Second, patch clusters with
coherent appearance are obtained by pruning hierarchical
clustering trees, and a simple but effective cross-view
training strategy is proposed to learn filters that are viewinvariant
and discriminative. Third, filter responses are integrated
with patch matching scores in RankSVM training.
The effectiveness of our approach is validated on the VIPeR
dataset and the CUHK01 dataset. The learned mid-level
features are complementary to existing handcrafted lowlevel
features, and improve the best Rank-1 matching rate
on the VIPeR dataset by 14%
                        	
                        <br></div>
                        <div id="divBibTex2014_CVPR2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			<br>@CONFERENCE{Ouyang2014Multi,
 <br>title = {Learning Mid-level Filters for Person Re-identfiation},
 <br>author={Zhao, Rui and Ouyang, Wanli and Wang, Xiaogang},
 <br>booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 <br>year = {2014},
 <br>month = {June},
 <br>address = {Columbus, USA}
                         <br>}</code></div>
                        </td></tr>

                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/jointdeep.PNG" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Joint Deep Learning for Pedestrian Detection ", In <em>Proc. IEEE ICCV </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="./projects/ouyangWiccv13Joint/index.html" target="_blank">Project & Source code</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/ouyangWiccv13.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV1" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> Feature extraction, deformation handling, occlusion
handling, and classification are four important components
in pedestrian detection. Existing methods learn or design
these components either individually or sequentially. The
interaction among these components is not yet well explored.
This paper proposes that they should be jointly
learned in order to maximize their strengths through cooperation.
We formulate these four components into a joint
deep learning framework and propose a new deep network
architecture. By establishing automatic, mutual interaction
among components, the deep model achieves a 9% reduction
in the average miss rate compared with the current
best-performing pedestrian detection approaches on
the largest Caltech benchmark dataset.
                        	
                        <br></div>
                        <div id="divBibTex2013_ICCV1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang2013Joint,
			  <br>author = {W. Ouyang and X. Wang},
			  <br>title = {Joint Deep Learning for Pedestrian Detection},
			  <br>booktitle = {ICCV},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <!--<img alt="OHT_Complexity" border="0" src="imgs/cascade.PNG" width="304px" height="256px">--></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> Xingyu Zeng, <strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Multi-Stage Contextual Deep Learning for Pedestrian Detection ", In <em>Proc. IEEE ICCV </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV2&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV2&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="" target="_blank">Project </a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/zengOWiccv13.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV2" class="pub_div_abstract"><br><b>Abstract:</b> 
                        	Multi-Stage classifiers have been widely used in pedestrian
detection and achieved great success. However, these
classifiers are usually trained sequentially without joint optimization.
In this paper, we propose a new deep architecture
that can jointly train multiple classifiers through several
stages of back-propagation. Unsupervised pre-training
and specifically designed stage-wise supervised training are
used to regularize the optimization problem. Through a specific
design of the training scheme, this deep architecture
is able to simulate the cascaded classifiers in using hard
samples to train the network stage-by-stage. Both theoretical
analysis and experimental results show that the training
scheme helps to avoid overfitting. Experimental results
on three datasets(Caltech, ETH and TudBrussels) show that
our approach performs better than the state-of-the-art approaches.                        	
                        <br></div>
                        <div id="divBibTex2013_ICCV2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Zeng2013Multi,
			  <br>author = {X. Zeng and W. Ouyang and X. Wang},
			  <br>title = {Multi-Stage Contextual Deep Learning for Pedestrian Detection},
			  <br>booktitle = {ICCV},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <!--<img alt="OHT_Complexity" border="0" src="imgs/pedestrian_salience2.PNG" width="304px" height="256px">--></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> Rui Zhao, <strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Person Re-identification by Salience Matching ", In <em>Proc. IEEE ICCV </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV3&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV3&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="" target="_blank">Project</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~rzhao/papers/zhaoOWiccv13.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV3" class="pub_div_abstract"><br><b>Abstract:</b> 
Human salience is distinctive and reliable information
in matching pedestrians across disjoint camera views. In
this paper, we exploit the pairwise salience distribution relationship
between pedestrian images, and solve the person
re-identification problem by proposing a salience matching
strategy. To handle the misalignment problem in pedestrian
images, patch matching is adopted and patch salience
is estimated. Matching patches with inconsistent salience
brings penalty. Images of the same person are recognized
by minimizing the salience matching cost. Furthermore, our
salience matching is tightly integrated with patch matching
in a unified structural RankSVM learning framework.
The effectiveness of our approach is validated on the VIPeR
dataset and the CUHK Campus dataset. It outperforms the
state-of-the-art methods on both datasets.
                        <br></div>
                        <div id="divBibTex2013_ICCV3" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Zhao2013Person,
			  <br>author = {R. Zhao and W. Ouyang and X. Wang},
			  <br>title = {Person Re-identification by Salience Matching},
			  <br>booktitle = {ICCV},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>
                                                
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="two_pedestrian.PNG" width="304px" height="156px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Single-Pedestrian Detection aided by Multi-pedestrian Detection ", In <em>Proc. IEEE CVPR </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_CVPR1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_CVPR1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="./projects/ouyangWcvpr13MultiPed/index.html" target="_blank">Project & Source code</a>]  
                        [<a class="aLink" title="Download Full Text" href="./Papers/Ouyang2013MultiPed.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_CVPR1" class="pub_div_abstract"><br><b>Abstract:</b> 
                        	In this paper, we address the challenging problem of detecting
pedestrians who appear in groups and have interaction.
A new approach is proposed for single-pedestrian
detection aided by multi-pedestrian detection. A mixture
model of multi-pedestrian detectors is designed to capture
the unique visual cues which are formed by nearby multiple
pedestrians but cannot be captured by single-pedestrian
detectors. A probabilistic framework is proposed to model
the relationship between the configurations estimated by
single- and multi-pedestrian detectors, and to refine the
single-pedestrian detection result with multi-pedestrian detection.
It can integrate with any single-pedestrian detector
without significantly increasing the computation load. 15
state-of-the-art single-pedestrian detection approaches are
investigated on three widely used public datasets: Caltech,
TUD-Brussels and ETH. Experimental results show that our
framework significantly improves all these approaches. The
average improvement is 9% on the Caltech-Test dataset,
11% on the TUD-Brussels dataset and 17% on the ETH
dataset in terms of average miss rate. The lowest average
miss rate is reduced from 48% to 43% on the Caltech-Test
dataset, from 55% to 50% on the TUD-Brussels dataset and
from 51% to 41% on the ETH dataset.
                        <br></div>
                        <div id="divBibTex2013_CVPR1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang2013MultiPed,
			  <br>author = {W. Ouyang and X. Wang},
			  <br>title = {Single-Pedestrian Detection aided by Multi-pedestrian Detection},
			  <br>booktitle = {CVPR},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="mutual_visibility.PNG" width="304px" height="206px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Xingyu Zeng and Xiaogang Wang, 
                      	"Modeling Mutual Visibility Relationship in Pedestrian Detection ", In <em>Proc. IEEE CVPR </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_CVPR2&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_CVPR2&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="./projects/ouyangZWcvpr13MutVisibility/index.html" target="_blank">Project</a>]  
                        [<a class="aLink" title="Download Full Text" href="./Papers/Ouyang2013MutualDBN.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_CVPR2" class="pub_div_abstract"><br><b>Abstract:</b> Detecting pedestrians in cluttered scenes is a challenging
problem in computer vision. The difficulty is
added when several pedestrians overlap in images and occlude
each other. We observe, however, that the occlusion/
visibility statuses of overlapping pedestrians provide
useful mutual relationship for visibility estimation - the visibility
estimation of one pedestrian facilitates the visibility
estimation of another. In this paper, we propose a mutual
visibility deep model that jointly estimates the visibility
statuses of overlapping pedestrians. The visibility relationship
among pedestrians is learned from the deep model
for recognizing co-existing pedestrians. Experimental results
show that the mutual visibility deep model effectively
improves the pedestrian detection results. Compared with
existing image-based pedestrian detection approaches, our
approach has the lowest average miss rate on the Caltech-
Train dataset, the Caltech-Test dataset and the ETH dataset.
Including mutual visibility leads to 4%−8% improvements
on multiple benchmark datasets.
                        <br></div>
                        <div id="divBibTex2013_CVPR2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang2013MutualDBN,
			  <br>author = {Wanli Ouyang and Xingyu Zeng and Xiaogang Wang},
			  <br>title = {Modeling Mutual Visibility Relationship in Pedestrian Detection},
			  <br>booktitle = {CVPR},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="pedestrian_salience.PNG" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> Rui Zhao <strong>Wanli Ouyang</strong>, and Xiaogang Wang, 
                      	"Unsupervised Salience Learning for Person Re-identification ", In <em>Proc. IEEE CVPR </em> 2013.
                        [<a class="aLink" title="Project webpage" href="http://mmlab.ie.cuhk.edu.hk/projects/project_salience_reid/index.html" target="_blank">Project & source code</a>]  
                        [<a class="aLink" title="Download Full Text" href="./Papers/ZhaoCVPR2013Salience.pdf" target="_blank">PDF</a>]  
                        </code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <!--<img border="0" alt="HumanDBNDetRes" src="HumanDBNDetRes.jpg" width="304px" height="200px">--> </td>
                      <td class="pub_td_text"  width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong> and Xiaogang Wang, 
                      	"A Discriminative Deep Model for Pedestrian Detection with Occlusion Handling," 
                      	In <em>Proc. IEEE CVPR </em> 2012.                      	
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2012_CVPR&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2012_CVPR&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="./projects/ouyangWcvpr12Occ/index.html" target="_blank">Project</a>]  
			[<a class="aLink" href="./Papers/CDBN-Ped12.pdf" target="_blank">  Full Text</a>]
			[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~xgwang/CUHK_pedestrian.html" target="_blank">CUHK Occlusion Dataset</a>]
<!--                          [<a class="aLink" title="Download Poster" href="./2009_icip_poster.pdf" target="_blank">Poster</a>]
                        [<a class="aLink" title="Download Full Text" href="./2009_icip.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Errata" href="./2009_icip_errata.pdf" target="_blank">Errata</a>]     
-->                        <br>
                        <div id="divAbstract2012_CVPR" class="pub_div_abstract"><br><b>Abstract:</b> Part-based models have demonstrated their merit in object detection.
			However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions or large
			deformations. To handle the imperfectness of part detectors, this paper presents a probabilistic pedestrian detection framework.
			In this framework, a deformable part-based model is used to obtain the scores of part detectors and the visibilities of parts
			are modeled as hidden variables. Unlike previous occlusion handling approaches that assume independence among visibility probabilities of parts
			or manually define rules for the visibility relationship,
			a discriminative deep model is used in this paper for learning the visibility relationship among overlapping parts at multiple layers.
			Experimental results on three public datasets (Caltech, ETH and Daimler) and a new dataset (the new dataset will be released to the public)
			specially designed for the evaluation of occlusion handling approaches show the effectiveness of the proposed approach.
                        <br></div>
                        <div id="divBibTex2012_CVPR" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @CONFERENCE{Ouyang:DBNHuman,
			 <br>author = {Wanli Ouyang and Xiaogang Wang},
			 <br>title = {A Discriminative Deep Model for Pedestrian Detection with Occlusion
			 <br>Handling},
			 <br>booktitle = {CVPR},
			 <br>year = {2012},
			}</code></div>
                        </td></tr>
                                                                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="mainpa9.gif" width="304px" height="206px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Renqi Zhang and Wai-Kuen Cham, 
                      	"Fast pattern matching using orthogonal Haar transform ", In <em>Proc. IEEE CVPR </em> 2010.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2011_iscas&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2011_iscas&#39;)">BibTeX</a>]
			[<a class="aLink" href="./OHT.htm" target="_blank">Project & source code</a>]
                        [<a class="aLink" title="Download Slides" href="./PPT/OHT1.ppt" target="_blank">Slides</a>]
                        [<a class="aLink" title="Download Full Text" href="./Papers/Manu_OHT.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2011_iscas" class="pub_div_abstract"><br><b>Abstract:</b> This paper introduces 
                        	strip sum on the image. The sum of pixels in a rectangle can be computed by 
                        	one addition using the strip sum. Then we propose to use the orthogonal Haar 
                        	transform (OHT) for pattern matching. Applied for pattern matching, 
                        the algorithm using strip sum requires <i>O</i>(log <i>u</i>) additions per pixel to 
                        	project input data of size <i>N</i> by <i>N</i> onto <i>u</i> 2-D OHT basis while existing fast algorithms require 
                        	<i>O</i>(<i>u</i>) additions per pixel to project the same data onto <i>u</i> 2-D WHT or GCK basis.
                        <br></div>
                        <div id="divBibTex2011_iscas" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang:OHT,
			  <br>author = {W. Ouyang and R. Zhang and W.K. Cham},
			  <br>title = {Fast pattern matching using orthogonal {H}aar transform},
			  <br>booktitle = {CVPR},
			  <br>year = {2010},
                        <br>}</code></div>
                        </td></tr>
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img border="0" alt="Img_coded" src="mainpa12.jpg" width="147px" height="161px"><img border="0"  alt="Img_filtered" src="mainpa13.jpg" width="147px" height="161px"> </td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">Renqi Zhang, <strong>Wanli Ouyang</strong> and Wai-Kuen Cham, 
                      	"Image Deblocking using Dual Adaptive FIR Wiener Filter in the DCT Transform Domain," 
                      	<em>In Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</em>, ICASSP 2009, 
                      	Taiwan, April 19-24, 2009, pp.1181-1184.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2009_icip&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2009_icip&#39;)">BibTeX</a>]
<!--                        [<a class="aLink" title="Download Errata" href="./2009_icip_errata.pdf" target="_blank">Errata</a>]     
-->                        <br>
                        <div id="divAbstract2009_icip" class="pub_div_abstract"><br><b>Abstract:</b> Blocking artifacts exist in images and video sequences
						compressed to low bit rates using block discrete cosine
						transform (DCT) compression standards. In order to reduce
						blocking artifacts, a novel DCT domain technique is
						presented in this paper. Firstly, a new FIR Wiener filter
						which exploits the dependence of neighboring DCT
						coefficients based on the linear minimum
						mean-square-error (LMMSE) criterion is proposed. Then
						we apply the new FIR Wiener filter twice in a dual adaptive
						filtering structure to restore each quantized DCT coefficient.
						In addition, an efficient parameter estimation method is
						proposed for the designed filter. Experimental results show
						that the performance of the proposed method is comparable
						to the state-of-the-art methods but has low computational
						complexity.
                        <br></div>
                        <div id="divBibTex2009_icip" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @CONFERENCE{Zhangrq:WienerDeblkICAASP,
				  <br>author = {Renqi Zhang and Wanli Ouyang and W.K. Cham},
				  <br>title = {Image Deblocking using Dual Adaptive FIR Wiener Filter in the {DCT} Transform Domain},
				  <br>booktitle = {Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)},
				  <br>year = {2009},
				  <br>pages = {1181-1184},
				  <br>address = {Taiwan},
			  <br>month = {April 19-24},
                        <br>}</code></div>
                        </td></tr>
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img border="0" alt="Img_ori" src="mainpa14.jpg" width="100px" height="101px"><img border="0"  alt="Img_canny" src="mainpa15.gif" width="100px" height="101px"> <img border="0"  alt="Img_our3DEdge" src="mainpa16.gif" width="100px" height="101px">  </td>
                      <td class="pub_td_text">Renqi Zhang, <strong>Wanli Ouyang</strong> and Wai-Kuen Cham, 
                      	"Image Multi-scale Edge Detection using 3-D Hidden Markov Model based on the Non-decimated Wavelet," 
                      	In <em> Proc. 2009 IEEE International Conference on Image Processing</em> (ICIP), 
                      	Cairo, Egypt, November 7-10, 2009, pp.2173-2176.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2007_iscas&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2007_iscas&#39;)">BibTeX</a>]
<!--                        [<a class="aLink" title="Download Slides" href="./2007_iscas_lecture.pdf" target="_blank">Slides</a>]
                        [<a class="aLink" title="Download Full Text" href="./2007_iscas.pdf" target="_blank">Full Text</a>] -->
                        <br>
                        <div id="divAbstract2007_iscas" class="pub_div_abstract"><br><b>Abstract:</b> Edge detection plays an important role in digital image
			processing. Based on the non-decimated wavelet which is
			shift-invariant, in this paper, we develop a new edge
			detecting technique using 3-D Hidden Markov Model.
			Our proposed model can not only capture the relationship
			of the wavelet coefficients inter-scale, but also consider
			the intra-scale dependence. A computationally efficient
			maximum likelihood (ML) estimation algorithm is
			employed to compute parameters and the hidden state of
			each coefficient is revealed by maximum a posteriori
			(MAP) estimation. Experimental results of natural images
			are provided to evaluate the algorithm. In addition, the
			proposed model has the potential to be an efficient
			multi-scale statistical modeling tool for other image or
			video processing tasks.
                        <br></div>
                        <div id="divBibTex2007_iscas" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @CONFERENCE{Renqi:Edge,
			  <br>author = {Renqi Zhang and Wanli Ouyang and Wai-Kuen Cham},
			  <br>title = {Image Multi-scale Edge Detection using 3-D Hidden Markov Model based
			  <br>on the Non-decimated Wavelet},
			  <br>booktitle = {Proc. 2009 IEEE International Conference on Image Processing (ICIP)},
			  <br>year = {2009},
			  <br>}</code></div>
                        </td></tr>
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
		      <td width="304px"> </td> 
                      <td class="pub_td_text"> <strong>Wanli Ouyang</strong>, D. Song, C. Xiao, and W. Ju. 
                      	The matrix decomposition representation of DCT algorithms. 
                      	In <em>IEEE midwest sym. Circuits and Syst.</em> (MWCAS), 2005.
                        <br>            
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
		      <td width="304px"> </td> 
                      <td class="pub_td_text"> <strong>Wanli Ouyang</strong>, C. Xiao, W. Ju, and D. Song. The dynamic range acquisition of DCT and IDCT algorithms. 
                      	In <em>IEEE midwest sym. Circuits and Syst. </em> (MWCAS), 2005.
                        <br>            
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
		      <td width="304px"> </td> 
                      <td class="pub_td_text"> <strong>Wanli Ouyang</strong>, C. Xiao, W. Ju, and D. Song. Practical fast asymmetric DCT algorithm based on SIMD and VLIW. 
                      	In <em>IEEE Int. Sym. Intelligent Signal Processing </em>, 2005.
                        <br>            
                        
                    <tr class="pub_tr_1">  
                      <td width="20%" align="left" colspan="2" style="HEIGHT:10px"></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="20%" align="left" colspan="2"><a class="aLink" href="./#Top">Back To Top</a></td></tr>
                    </tbody></table>
</div>
                    
</td></tr></tbody></table>
<!--================================End of Body of Windows Body====================================-->
<!--================================Begin of Footer of Windows Body====================================-->
      <center>
      <table border="0" cellspacing="0" cellpadding="0" width="900px" style="BACKGROUND-IMAGE: url(footer_bg.jpg); COLOR: #2b547e">
        <tbody>
        <tr style="VERTICAL-ALIGN: middle">
<!--          <td style="TEXT-ALIGN: right; WIDTH: 200px; HEIGHT: 30px">
          <img alt="AVSX.org" src="avsx_logo_small.gif"></td> -->
          <td class="i_td_windows_footer">
          Last Update: Apr. 2013. Copyright © 2013-. 
          <a href="http://validator.w3.org/check?uri=./saved_resource.htm">
          <img style="BORDER-BOTTOM: 0px; BORDER-LEFT: 0px; WIDTH: 45px; HEIGHT: 15px; BORDER-TOP: 0px; BORDER-RIGHT: 0px" alt="Valid HTML 4.01 Transitional" src="valid-html401-blue.png"></a> 
          <a href="http://jigsaw.w3.org/css-validator/validator?uri=./saved_resource.htm">
          <img style="BORDER-BOTTOM: 0px; BORDER-LEFT: 0px; WIDTH: 45px; HEIGHT: 15px; BORDER-TOP: 0px; BORDER-RIGHT: 0px" alt="Valid CSS!" src="valid-css-blue.png"></a>
            </td>
        </tr>
        </tbody>
      </table>
      </center>
<!--================================End of Footer of Windows Body====================================-->
	  </center>
    </td>
  </tr>
  </tbody>
</table>



</body></html>