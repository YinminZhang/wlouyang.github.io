<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- saved from url=(0022)https://wlouyang.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Wanli Ouyang - Homepage</title>

<!--- <link rel="shortcut icon" type="image/ico" href="http://www.interdigital.com/idsystem/favicon.ico"><link rel="stylesheet" type="text/css" href="style.css" media="screen"> -->
 <script type="text/javascript" src="jsript.js"></script>  

<style type="text/css">@import url( style.css );
DIV.b-mobile {
	DISPLAY: none
}
</style>

<meta name="GENERATOR" content="MSHTML 8.00.6001.18852">
</head>

<body style="VERTICAL-ALIGN: middle">
<table border="0" cellspacing="0" cellpadding="0" width="100%">
  <tbody>
  <tr>
    <td><a name="Top"></a>
      <center>
      <!--================================Begin of Header of Windows Body====================================-->
      <table style="BACKGROUND-IMAGE: url(wave.jpg); BACKGROUND-REPEAT: no-repeat;BACKGROUND-POSITION:center" border="0" cellspacing="0" cellpadding="0" width="100%">
        <tbody>
        <tr>
          <td>
            <center>
            <table style="TEXT-ALIGN: left; WIDTH: 900px" border="0" cellspacing="0" cellpadding="0">
              <tbody>
              <tr>
                <td class="item"><a href="#Top">Home</a></td>
                <td class="item" style="WIDTH: 40px"></td>
                <td class="item"><a href="#Biography">Biography</a></td>
                <td class="item" style="WIDTH: 40px"></td>
                <td class="item"><a href="#Education">Education</a></td>
                <td class="item" style="WIDTH: 40px"></td>
                <td class="item"><a href="#Research">Research</a></td>
                <td class="item" style="WIDTH: 40px"></td>
<!--                <td class="item"><a href="http://www.h265.net/" target="_blank">Blog</a></td>  -->
                <td style="WIDTH: 540px"></td></tr>
             <tr><td style="HEIGHT: 30px;WIDTH: 900px" colspan="12"></td></tr></tbody></table></center>
          </td></tr></tbody></table>
<!--================================End of Header of Windows Body====================================-->

<!--================================Begin of Body of Windows Body====================================-->
            <table id="main" border="0" cellspacing="0" cellpadding="0" width="900">
              <tbody>
              <tr>
                <td style="BACKGROUND-COLOR: #fcfcfc; WIDTH: 100%; OVERFLOW: hidden" valign="top">
                
                  <div id="divContentHome" class="content_div_block">
                  <table class="garde_table_photo" align="center" border="0">
                    <tbody>
                    <tr>
                      <td style="width:412px;height:200px;background-color: #FFFFFF; font-family:garamond; font-size:13px; padding: 10px 20px 10px 20px ; border: 1px solid #D2D2D2;">
                      	<table>
							<tbody><tr>
								<td colspan="2" style="width:165px; height:44px;"><a href="https://sydney.edu.au" target="_blank"><img src="./images/USYD_logo.jpg"></a></td>								
<!--								<td colspan="2" style="width:412px; height:44px;"><img src="cu-hk-university.jpg" alt="CUHK" width="308px" height="70px"></td> -->
							</tr>
							<tr>
								<td colspan="2" style="width:412px; height:20px;"></td>
							</tr>
							<tr>
								<td colspan="2" style="width:412px; height:22px;"><b style="font-family:garamond; font-size: 19px;font-weight: bold;">Wanli Ouyang, Ph.D, IEEE Senior Member.</b></td>
							</tr>
							<tr>
								<td colspan="2" style="width:412px; height:22px;"> <b style="font-family:garamond; font-size: 15px;">Senior Lecturer at the University of Sydney</b></td>
							</tr>
							<tr>
								<td colspan="2" style="width:412px; height:22px;"> <b style="font-family:garamond; font-size: 15px;">I'm with <a class="aLink" href="http://mmlab.ie.cuhk.edu.hk/" target="_blank"><i> MMlab </i></a> and <a class="aLink" href="https://mmlab-usyd.github.io/" target="_blank"><i> SIGMA lab</i></a> </b></td>
							</tr>
						</tbody></table>
						<table cellspacing="0" cellpadding="0" border="0" width="100%">
							<tbody><tr>
								<td bgcolor="#0099FF"><img src="transparent.gif" alt="" width="1" height="1" border="0"></td>
							</tr>
						</tbody></table>
						<table>
							<tbody><tr>
								<td style="width:210px; height:44px; line-height:125%; ">
								<a class="aLink" href="https://mmlab-usyd.github.io/" target="_blank"> SIGMA lab</a>, <a class="aLink" href="https://sydney.edu.au/engineering/about/school-of-electrical-and-information-engineering.html" target="_blank"> School of Electric and Information Engineering, </a> <br>
								<a class="aLink" href="https://sydney.edu.au/" target="_blank"> The University of Sydney, </a> <br>
								Sydney, Australia<br>
								</td>
								<td style="width:182px; height:44px; line-height:125%; text-align:left;">
								<br>
								<br>
								<img src="mainpa1.gif" alt="CUHK" width="80px" height="12px">sydney.edu.au<br>
								</td>
							</tr>
						</tbody></table>
<!--						<table>
							<tbody><tr>
								<td colspan="2" style=" line-height:50%; text-align:center;VERTICAL-ALIGN:middle; width:356px; height:10px;">www.interdigital.com</td>
							</tr> 
						</tbody>
						</table>-->
                      </td>
                      <td width="150px"></td>
                      <td style="VERTICAL-ALIGN: middle; width:120px"><img alt="Photo" src="Wanli-New.jpg" width="180px" height="200px"></td>
                      <td width="167px"></td>
                     </tr>
                    </tbody>
                  </table>
				  </div>
                  <div id="divContentBiography" class="content_div_block"><a name="Biography"></a>
                  <h1>Biography</h1>
                  <p class="garde_p">
                  	Wanli Ouyang obtained Ph.D from <a class="aLink" href="http://www.ee.cuhk.edu.hk/" target="_blank">  the Dept. of Electronic Engineering </a>, 
                  	the Chinese University of Hong Kong. 
                  	He is now a Senior Lecturer (considered as associate professor in US university systems) at the University of Sydney.
                  	His research interests include deep learning and its application to computer vision and pattern recognition, image and video processing.  <br><br> 

<!-- Dr. Dong received the B.Eng. and M.Eng. degrees, both in Information Engineering from 
                  <a class="aLink" href="http://www.zju.edu.cn/english/" target="_blank">Zhejiang University</a>, Hangzhou, China, 
                  in 2002 and 2005, respectively, and received the Ph.D. degree in Electronic Engineering in 2009, from 
                  <a class="aLink" href="http://www.cuhk.edu.hk/english/" target="_blank">the Chinese University of Hong Kong</a>, 
                  Hong Kong, China, where she worked as a Postdoctoral Fellow in the following year. In 2011,
                  she joined the CTO Office of 
                  <a class="aLink" href="http://www.interdigital.com/" target="_blank">InterDigital Communications</a>, U.S.A.,
                  as Staff Engineer. From 2003 to 2009, she was an active participant in Chinese standardization for 
                  multimedia with successful submissions to 
                  <a class="aLink" href="http://www.avs.org.cn/english/" target="_blank">AVS workgroup</a>. She has been engaged in 
                  HEVC standardization effort since 2011. 
                  Her research interests include high efficiency video coding and real-time video processing.<br><br> -->
				  <a target="_blank" class="aLink" href="./CV_WanliOuyang_CUHK.pdf"><img src="icone_pdf.gif" alt="CV PDF" border="0"> Download Wanli Ouyang's Full CV</a>
				  &nbsp;&nbsp;&nbsp;&nbsp;
				  <a target="_blank" class="aLink" href="http://hk.linkedin.com/pub/wanli-ouyang/24/61b/293">
				  <img src="btn_in_20x15.png" alt="View Wani Ouyang&#39;s LinkedIn Profile" border="0">View Wanli Ouyang's LinkedIn Profile</a> 
				  &nbsp;&nbsp;&nbsp;&nbsp;
				  <a target="_blank" class="aLink" href="http://scholar.google.com/citations?user=pw_0Z_UAAAAJ&amp hl=en">
				  <img src="google.png" alt="View Wani Ouyang&#39;s Google Scholar Citations" border="0"> View Wanli Ouyang's Google Scholar Citations</a><br><br>
                  <a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/#Top">Back To Top</a> <br><br> </p>
</div>
                  <h1>Information for potential Postdoctoral Fellow, Master and Ph.D. students </h1>                   
                  I moved  <a href="http://sydney.edu.au/engineering/electrical/"> the School of Electrical and Information Engineering, University of Sydney </a> as senior lecturer on 2017. 
                  If you are interested in my research topic and this university, please feel free to contact me. Some information is available <a class="aLink" href="https://mmlab-usyd.github.io/recruitment" target="_blank"> here </a>, <a class="aLink" href="https://mmlab-usyd.github.io/recruitment/" target="_blank">
                  <h1>News </h1>                   
                  <a href="./projects/GBD/index.html">Our team rank as #1 for object detection with provided data and external data and #1 for video object detection/tracking in the ImageNet Large Scale Visual Recognition Challenge 2016. Project page with source code </a> <br />

                  <h1>Good resources on Paper Writing </h1>         
                  <a href="http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649439633&idx=1&sn=f9a6b894e4266a6e5ec4b6ec673255a2&chksm=82c0d415b5b75d03faa511eb691630d2e5d3ed2fbe3598629ab20a037b8d95bfa5c795ad1c66&mpshare=1&scene=1&srcid=1205JJGMMw3BmvYdPXJBrNyq##"> How to do good research on computer vision (Chinese) </a> <br />                            
                  <a href="http://ec.europa.eu/eurostat/documents/64157/4374310/33-UNECE-making-data-meaningful-Part2-EN.pdf/d5b954e2-b110-469b-a2b5-78aa7c12ab62"> Making Data meaningful </a> <br />
                  <a href="https://www.dropbox.com/s/wgrdpmxcmb4nhl0/How%20to%20Get%20Your%20CVPR%20Paper%20Rejected.pptx?dl=0"> Slides on "How to get your paper rejected." By Prof. Ming-Hsuan Yang from UC Merced </a> <br />
                  <a href="http://blog.csdn.net/daiyuchao/article/details/6419543"> Chinese blog on how to publish a top journal </a> <br />

<!--                   <a href="./projects/ImageNet/index.html">Our team rank as #1 for video object detection and #2 for still image object detection in the ImageNet Large Scale Visual Recognition Challenge 2015. Project page</a> <br />
                  <a href="./projects/ImageNet/index.html">Our team rank as #2 in the ImageNet Large Scale Visual Recognition Challenge 2014. Project page</a> <br /> -->

<!--                  <div id="divContentEducation" class="content_div_block"><a name="Education"></a>
                  <h1>Education </h1>
                  <table class="i_table_entier">
                    <tbody>
                    <tr class="pub_tr_2">
                      <td width="25%">Aug. 2007 - Dec. 2010</td>
                      <td width="17%">Ph.D. Degree</td>
                      <td width="23%">in Electronic Engineering</td>
                      <td width="35%"> <a class="aLink" href="http://www.ee.cuhk.edu.hk/" target="_blank"> The Chinese University of Hong Kong, China</a></td></tr>
                    <tr class="pub_tr_1">
                      <td width="25%">Sept. 2003 - Jul. 2006</td>
                      <td width="17%">M.Eng. Degree</td>
                      <td width="23%">in Computer Science</td>
                      <td width="35%"><a class="aLink" href="http://www.bjut.edu.cn/" target="_blank">  Beijing University of Technology, China</a></td></tr>
                    <tr class="pub_tr_2">
                      <td width="25%">Sept. 1999 - Jul. 2003 </td>
                      <td width="17%">B.Eng. Degree</td>
                      <td width="23%">in Computer Science</td>
                      <td width="35%"><a class="aLink" href="http://www.xtu.edu.cn/index.html" target="_blank">  Xiangtan University, China </a></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="100%" align="left" colspan="4" style="HEIGHT:10px"></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="100%" align="left" colspan="4"><a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/#Top">Back To Top</a></td></tr></tbody></table>
</div>
                  <div id="divContentIntern" class="content_div_block"><a name="Internship Experience"></a>
                  <h1>Internship Experience </h1>
                  <table class="i_table_entier">
                    <tbody>
                    <tr class="pub_tr_2">
                      <td width="25%">Jul. 2004-Oct. 2005</td>
                      <td width="17%"><a class="aLink" href="http://www.secwatch.com.cn/" target="_blank"> Secwatch Technology</a></td>
                      <td width="23%">Software Engineer</td>
                      <td width="35%">  Video encoder and decoder optimization (H.264, MPEG2/4) on digital signal processor (DSP) of Phillips. </td>
		    </tr>
                    <tr class="pub_tr_1">  
                      <td width="100%" align="left" colspan="4"><a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/#Top">Back To Top</a></td>
		    </tr>
		    </tbody>
		  </table>
</div>
                  <div id="divContentWorkExperience" class="content_div_block"><a name="Work Experience"></a>
                  <h1>Work Experience </h1>
                  <table class="i_table_entier">
                    <tbody>
                    <tr class="pub_tr_2">
                      <td width="25%">Jul. 2006-Jul. 2007</td>
                      <td width="17%"><a class="aLink" href="http://www.esstech.com/" target="_blank"> ESS Technology Inc.</a></td>
                      <td width="23%">Software Engineer</td>
                      <td width="35%">  Audio decoder optimization on digital signal processor (DSP), embedded system design</td></tr>
                    <tr class="pub_tr_1">  
                      <td width="100%" align="left" colspan="4"><a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/#Top">Back To Top</a></td>
		    </tr>
		    </tbody>
		  </table>
</div>
--->
                  <div id="divContentPublications" class="content_div_block"><a name="Research"></a>
                 <script language="JavaScript" type="text/javascript">
				function menu(name){								
					idDiv = name;
					div = document.getElementById(idDiv);
					display = div.style.display;
					
					if (display=='block'){
						div.style.display='none';
					}
					else{
						div.style.display='block';
					}
				}
				</script>
<!--                   <h1>Theses</h1>
                  <table border="0" cellspacing="0" cellpadding="5" width="900px">
                    <tbody>
                    <tr class="pub_tr_2">
                      <td class="pub_td_number">1 </td>
                      <td class="pub_td_text"> 
                        "Analysis, Coding, and Processing for High-Definition Videos," Ph.D. Thesis,
                        The Chinese University of Hong Kong. Dec. 2009.
                        <br>Thesis Advisor: <a class="aLink" href="http://www.ee.cuhk.edu.hk/~knngan" target="_blank">Prof. King Ngi NGAN</a>.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract_thesis_phd&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex_thesis_phd&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/2009_phdthesis.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Oral Slides" href="http://www.ee.cuhk.edu.hk/~wlouyang/2009_phdoral.pdf" target="_blank">Oral</a>]
                        <br>
                        <div id="divAbstract_thesis_phd" class="pub_div_abstract"><br><b>Abstract:</b> 
                        Today, High-Definition (HD) videos become more and more popular with many applications.
                        This thesis analyzes the characteristics of HD videos and develops the appropriate coding
                        and processing techniques accordingly for hybrid video coding.
                        <p>Firstly, the characteristics of HD videos are studied quantitatively. The results
                        show that HD videos distinguish from other lower resolution videos by higher spatial
                        correlation and special power spectral density (PSD), mainly distributed along the
                        vertical and horizontal directions.</p>
                        <p>Secondly, two techniques for HD video coding are developed based on the aforementioned
                        analysis results. To exploit the spatial property, 2D order-16 transforms are
                        proposed to code the higher correlated signals more efficiently. Specially, two series
                        of 2D order-16 integer transforms, named modified integer cosine transform (MICT)
                        and non-orthogonal integer cosine transform (NICT), are developed to
                        provide different trade-offs between the performance and the complexity. Based on the
                        property of special PSD, parametric interpolation filter (PIF) is proposed for motioncompensated
                        prediction (MCP). Not only can PIF track the non-stationary statistics
                        of video signals as the related work shows, but also it represents interpolation filters
                        by parameters instead of individual coefficients, thus solving the conflict of the accuracy
                        of coefficients and the size of side information. The experimental results show
                        the proposed two coding techniques significantly outperform their equivalents in the
                        state-of-the-art international video coding standards.</p>
                        <p>Thirdly, interlaced HD videos are studied, and to satisfy different delay constraints,
                        two real-time de-interlacing algorithms are proposed specially for H.264 coded videos.
                        They adapt to local activities, according to the syntax element (SE) values. Accuracy
                        analysis is also introduced to deal with the disparity between the SE values and the
                        real motions and textures. The de-interlacers provide better visual quality than the
                        commonly used ones and can de-interlace 1080i sequences in real time on PCs.</p>
                        </div>
                        <div id="divBibTex_thesis_phd" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @PHDTHESIS{2009_dong_phdthesis,
  			<br>author = {J. Dong},
  			<br>title = {Analysis, Coding, and Processing for High-Definition Videos},
  			<br>school = {The Chinese University of Hong Kong},
  			<br>year = {2009},
  			<br>month = {Dec.}
  			<br>}</code></div></td></tr>
                    <tr class="pub_tr_1">
                      <td class="pub_td_number">2 </td>
                      <td class="pub_td_text"> 
                        "Context-based Entropy Coding in Video Compression," Master Thesis,
                        Zhejiang University. Mar. 2005. 
                        <br> Thesis Advisor: <a class="aLink" href="http://mypage.zju.edu.cn/en/yul" target="_blank">Prof. Lu YU</a>.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract_thesis_master&#39;)">Abstract</a>]
                        <br>
                        <div id="divAbstract_thesis_master" class="pub_div_abstract"><br><b>Abstract:</b> 
                        Entropy coding is a basic and critical technology in video compression. No matter what
                         video system is used, all the syntax elements should be converted into binary symbols 
                         and removed statistic redundancy by entropy coding.
                        <p>As a kind of lossless codings, the entropy coding used in video compression includes 
                        the scalar lossless coding, the vector lossless coding and the conditional lossless coding. 
                        In this thesis, the theoretical limits of the 3 kinds of lossless codings are introduced, 
                        and it is obvious that the latter 2 could achieve higher coding efficiency than the scalar 
                        lossless coding.</p>
                        <p>In the area of video compression, Huffman coding and arithmetic coding are widely used. 
                        Huffman coding converts a fixed number of symbols into a variable-length codeword, while 
                        arithmetic coding converts a variable number of symbols into a variable-length codeword. 
                        Huffman and arithmetic methods are probability-model-based, and both can reach the entropy 
                        bound asymptotically. Because of the higher coding efficiency and ease of adptation, arithmetic 
                        coding is a better alternative than Huffman coding, as long as the computation involved is 
                        acceptable.</p>
			<p>In the past video coding standards, such as MPEG-2/4, H.261/H.263, syntax elements are 
			compressed by scalar lossless Huffman coding. To improve the coding efficiency, in the 
			Baseline Profile and the Extended Profile of the latest international standard H.264, 
			conditional Huffman coding is employed, known as CAVLC (Context-based Adaptive Variable-Length 
			Coding). CAVLC regards contexts as different conditions, and uses multiple VLC tables for 
			different conditioning states, and improves the coding efficiency greatly.</p>
			<p>In order to avoid using multiple VLC tables and further improve the coding efficiency, 
			the Main Profile of H.264 adopts an alternative scheme of entropy coding, known as CABAC 
			(Context-based Adaptive Binary Arithmetic Coding). The arithmetic coding engine it employs 
			can adapt to changes in the source statistics simply by updating the symbol probability table. 
			Compared with CAVLC, CABAC can save 9%~14% bitrate but leads to more computational complexity.</p>
			<p>National standard AVS (China's Audio Video Coding Standard) employs the 2-D joint 
			variable-length coding to remove the redundancy between the level and run in transformed 
			coefficients blocks, which means to regard the pair of (level, run) as a joint event, 
			estimate its probability distribution and design 2-D VLC tables accordingly. This method 
			is more suitable for parallel implementation compared with CAVLC and CABAC. Though widely 
			used in many international video coding standards, many existing 2-D joint variable-length 
			coding methods are not conditional lossless coding ones. However, the method used in AVS is 
			a kind of conditional lossless codings, which regards the asymptotically changed (level, run) 
			as contexts and improve the coding efficiency effectively.</p>
                        </div>
                        </td></tr>
                    <tr class="pub_tr_1">  
                      <td width="100%" align="left" colspan="2" style="HEIGHT:10px"></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="100%" align="left" colspan="2"><a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/#Top">Back To Top</a></td></tr></tbody></table>
                  <h1>Book Chapter</h1>
                  <table border="0" cellspacing="0" cellpadding="5" width="900">
                    <tbody>
                    <tr class="pub_tr_2">
                      <td class="pub_td_number">1 </td>
                      <td class="pub_td_text"> 
                        <strong>Jie Dong</strong> and King Ngi Ngan, “Present and future video coding standards,” in <em> Intelligent Multimedia 
                        Communication: Techniques and Applications</em>, Springer-Verlag Publisher, ISBN 978-3-642-11685-8, Jan. 2010, pp. 75-124.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2010_chapter&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2010_chapter&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/2010_chapter.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Errata" href="http://www.ee.cuhk.edu.hk/~wlouyang/2010_chapter_errata.pdf" target="_blank">Errata</a>]                        
                        <br>
                        <div id="divAbstract2010_chapter" class="pub_div_abstract"><br><b>Abstract:</b> 
                        Video coding systems may greatly differ from each other, as they consist of complicated
                        functional modules and each module can be realized by using different
                        techniques. For the interoperability in communication, video coding standards
                        are developed to restrict various video coding systems, such that manufacturers
                        can successfully interwork with each other by producing compliant encoders and
                        decoders, and at the same time still have the freedom to develop competitive
                        and innovative products. 
                        <p>A standard defines a coded representation, syntax, which describes the video
                        in a compressed form. In other words, a standard only specifies the output of
                        the encoder, i.e., the input of the decoder, instead of the codec itself. Although
                        the standard also provides a method of decoding the syntax to reconstruct the
                        video, the manufacturers are free to develop alternative decoders as long as they
                        can decode the syntax and produce the same result as that in the standard.</p>
                        <p>Video coding standards have been developing for about 20 years, driven by
                        applications and advances in hardware capability. This chapter begins with an
                        introduction of the block-based hybrid coding scheme, which is essentially the
                        core of all the video coding standards. In Section 2, the past video coding standards
                        are briefly reviewed, including H.261, MPEG-1, MPEG-2/H.262, H.263,
                        and MPEG-4. The latest standard, H.264/AVC, is the emphasis of this chapter,
                        which is introduced in Section 3, including the new technical developments
                        and profiles favoring a wide range of applications. The recently finalized amendments
                        on scalable video coding (SVC) and multiview video coding (MVC) are
                        another two highlights. Section 4 presents the Audio and Video Coding Standard
                        (AVS) of China, which has received much attention throughout the world,
                        even though it is a national standard. Finally, Section 5 introduces the current
                        topics intensively studied in the standardization groups, which may become the
                        key techniques in the future coding standards.</p>
                        <br></div>
                        <div id="divBibTex2010_chapter" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @INBOOK{2010_dong_chapter,
                        <br>chapter = {Present and future video coding standards},
                        <br>pages = {75-124},
                        <br>title = {Intelligent Multimedia Communication: Techniques and Applications},
                        <br>publisher = {Springer-Verlag Publisher},
                        <br>year = {2010},
                        <br>editor = {C. W. Chen and Z. Li and S. Lian},
                        <br>author = {J. Dong and K. N. Ngan},
                        <br>month = {Jan.},
                        <br>doi = {10.1007/978-3-642-11686-5_3}
                        <br>}</code></div></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="100%" align="left" colspan="2" style="HEIGHT:10px"></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="100%" align="left" colspan="2"><a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/#Top">Back To Top</a></td></tr></tbody></table> -->

<!--========================  Journal Papers  ==========================--> 
                  <h1>Journal Papers</h1>
                  
                  <table border="0" cellspacing="0" cellpadding="5" width="900">
                    <tbody>


                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/PedCNN_TPAMI.jpg" width="336px" height="155px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	<strong>Wanli Ouyang</strong>, Hui Zhou, Hongsheng Li, Quanquan Li, Junjie Yan, Xiaogang Wang,
                      "Jointly learning deep features, deformable parts, occlusion and classification for pedestrian detection," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, accepted, 2017.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2017_PAMI1&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2017_PAMI1&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/Ouyang2017JoingCNNPed.pdf" target="_blank">Full Text</a>]
                        <br>
                        <div id="divAbstract2017_PAMI1" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian
detection. Existing methods learn or design these components either individually or sequentially. The interaction among these
components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths
through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network
architecture ( Code available on www.ee.cuhk.edu.hk/∼wlouyang/projects/ouyangWiccv13Joint/index.html). By establishing automatic,
mutual interaction among components, the deep model has average miss rate 8.57%/11.71% on the Caltech benchmark dataset with
new/original annotations.

                        </div>
                        <div id="divBibTex2017_PAMI1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016Jointly,
  title={Jointly learning deep features, deformable parts,
occlusion and classification for pedestrian detection},
  author={Wanli Ouyang and Hui Zhou and 
Hongsheng Li and Quanquan Li and Junjie Yan and Xiaogang Wang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell},
  pages={1--14},
  year={2017},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   


                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/GBD_TPAMI.jpg" width="336px" height="155px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	Xingyu Zeng (equal contribution), <strong>Wanli Ouyang</strong> (equal contribution), Junjie Yan, Hongsheng Li, Tong Xiao, Kun Wang, Yu Liu, Yucong Zhou, Bin
Yang, Zhe Wang, Hui Zhou, Xiaogang Wang,
                      "Crafting GBD-Net for Object Detection," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, accepted, 2017.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2017_PAMI2&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2017_PAMI2&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/Zeng2017_GBD_PAMI.pdf" target="_blank">Full Text</a>]
												[<a class="aLink" href="https://github.com/craftGBD/craftGBD" target="_blank"> Code </a>]
												[<a class="aLink" href="./projects/GBD/index.html" target="_blank">Project page & code </a>]												
                        <br>
                        <div id="divAbstract2017_PAMI2" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

The visual cues from multiple support regions of
different sizes and resolutions are complementary in classifying
a candidate box in object detection. Effective integration of
local and contextual visual cues from these regions has become
a fundamental problem in object detection. In this paper, we
propose a gated bi-directional CNN (GBD-Net) to pass messages
among features from different support regions during both
feature learning and feature extraction. Such message passing
can be implemented through convolution between neighboring
support regions in two directions and can be conducted in
various layers. Therefore, local and contextual visual patterns can
validate the existence of each other by learning their nonlinear
relationships and their close interactions are modeled in a more
complex way. It is also shown that message passing is not always
helpful but dependent on individual samples. Gated functions are
therefore needed to control message transmission, whose on-oroffs
are controlled by extra visual evidence from the input sample.
The effectiveness of GBD-Net is shown through experiments
on three object detection datasets, ImageNet, Pascal VOC2007
and Microsoft COCO. Besides the GBD-Net, this paper also
shows the details of our approach in winning the ImageNet
object detection challenge of 2016, with source code provided on
https://github.com/craftGBD/craftGBD. In this winning system,
the modified GBD-Net, new pretraining scheme and better region
proposal designs are provided. We also show the effectiveness of
different network structures and existing techniques for object
detection, such as multi-scale testing, left-right flip, bounding box
voting, NMS, and context.

                        </div>
                        <div id="divBibTex2017_PAMI2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016Jointly,
  title={Crafting GBD-Net for Object Detection},
  author={Zeng, Xingyu and Ouyang, Wanli and Yan, Junjie and Li, Hongsheng and Xiao, Tong and Wang, Kun and Liu, Yu and Zhou, Yucong and Yang, Bin and Wang, Zhe and Hui Zhou and Xiaogang Wang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell},
  pages={1--14},
  year={2017},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   



                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="imgs/Deep_IDNet_PAMI.jpg" width="336px" height="155px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	<strong>Wanli Ouyang</strong>, Xingyu Zeng,  Xiaogang Wang, et al,
                      "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks," 
                      <em>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</em>, accepted, 2016.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2016_PAMI2&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2016_PAMI2&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/DeepID-Net.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Project" href="./projects/ImageNet/index.html" target="_blank">Project</a>]                          
                        <br>
                        <div id="divAbstract2016_PAMI2" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep
learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation
constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training
strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability.
By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of
models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach
improves the mean averaged precision obtained by RCNN [16], which was the state-of-the-art, from 31% to 50.3% on the ILSVRC2014
detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1%. Detailed component-wise analysis is also
provided through extensive experimental evaluation, which provides a global view for people to understand the deep learning object
detection pipeline.

Wanli Ouyang and Xingyu Zeng and Xiaogang Wang  and Shi Qiu and Ping Luo and Yonglong Tian and
Hongsheng Li and Shuo Yang and Zhe Wang and Hongyang Li and Kun Wang and Junjie Yan and Chen-Change Loy and Xiaoou Tang
                        </div>
                        <div id="divBibTex2016_PAMI2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016DeepID-Net,
  title={DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks},
  author={Wanli Ouyang and Xingyu Zeng and Xiaogang Wang  and Shi Qiu and Ping Luo and Yonglong Tian and Hongsheng Li and Shuo Yang and Zhe Wang and Hongyang Li and Kun Wang and Junjie Yan and Chen-Change Loy and Xiaoou Tang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell},
  pages={1--14},
  year={2016},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="images/OHT_J.jpg" width="336px" height="205px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	<strong>Wanli Ouyang</strong>, Tianle Zhao, Wai-Kuen Cham,  Liying Wei
                      "Fast Full-Search Equivalent Pattern Matching Using Asymmetric Haar Wavelet Packets," 
                      <em>IEEE Trans. Circuits and System for Video Technology (CSVT)</em>, accepted, 2016.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2016_IJCV&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2016_IJCV&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/AHT_CSVT16.pdf" target="_blank">Full Text</a>]
			                  [<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/OHT.htm" target="_blank">Project & source code</a>]
                        [<a class="aLink" title="Download Slides" href="http://www.ee.cuhk.edu.hk/~wlouyang/PPT/OHT1.ppt" target="_blank">Slides</a>]
                        <br>
                        <div id="divAbstract2016_CSVT" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Pattern matching is widely used in signal processing,
computer vision, image and video processing. One efficient
approach is to perform pattern matching in a transform domain
that has good energy packing ability and so allows early rejection
of most mismatched candidates. Calculating the transforms of
pixels in sliding windows requires much computation, and so
fast algorithms are employed. Existing methods require O(u)
additions per pixel for projecting input pixels onto u 2D basis
vectors. In this paper, we propose a new 2D transform, called
asymmetric 2D Haar transform (A2DHT), and extend it to
wavelet packets that contain exponentially large number of bases.
A basis selection algorithm is then proposed to search for the
optimal basis in the wavelet packets. A fast algorithm is also
developed which can compute u projection coefficients with
only O(log u) additions per pixel. Results of experiments show
that the proposed fast algorithm and the proposed transform
can significantly accelerate the full-search equivalent pattern
matching process and outperform state-of-the-art methods.
                        </div>
                        <div id="divAbstract2016_CSVT" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016fast,
  title={Fast Full-Search Equivalent Pattern Matching Using Asymmetric Haar Wavelet Packets},
  author={Ouyang, Wanli and Zhao, Tianle and Cham, Wai-kuen and Wei, Liying},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2016},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   

                      
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="mutual_visibility.PNG" width="336px" height="205px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      	<strong>Wanli Ouyang</strong>, Xingyu Zeng and Xiaogang Wang, 
                      "Learning Mutual Visibility Relationship for Pedestrian Detection with a Deep Model," 
                      <em>International Journal of Computer Vision (IJCV)</em>, accepted, 2016.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2016_IJCV&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2016_IJCV&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/Mutual-DBN-IJCV16.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ouyangZWcvpr13MutVisibility/index.html" target="_blank">  Project and code</a>]
                        <br>
                        <div id="divAbstract2016_IJCV" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. 
The difficulty is added when several pedestrians overlap in images and occlude each other. 
We observe, however, that the occlusion/visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation - the visibility estimation of one pedestrian facilitates the visibility estimation of another. 
In this paper, we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. 
The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. 
Then the evidence of co-existing pedestrians is used for improving the single pedestrian detection results. 
Compared with existing image-based pedestrian detection approaches, our approach has the lowest average miss rate on the Caltech-Train dataset and the ETH dataset. 
Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. 
The mutual visibility deep model leads to 6% - 15% improvements on multiple benchmark datasets.
                        </div>
                        <div id="divBibTex2016_IJCV" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyang2016learning,
  title={Learning Mutual Visibility Relationship for Pedestrian Detection with a Deep Model},
  author={Ouyang, Wanli and Zeng, Xingyu and Wang, Xiaogang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  pages={1--14},
  year={2016},
  publisher={IEEE}
}
<br>}
			  </code></div>
                      </td></tr>   

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src=".\projects\Person_reid_pami\Main.JPG" width="336px" height="120px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> Rui Zhao, <strong>Wanli Ouyang</strong> and Xiaogang Wang, 
                      "Person Re-identification by saliency Learning," 
                      <em>IEEE Trans. Pattern Analysis and machine Intelligence(TPAMI)</em>, accepted, 2016.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2016_tpami&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2016_TPAMI&#39;)">BibTeX</a>]
												[<a class="aLink" href=".\projects\Person_reid_pami\TPAMI2544310_proof2.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="" target="_blank">  Project and dataset(N/A)</a>]
                        <br>
                        <div id="divAbstract2015_tpami" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Human eyes can recognize person identities based on small salient regions, i.e. person saliency is distinctive and reliable
in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities
of pedestrian images with existing approaches. Inspired by our user study result of human perception on person saliency, we propose
a novel perspective for person re-identification based on learning person saliency and matching saliency distribution. The proposed
saliency learning and matching framework consists of four steps: (1) To handle misalignment caused by drastic viewpoint change
and pose variations, we apply adjacency constrained patch matching to build dense correspondence between image pairs. (2) We
propose two alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a saliency score for each image patch,
through which distinctive features stand out without using identity labels in the training procedure. (3) saliency matching is proposed
based on patch matching. Matching patches with inconsistent saliency brings penalty, and images of the same identity are recognized
by minimizing the saliency matching cost. (4) Furthermore, saliency matching is tightly integrated with patch matching in a unified
structural RankSVM learning framework. The effectiveness of our approach is validated on the four public datasets. Our approach
outperforms the state-of-the-art person re-identification methods on all these datasets.
                        </div>
                        <div id="divBibTex2016_TPAMI" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{zhao2014person,
  title={Person Re-identification by Saliency Learning},
  author={Zhao, Rui and Ouyang, Wanli and Wang, Xiaogang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2016}
}
<br>}
			  </code></div>
                      </td></tr>   
                      
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="HumanDBNDetRes.jpg" width="336px" height="205px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> <strong>Wanli Ouyang</strong>, Xingyu Zeng and Xiaogang Wang, 
                      "Partial Occlusion Handling in Pedestrian Detection with a Deep Model," 
                      <em>IEEE Trans. Circuits and System for Video Technology (TCSVT)</em>, accepted, 2015.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_tpami&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_TPAMI&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/TCSVT_ouyang_xyZeng_xgwang.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="http://www.ee.cuhk.edu.hk/~xgwang/CUHK_pedestrian.html" target="_blank">  Project</a>]
                        <br>
                        <div id="divAbstract2015_tpami" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

Part-based models have demonstrated their merit	in object detection. However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions, abnormal deformations, appearances or illuminations. To handle the imperfection of part detectors, this paper presents a probabilistic pedestrian detection framework. 
In this framework, a deformable part-based model is used to obtain the scores of part detectors and the visibilities of parts are modeled as hidden variables. 
Once the occluded parts are identified, their effects are properly removed from the final detection score.
Unlike previous occlusion handling approaches that assumed independence among the visibility probabilities of parts or manually defined rules for the visibility relationship, a deep model is proposed in this paper for learning the visibility relationship among overlapping parts at multiple layers. 
The proposed approach can be viewed as a general post-processing of part-detection results and can take detection scores of existing part-based models as input. 
Experimental results on three public datasets (Caltech, ETH and Daimler) and a new CUHK occlusion dataset, which is specially designed for the evaluation of occlusion handling approaches,  show the effectiveness of the proposed approach.

                        </div>
                        <div id="divBibTex2015_TPAMI" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyangPartial2015,
  <br>title={Partial Occlusion Handling in Pedestrian Detection with a Deep Model},
  <br>author={Ouyang, Wanli and Zeng, Xingyu and Wang, Xiaogang},
  <br>publisher={IEEE Trans. Circuits and System for Video Technology}
<br>}
			  </code></div>
                      </td></tr>   


                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="two_pedestrian.PNG" width="336px" height="205px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> <strong>Wanli Ouyang</strong>, Xingyu Zeng and Xiaogang Wang, 
                      "Single-Pedestrian Detection Aided by Two-Pedestrian Detection," 
                      <em>IEEE Trans. Pattern Analysis and machine Intelligence(TPAMI)</em>, accepted, 2015.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_tpami&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_tip&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/tpami15-ouyang_xgWang_xyZeng.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ouyangWcvpr13MultiPed/index.html" target="_blank">  Project, source code and dataset</a>]
                        <br>
                        <div id="divAbstract2015_tpami" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->

In this paper, we address the challenging problem of detecting pedestrians who appear in groups. A new approach is
proposed for single-pedestrian detection aided by two-pedestrian detection. A mixture model of two-pedestrian detectors is
designed to capture the unique visual cues which are formed by nearby pedestrians but cannot be captured by single-pedestrian
detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and
two-pedestrian detectors, and to refine the single-pedestrian detection result using two-pedestrian detection. The two-pedestrian
detector can integrate with any single-pedestrian detector. Twenty-five state-of-the-art single-pedestrian detection approaches are
combined with the two-pedestrian detector on three widely used public datasets: Caltech, TUD-Brussels, and ETH. Experimental
results show that our framework improves all these approaches. The average improvement is 9 percent on the Caltech-Test
dataset, 11 percent on the TUD-Brussels dataset and 17 percent on the ETH dataset in terms of average miss rate. The lowest
average miss rate is reduced from 37 to 32 percent on the Caltech-Test dataset, from 55 to 50 percent on the TUD-Brussels dataset
and from 43 to 38 percent on the ETH dataset.
                        </div>
                        <div id="divBibTex2013_tip" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@article{ouyangsingle,
  <br>title={Single-Pedestrian Detection Aided by 2-Pedestrian Detection},
  <br>author={Ouyang, Wanli and Zeng, Xingyu and Wang, Xiaogang},
  <br>publisher={IEEE Trans. Pattern Anal. Mach. Intell.}
<br>}
			  </code></div>
                      </td></tr>   
                      
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                      <td width="304px" style="text-align: center"> <img style="width: 336Px;" alt="Wanli" src="mainpa1.jpg" width="336px" height="205px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> <strong>Wanli Ouyang</strong>, Renqi Zhang and Wai-Kuen Cham, 
                      "Segmented Gray-Code Kernels for Fast Pattern Matching," 
                      <em>IEEE Trans. Image Processing(TIP)</em>, 22(4):1512-1525, Apr. 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_tip&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_tip&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/SegGCK.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/SegGCK.htm" target="_blank">  Project, source code and dataset</a>]
                        <br>
                        <div id="divAbstract2013_tip" class="pub_div_abstract"><br><b>Abstract:</b> 
<!--                        <div id="divAbstract2012_tpami" class="pub_div_abstract"> <br><b>Abstract:</b>  -->
                        The gray-code kernels (GCK) family, which has
												Walsh Hadamard transform on sliding windows as a member, is a
												family of kernels that can perform image analysis efficiently using
												a fast algorithm, such as the GCK algorithm. The GCK has been
												successfully used for pattern matching. In this paper, we propose
												that the G4-GCK algorithm is more efficient than the previous
												algorithm in computing GCK. The G4-GCK algorithm requires
												four additions per pixel for three basis vectors independent of
												transform size and dimension. Based on the G4-GCK algorithm,
												we then propose the segmented GCK. By segmenting input data
												into Ls parts, the SegGCK requires only four additions per
												pixel for 3Ls basis vectors. Experimental results show that the
												proposed algorithm can significantly accelerate the full-search
												equivalent pattern matching process and outperforms state-ofthe-
												art methods.
                        </div>
                        <div id="divBibTex2013_tip" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @ARTICLE{Ouyang2013SegGCK,                        
			  <br>author = {Wanli Ouyang and Federico Tombari and Stefano Mattoccia and Luigi
			  <br>Di Stefano and Wai-Kuen Cham},
			  <br>title = {Segmented Gray-Code Kernels for Fast Pattern Matching},
			  <br>journal = {IEEE Trans. Image Processing},
			  <br>year = {2013},
			  <br>volume = {22},
			  <br>pages = {1512-1525},
			  <br>number = {4},
			  <br>month = {Apr.},
			  <br>}</code></div>
                      </td></tr>   

                    
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                     <td width="304px" style="text-align: center"> <!--  <img style="width: 336Px;" alt="Wanli" src="mainpa1.jpg" width="336px" height="205px"> </td>  -->                    
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> <strong>Wanli Ouyang</strong>, Federico Tombari, Stefano Mattoccia, Luigi Di Stefano and Wai-Kuen Cham, 
                      "Performance Evaluation of Full Search Equivalent Pattern Matching Algorithms," 
                      <em>IEEE Trans. Pattern Analysis and machine Intelligence(TPAMI)</em>, 34(1):127 - 143, Jan. 2012.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2012_tpami&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2012_tpami&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/PME_TPAMI.pdf" target="_blank">  Full Text</a>]
                        [<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/PMEval/index.htm" target="_blank">  Project, source code, and dataset</a>]
                        <br>
                        <div id="divAbstract2012_tpami" class="pub_div_abstract"><br><b>Abstract:</b> 
                        Pattern matching is widely used in signal processing, computer vision, and image and video processing. 
                        Full search equivalent algorithms accelerate the pattern matching process and, in the meantime, 
                        yield exactly the same result as the full search. This paper proposes an analysis and comparison 
                        of state-of-the-art algorithms for full search equivalent pattern matching. Our intention is that 
                        the data sets and tests used in our evaluation will be a benchmark for testing future pattern 
                        matching algorithms, and that the analysis concerning state-of-the-art algorithms could inspire new fast algorithms. 
                        We also propose extensions of the evaluated algorithms and show that they outperform the original formulations.
                        </div>
                        <div id="divBibTex2012_tpami" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @ARTICLE{Ouyang:PME,                        
			  <br>author = {Wanli Ouyang and Federico Tombari and Stefano Mattoccia and Luigi
			  <br>Di Stefano and Wai-Kuen Cham},
			  <br>title = {Performance Evaluation of Full Search Equivalent Pattern Matching
			  <br>Algorithms},
			  <br>journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
			  <br>year = {2012},
			  <br>volume = {34},
			  <br>pages = {127 - 143},
			  <br>number = {1},
			  <br>month = {Jan.},
			  <br>}</code></div>
                      </td></tr>   
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number">  </td>
                       <td width="304px"> </td> 
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"> 
                      F. Tombari, <strong>Wanli Ouyang</strong>, L. Di Stefano, W.K. Cham, 
                      “Adaptive Low Resolution Pruning for Fast Full-Search Equivalent Pattern Matching,”
                      <em>Pattern Recognition Letters (JPRL)</em>, 32(15), pp 2119-2127, November 2011

                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2011_Jprl&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2011_Jprl&#39;)">BibTeX</a>]
												[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/jprl11b.pdf" target="_blank">  Full Text</a>]
                        <br>
                        <div id="divAbstract2011_Jprl" class="pub_div_abstract"> <br><b>Abstract:</b> 
                        Several recent proposals have shown the feasibility of signiﬁcantly speeding-up pattern matching by means of Full Search-equivalent techniques, i.e. without approximating the outcome of the search with
                        respect to a brute force investigation. These techniques are generally heavily based on efﬁcient incremental calculation schemes aimed at avoiding unnecessary computations. In a very recent and extensive
                        experimental evaluation, Low Resolution Pruning turned out to be in most cases the best performing
                        approach. In this paper we propose a computational analysis of several incremental techniques speciﬁ-
                        cally designed to enhance the efﬁciency of LRP. In addition, we propose a novel LRP algorithm aimed
                        at minimizing the theoretical number of operations by adaptively exploiting different incremental
                        approaches. We demonstrate the effectiveness of our proposal by means of experimental evaluation
                        on a large dataset
                         <br></div>
                        <div id="divBibTex2011_Jprl" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @ARTICLE{Ouyang:PME,                        
			  <br>author = {Federico Tombari and Wanli Ouyang and Luigi	Di Stefano and Wai-Kuen Cham},
			  <br>title = {“Adaptive Low Resolution Pruning for Fast Full-Search Equivalent Pattern Matching},
			  <br>journal = {Pattern Recognition Letters (JPRL)},
			  <br>year = {2011},
			  <br>volume = {32},
			  <br>pages = {2119-2127},
			  <br>number = {15},
			  <br>month = {Nov.},
			  <br>}</code></div>
                      </td></tr>                             
                                     
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="Photo" src="mainpa7.gif" width="336px" height="205px"> </td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"><strong> Wanli Ouyang </strong> and Wai-Kuen Cham, 
                      	"Fast algorithm for Walsh Hadamard transform on sliding windows", 
                      	<em> IEEE Trans. Pattern Analysis and machine Intelligence (TPAMI), </em>
                      	32(1):165-171, Jan. 2010.                      
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2010_PAMI&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2010_PAMI&#39;)">BibTeX</a>]
			[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Software/FWHT.m" target="_blank">Matlab Code for the proposed fast algorithm</a>]			
			[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/FWHT.htm" target="_blank"> Project & source code</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/Manu_FWHT.pdf" target="_blank">Full Text</a>]
                        <br>
                        <div id="divAbstract2010_PAMI" class="pub_div_abstract"><br><b>Abstract:</b> 
                        	This paper proposes a fast algorithm for Walsh Hadamard Transform on sliding windows 
                        	which can be used to implement pattern matching most efficiently. 
                        	The computational requirement of the proposed algorithm is about 1.5 additions per projection vector per sample, 
                        	which is the lowest among existing fast algorithms for Walsh Hadamard Transform on sliding windows.
                        <br></div>
                        <div id="divBibTex2010_PAMI" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@ARTICLE{Wanli:GCK,
			  <br>author = {W. Ouyang and W.K. Cham},
				  <br>title = {Fast Algorithm for {W}alsh {H}adamard transform on Sliding Windows},
				  <br>journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
				  <br>year = {2010},
				  <br>volume = {32},
				  <br>pages = {165-171},
				  <br>number = {1},
				  <br>month = {Jan.},
			<br>}</code></div>                        	
                      </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
			<td width="304px">  <img style="vertical-align: middle" alt="Ori_Edgeimage" border="0" src="mainpa3.gif" width="308" height="115"><br>
				<img border="0" alt="Edgeimage1" src="mainpa4.gif" width="100" height="100">
				<img border="0" alt="Edgeimage2" src="mainpa5.gif" width="100" height="101">
				<img border="0" alt="Edgeimage3" src="mainpa6.gif" width="100" height="100"></td>
		                    <td width="500px" style="vertical-align: middle" class="pub_td_text">Renqi Zhang, <strong> Wanli Ouyang </strong> and Wai-Kuen Cham,
                      	"Image Edge Detection Using Hidden Markov Chain Model Based on the Non-decimated Wavelet," 
                      	<em>International Journal of Signal Processing, Image Processing and Pattern, </em>
                      	Vol.1, No.2, March 2009, pp.109-117.       
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2010b_tcsvt&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2010b_tcsvt&#39;)">BibTeX</a>]
<!--                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/2010b_tcsvt.pdf" target="_blank">Full Text</a>] -->
                        <br>
                        <div id="divAbstract2010b_tcsvt" class="pub_div_abstract"><br><b>Abstract:</b> 
                        	Edge detection plays an important role in digital image processing. 
                        	Based on the non-decimated wavelet which is shift invariant, 
                        	in this paper, we develop a new edge detecting technique using hidden Markov chain (HMC) model. 
                        	With this proposed model (NWHMC), each wavelet coefficient contains a hidden state, 
                        	herein, we adopt Laplacian model and Gaussian model to represent the information of the state "big" and the state "small". 
                        	The model can be trained by EM algorithm, and then we employ Viterbi algorithm to reveal the hidden state of each coefficient 
                        	according to MAP estimation. The detecting results of several images are provided to evaluate the algorithm. 
                        	In addition, the algorithm can be applied to noisy images efficiently.
                        <br></div>
                        <div id="divBibTex2010b_tcsvt" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @ARTICLE{2010b_dong_tcsvt,
                        <br>author = {J. Dong and K. N. Ngan},
                          <br>title = {Parametric interpolation filter for {HD} video coding},
                          <br>journal = {IEEE Transactions on Circuits and Systems for Video Technology},
                          <br>year = {2010},
                          <br>volume = {20},
                          <br>pages = {1892-1897},
                          <br>number = {12},
                          <br>month = {Dec.}
                          <br>}</code></div>
                      </td></tr>
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img border="0" alt="NXP_DSPImage" src="MainpNXP.jpg" width="100" height="100"><img  alt="DCT" src="DCT.jpg" width="183" height="183"></td>
                      <td width="500px" style="vertical-align: middle" class="pub_td_text"><strong>Wanli Ouyang, </strong> C. Xiao, and G. liu.
                      	"A new IDCT and motion compensation algorithm based on VLIW (in Chinese)," 
                      	ACTA ELECTRONICA SINICA (One of the best Electronic Engineering journals in China), 
                      	33(11):2074-2079, Nov. 2005.                      	
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2010a_tcsvt&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2010a_tcsvt&#39;)">BibTeX</a>]
<!--                          [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/2010a_tcsvt.pdf" target="_blank">Full Text</a>] -->
                        <br>
                        <div id="divAbstract2010a_tcsvt" class="pub_div_abstract"><br><b>Abstract:</b> In 
				this paper, the matrix multiplication theory is utilized to obtain 
				Loeffler's DCT algorithm and Feig's DCT algorithm. In addition, the 
				Feig's algorithm is extended to other three forms. Utilizing matrix 
				decomposition representation, the links and differences between the two 
				algorithms are revealed. This decomposition representation helps 
				understanding and further improving the algorithms using matrix theory.
                        <br></div>
                        <div id="divBibTex2010a_tcsvt" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        <br>@ARTICLE{Ouyang:IDCT,
				<br>author = {Wanli Ouyang and Chuangbai Xiao and Guang liu},
				<br>title = {A New IDCT and Motion Compensation Algorithm Based on VLIW},
				 <br>journal = {ACTA ELECTRONICA SINICA},
				 <br>year = {2005},
				 <br>volume = {33},
				 <br>pages = {2074-2079},
				 <br>number = {11},
				 <br>month = {Nov.},
				 <br>owner = {wlouyang},
				 <br>timestamp = {2010.03.13}
				<br>}</code></div>
                      </td></tr>            
                    <tr class="pub_tr_1">  
                      <td width="20%" align="left" colspan="2" style="HEIGHT:10px"></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="20%" align="left" colspan="2"><a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/#Top">Back To Top</a></td></tr></tbody></table>
                      	
<!--========================Conference Papers ========================-->                      	
                  <h1>Conference Papers</h1>
                  <table border="0" cellspacing="0" cellpadding="5" width="900">
                    <tbody>
                    	

                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px"> <img alt="OHT_Complexity" border="0" src="./images/ChainCNN.JPG" width="304px" height="240">
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	<strong>Wanli Ouyang </strong>, KunWang, Xin Zhu, XiaogangWanli. "Chained Cascade Network
for Object Detection", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ouyang_Chained_Cascade_Network_ICCV_2017_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Wei Yang, Shuang Li, <strong>Wanli Ouyang</strong>, Hongsheng Li, XiaogangWang. "Learning
Feature Pyramids for Human Pose Estimation", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yang_Learning_Feature_Pyramids_ICCV_2017_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Yikang Li, <strong>Wanli Ouyang</strong>, Bolei Zhou, Kun Wang, Xiaogang Wang. "Scene
Graph Generation from Objects, Phrases and Region Captions", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Scene_Graph_Generation_ICCV_2017_paper.pdf" target="_blank">Full Text</a>]
																								<br>
                        <br>
                      	Qi Chu, <strong>Wanli Ouyang</strong>, Hongsheng Li, Xiaogang Wang, Bin Liu, Nenghai
Yu. "Online Multi-Object Tracking Using CNN-based Single Object Tracker with
Spatial-Temporal Attention Mechanism", <em>Proc. ICCV</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chu_Online_Multi-Object_Tracking_ICCV_2017_paper.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Dan Xu, <strong>Wanli Ouyang</strong>, Xavier Alameda-Pineda, Elisa Ricci, Xiaogang Wang,
Nicu Sebe. "Learning Deep Structured Multi-Scale Features using Attention-Gated
CRFs for Contour Prediction", <em>Proc. NIPS</em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="http://papers.nips.cc/paper/6985-learning-deep-structured-multi-scale-features-using-attention-gated-crfs-for-contour-prediction.pdf" target="_blank">Full Text</a>]
                       
                 </tr>       




                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                   <td width="304px"> <img alt="OHT_Complexity" border="0" src="./images/vip-cnn.jpg" width="304px" height="350">
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	Kai Kang, Hongsheng Li, <strong> W. Ouyang </strong>, Junjie  Yan, Xihui  Liu, Tong  Xiao, Xiaogang  Wang. ”Object Detection in Vidoes with Tubelet Proposal Networks”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/abs/1702.06355" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Feng  Zhu, Hongsheng Li, <strong> W. Ouyang </strong>, Nenghai  Yu, Xiaogang  Wang. ”Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/pdf/1702.05891" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Yu Liu, Junjie  Yan, <strong> W. Ouyang </strong>. ”Quality Aware Network for Set to Set Recognition”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/pdf/1704.03373" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Yikang  LI , <strong> W. Ouyang </strong>, Xiaogang  Wang. ”ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/pdf/1702.07191" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Xiao  Chu, Wei  Yang, <strong> W. Ouyang </strong>, Xiaogang  Wang, Alan  Yuille. ”Multi-Context Attention for Human Pose Estimation”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/pdf/1702.07432" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Source code" href="https://github.com/bearpaw/pose-attention" target="_blank">Code</a>]                        
                        
                        <br>
                        <br>
                      	Dan  Xu, Elisa  Ricci, <strong> W. Ouyang </strong>, Xiaogang  Wang, Nicu Sebe. Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/abs/1704.02157" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Dan  Xu,  <strong> W. Ouyang </strong>,  Elisa  Ricci, Xiaogang  Wang, Nicu Sebe. Learning Cross-Modal Deep Representations for Robust Pedestrian Detection”,<em> Proc. CVPR </em>, 2017.
                        [<a class="aLink" title="Download Full Text" href="https://arxiv.org/abs/1704.02431" target="_blank">Full Text</a>]

                 <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="./images/CRF_CNN.jpg" width="304px" height="65"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	X. Chu, <strong> W. Ouyang </strong>, H. Li, X. Wang. ”CRF-CNN: Modeling Structured Information in Human Pose Estimation”,<em> Advances In Neural Information Processing Systems (NIPS)</em>, 2016.
                        [<a class="aLink" title="Download Full Text" href="./Papers/CRF-CNN_NIPS16" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Demo Results" href="http://www.ee.cuhk.edu.hk/~xchu/files/nips_2016_supp.pdf" target="_blank">Demo Results</a>]
                      </td>
                 </tr>       


                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="./projects/GBD/images/motivation.jpg" width="304px" height="200"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	Xingyu Zeng, <strong>Wanli Ouyang</strong>, Bin Yang, Junjie Yan, Xiaogang
                      	"Gated Bidirectional CNN for Object Detection",  In <em>Proc. ECCV </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/GBD.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Z. Wang, H. Li, <strong>W. Ouyang </strong>, X. Wang Wanli
                      	"Learnable Histogram: Statistical Context Features for Deep Neural Networks",  In <em>Proc. ECCV </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/HistDNN.pdf" target="_blank">Full Text</a>]
                        
                    </td>
                 </tr>       
                        
                      	
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/ImageNetFactors.jpg" width="304px" height="200"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	<strong>Wanli Ouyang</strong>, X. Wang, C. Zhang, and X. Yang.
                      	"Factors in finetuning deep model for object detection",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/OuyangFactors_CVPR16.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Wei Yang, <strong>Wanli Ouyang</strong>, Hongsheng Li and Xiaogang Wang
                      	"End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation",  In <em>Proc. CVPR </em> 2016 (Oral). 
                        [<a class="aLink" title=" Download Full Text" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/yangOLWcvpr16.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title=" Project" href="http://www.ee.cuhk.edu.hk/~wyang/Deep-Deformable-Mixture-of-Parts-for-Human-Pose-Estimation/" target="_blank">Project</a>]
                        <br>
                        <br>
                      	Lijun Wang, <strong>Wanli Ouyang</strong>, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu.
                      	"STCT: Sequentially Training Convolutional Networks for Visual Tracking",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/WangLJ_CVPR16.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	K. Kang, <strong>Wanli Ouyang</strong>, H. Li, and X. Wang.
                      	"Object detection from video tubelets with convolutional neural networks",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	X. Chu,  <strong>Wanli Ouyang</strong> , H. Li, and X. Wang. 
                      	"Structured feature learning for pose estimation",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="http://arxiv.org/pdf/1603.09065.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Project" href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_structured_feature_pose.html" target="_blank">Project and dataset </a>]                          
                        [<a class="aLink" title="Project" href="https://www.youtube.com/watch?v=SMFt6TJ-ntA" target="_blank">Spotlight talk</a>]                          
                        [<a class="aLink" title="Code" href="https://github.com/chuxiaoselena/StructuredFeature" target="_blank">Source code </a>]                          
                        [<a class="aLink" title="Supplementary" href="http://www.ee.cuhk.edu.hk/~xgwang/StructureFeature/supp.pdf" target="_blank">Supplementary </a>]                          
                          <br>
                        <br>
                      	Tong Xiao, Hongsheng Li, <strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification",  In <em>Proc. CVPR </em> 2016. 
                        [<a class="aLink" title="Full Text on Arxiv" href="http://arxiv.org/abs/1604.07528" target="_blank">Full Text</a>]
                        <br>
                        <br>
                      	Hongyang Li, <strong>Wanli Ouyang</strong>, Xiaogang Wang
                      	"Multiple Bias on Non-linearity Activation in Deep Neural Networks",  In <em>Proc. ICML </em> 2016. 
                        [<a class="aLink" title="Download Full Text" href="./MBA_icml16.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Slides" href="http://www.ee.cuhk.edu.hk/~yangli/icml16_bias.pdf" target="_blank">Slides </a>]                          
                        [<a class="aLink" title="Code" href="https://github.com/hli2020/caffe/tree/bias" target="_blank">Code on Github </a>]                          
                      	
                    </td>
                 </tr>       



                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="projects/ImageNetAttribute/dataset.png" width="304px" height="120"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	<strong>Wanli Ouyang</strong>, Hongyang Li, Xingyu Zeng, and Xiaogang Wang,
                      	"Learning Deep Representation with Large-scale Attributes",  In <em>Proc. ICCV </em> 2015. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_ICCV&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_ICCV&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/ImageNetAttributes_ICCV15.pdf" target="_blank">Full Text</a>]  
                        [<a class="aLink" title="Project" href="./projects/ImageNetAttribute/iccv15.html" target="_blank">Project and dataset </a>]                          
                        <br>
                        <div id="divAbstract2015_ICCV" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        Learning strong feature representations from large scale supervision has achieved remarkable success in computer
vision as the emergence of deep learning techniques. It is
driven by big visual data with rich annotations. This paper
contributes a large-scale object attribute database that
contains rich attribute annotations (over 300 attributes)
for ∼180k samples and 494 object classes. Based on the
ImageNet object detection dataset, it annotates the rotation,
viewpoint, object part location, part occlusion, part
existence, common attributes, and class-specific attributes.
Then we use this dataset to train deep representations and
extensively evaluate how these attributes are useful on the
general object detection task. In order to make better use
of the attribute annotations, a deep learning scheme is proposed
by modeling the relationship of attributes and hierarchically
clustering them into semantically meaningful mixture
types. Experimental results show that the attributes are
helpful in learning better features and improving the object
detection accuracy by 2.6% in mAP on the ILSVRC 2014
object detection dataset and 2.4% in mAP on PASCAL VOC
2007 object detection dataset. Such improvement is well
generalized across datasets.
                            <br></div>
                        <div id="divBibTex2015_ICCV" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@CONFERENCE{ouyang2015learning,
   <br> title={Learning Deep Representation With Large-Scale Attributes},
   <br>author={Ouyang, Wanli and Li, Hongyang and Zeng, Xingyu and Wang, Xiaogang},
   <br>booktitle={Proceedings of the IEEE International Conference on Computer Vision},
   <br>pages={1895--1903},
   <br> year={2015}
                        <br>}</code></div>
                        </td></tr>                        
               
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/Immediacy.PNG" width="304px" height="150"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	Xiao Chu, <strong>Wanli Ouyang</strong>, Wei Yang, and Xiaogang Wang,
                      	"Multi-task Recurrent Neural Network for Immediacy Prediction",  In <em>Proc. ICCV </em> 2015.<strong> (Oral) </strong>
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_ICCVXchu&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_ICCVXchu&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/Chu_Multi-Task_Recurrent_Neural_ICCV_2015_paper.pdf" target="_blank">Full Text</a>]  
                        [<a class="aLink" title="Project" href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_immediacy.html" target="_blank">Project and dataset </a>]                          
                        [<a class="aLink" title="Oral" href="http://videolectures.net/iccv2015_chu_neural_network/" target="_blank">Oral presentation on videolectures</a>]                          
                        [<a class="aLink" title="Poster" href="http://www.ee.cuhk.edu.hk/~xchu/files/iccv_2015_poster.pdf" target="_blank">Poster</a>]                          
                        <br>
                        <div id="divAbstract2015_ICCVXchu" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        In this paper, we propose to predict immediacy for interacting persons from still images. A complete immediacy
set includes interactions, relative distance, body leaning direction and standing orientation. These measures are found
to be related to the attitude, social relationship, social interaction, action, nationality, and religion of the communicators. A large-scale dataset with 10, 000 images is
constructed, in which all the immediacy cues and the human poses are annotated. We propose a rich set of immediacy representations that help to predict immediacy from
imperfect 1-person and 2-person pose estimation results. A
multi-task deep recurrent neural network is constructed to
take the proposed rich immediacy representations as the input and learn the complex relationship among immediacy
predictions through multiple steps of refinement. The effectiveness of the proposed approach is proved through extensive experiments on the large-scale dataset.

                            <br></div>
                        <div id="divBibTex2015_ICCVXchu" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@inproceedings{chu2015multi,
  <br>title={Multi-Task Recurrent Neural Network for Immediacy Prediction},
  <br>author={Chu, Xiao and Ouyang, Wanli and Yang, Wei and Wang, Xiaogang},
  <br>booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  <br>pages={3352--3360},
  <br>year={2015}
<br>}</code></div>
</td></tr>                        
             

                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/Tracking_ICCV15.png" width="304px" height="150"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <br>
                      	Lijun Wang,  <strong>Wanli Ouyang</strong>, Xiaogang Wang, and Huchuan Lu,
                      	"Visual Tracking with Fully Convolutional Networks",  In <em>Proc. ICCV </em> 2015. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_ICCVWang&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_ICCVWang&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="Papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf" target="_blank">Full Text</a>]  
                      [<a class="aLink" title="Project" href="http://scott89.github.io/FCNT/" target="_blank">Project and source code </a>]                
                        <br>
                        <div id="divAbstract2015_ICCVWang" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        We propose a new approach for general object tracking
with fully convolutional neural network. Instead of treating
convolutional neural network (CNN) as a black-box feature
extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and
classification task on ImageNet. The discoveries motivate
the design of our tracking system. It is found that convolutional layers in different levels characterize the target from
different perspectives. A top layer encodes more semantic
features and serves as a category detector, while a lower
layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism
during tracking. It is also found that for a tracking target,
only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy
and improve tracking accuracy. Extensive evaluation on the
widely used tracking benchmark [36] shows that the proposed tacker outperforms the state-of-the-art significantly.
                            <br></div>
                        <div id="divBibTex2015_ICCVWang" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
@inproceedings{wang2015visual,
  <br>title={Visual Tracking With Fully Convolutional Networks},
  <br>author={Wang, Lijun and Ouyang, Wanli and Wang, Xiaogang and Lu, Huchuan},
  <br>booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  <br>pages={3119--3127},
  <br>year={2015}
<br>}</code></div>
</td></tr>                        

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/overview_DeepID.png" width="304px" height="120"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> <strong>Details that show how our team achieve #2 in the ImageNet Large Scale Visual Recognition Challenge 2014: </strong> <br>
                      	<strong>Wanli Ouyang</strong>, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping Luo, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy and Xiaoou Tang,
                      	"DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection",  In <em>Proc. CVPR </em> 2015. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_CVPR1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_CVPR1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/DeepIDNet_CVPR15.pdf" target="_blank">Full Text</a>]  
                        [<a class="aLink" title="Project" href="./projects/ImageNet/index.html" target="_blank">Project</a>]                          
                        <br>
                        <div id="divAbstract2015_CVPR1" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        In this paper, we propose deformable deep convolutional
neural networks for generic object detection. This new deep
learning object detection diagram has innovations in multiple
aspects. In the proposed new deep architecture, a
new deformation constrained pooling (def-pooling) layer
models the deformation of object parts with geometric constraint
and penalty. A new pre-training strategy is proposed
to learn feature representations more suitable for the object
detection task and with good generalization capability. By
changing the net structures, training strategies, adding and
removing some key components in the detection pipeline,
a set of models with large diversity are obtained, which
significantly improves the effectiveness of model averaging.
The proposed approach improves the mean averaged precision
obtained by RCNN [13], which is the state-of-the-art,
from 31% to 50.3% on the ILSVRC2014 detection dataset.
Detailed component-wise analysis is also provided through
extensive experimental evaluation, which provide a global
view for people to understand the deep learning object detection
pipeline.
                        <br></div>
                        <div id="divBibTex2015_CVPR1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
				@CONFERENCE{ouyang2015deepid,
				  <br>author = {Ouyang, Wanli and Wang, Xiaogang and Zeng, Xingyu and Qiu, Shi and Luo, Ping and Tian, Yonglong and Li, Hongsheng and Yang, Shuo and Wang, Zhe and Loy, Chen-Change, and Tang, Xiaoou},
				  <br>title = {DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection},
				  <br>booktitle = {CVPR},
				  <br>year = {2015},
                        <br>}</code></div>
                        </td></tr>                        
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/ZhaoCVPR15.JPG" width="304px" height="120"></td> 
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">  <br>
                      	Rui Zhao, <strong>Wanli Ouyang</strong>,  Hongsheng Li, and Xiaogang Wang,
                      	"Saliency Detection by Multi-context Deep Learning",  In <em>Proc. CVPR </em> 2015. 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2015_CVPR2&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2015_CVPR2&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~rzhao/project/deepsal_cvpr15/zhaoOLWcvpr15.pdf" target="_blank">Full Text</a>]  
                        [<a class="aLink" title="Code" href="https://github.com/Robert0812/deepsaldet" target="_blank">Code</a>]                          
                        [<a class="aLink" title="Supplementary Material" href="http://www.ee.cuhk.edu.hk/~rzhao/project/deepsal_cvpr15/zhaoOLWcvpr15.html" target="_blank">Supplementary Material</a>]                          
                        <br>
                        <div id="divAbstract2015_CVPR2" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        Low-level saliency cues or priors do not produce good
enough saliency detection results especially when the
salient object presents in a low-contrast background with
confusing visual appearance. This issue raises a serious
problem for conventional approaches. In this paper, we
tackle this problem by proposing a multi-context deep learning
framework for salient object detection. We employ deep
Convolutional Neural Networks to model saliency of objects
in images. Global context and local context are both taken
into account, and are jointly modeled in a unified multicontext
deep learning framework.

To provide a better initialization for training the deep
neural networks, we investigate different pre-training
strategies, and a task-specific pre-training scheme is designed
to make the multi-context modeling suited for
saliency detection. Furthermore, recently proposed contemporary
deep models in the ImageNet Image Classification
Challenge are tested, and their effectiveness in saliency detection
are investigated. Our approach is extensively evaluated
on five public datasets, and experimental results show
significant and consistent improvements over the state-ofthe-art
methods

                        <br></div>                        
                         <div id="divBibTex2015_CVPR2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
				@CONFERENCE{ouyang2015deepid,
  <br>title={Saliency Detection by Multi-Context Deep Learning},
  <br>author={Zhao, Rui and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
	<br>booktitle = {CVPR},
  <br>year={2015}
                        <br>}</code></div>
                        </td></tr>                        
                        
                                               
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/ZengECCV14.jpg" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">Xinyu Zeng, <strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Deep Learning of Scene-Specific Classifier for Pedestrian Detection", In <em>Proc. ECCV </em> 2014.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://link.springer.com/chapter/10.1007/978-3-319-10578-9_31" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV1" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        	The performance of a detector depends much on its training
dataset and drops significantly when the detector is applied to a new scene due to the large variations between the source training dataset
and the target scene. In order to bridge this appearance gap, we propose a deep model to automatically learn scene-specific features and
visual patterns in static video surveillance without any manual labels from the target scene. It jointly learns a scene-specific classifier and the
distribution of the target samples. Both tasks share multi-scale feature
representations with both discriminative and representative power. We also propose a cluster layer in the deep model that utilizes the scene-specific visual patterns for pedestrian detection. Our specifically designed
objective function not only incorporates the condence scores of target training samples but also automatically weights the importance of source
training samples by fitting the marginal distributions of target samples.
It significantly improves the detection rates at 1 FPPI by 10% compared with the state-of-the-art domain adaptation methods on MIT TrafficDataset and CUHK Square Dataset.                        	
                        <br></div>
                        <div id="divBibTex2013_ICCV1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Zeng2014Deep,
			  <br>author = {X. Zeng and W. Ouyang and X. Wang},
			  <br>title = {Deep Learning of Scene-Specific Classifier for Pedestrian Detection},
			  <br>booktitle = {ECCV},
			  <br>year = {2014},
                        <br>}</code></div>
                        </td></tr>
                        
                                            	
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/PoseDBN.PNG" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Xiao Chu, Xiaogang Wang, 
                      	"Multi-source Deep Learning for Human Pose Estimation", In <em>Proc. IEEE CVPR </em> 2014.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/ouyangCWcvpr14.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV1" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        	Visual appearance score, appearance mixture type and deformation are three important information sources for human pose estimation. This paper proposes to build a multi-source deep model  in order to extract non-linear representation from these different aspects of information sources. With the deep model, the global, high-order human body articulation patterns in these information sources are extracted for pose estimation. The task for estimating body locations and the task for human detection are jointly learned using a unified deep model. The proposed approach can be viewed as a post-processing of pose estimation results and can flexibly integrate with existing methods by taking their information sources as input.
By extracting the non-linear representation from multiple information sources, the deep model outperforms  state-of-the-art by up to 8.6 percent on three public benchmark datasets.
                        	
                        <br></div>
                        <div id="divBibTex2013_ICCV1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang2014Multi,
			  <br>author = {W. Ouyang and X. Chu and X. Wang},
			  <br>title = {Multi-source Deep Learning for Human Pose Estimation},
			  <br>booktitle = {CVPR},
			  <br>year = {2014},
                        <br>}</code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/ZhaoCVPR14.JPG" width="304px" height="180px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> Rui Zhao,  <strong>Wanli Ouyang</strong>, and Xiaogang Wang, 
                      	"Learning Mid-level Filters for Person Re-Identfiation", In <em>Proc. IEEE CVPR </em> 2014.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2014_CVPR2&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2014_CVPR2&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project page" href="http://www.ee.cuhk.edu.hk/~rzhao/project/midfilter_cvpr14/zhaoOWcvpr14.html" target="_blank">Project</a>]  
                        [<a class="aLink" title="Download Code" href="https://github.com/Robert0812/midfilter_reid" target="_blank">Code</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~rzhao/project/midfilter_cvpr14/zhaoOWcvpr14.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2014_CVPR2" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> 
                        	In this paper, we propose a novel approach of learning
mid-level filters from automatically discovered patch clusters
for person re-identification. It is well motivated by our
study on what are good filters for person re-identification.
Our mid-level filters are discriminatively learned for identifying
specific visual patterns and distinguishing persons,
and have good cross-view invariance. First, local patches
are qualitatively measured and classified with their discriminative
power. Discriminative and representative patches
are collected for filter learning. Second, patch clusters with
coherent appearance are obtained by pruning hierarchical
clustering trees, and a simple but effective cross-view
training strategy is proposed to learn filters that are viewinvariant
and discriminative. Third, filter responses are integrated
with patch matching scores in RankSVM training.
The effectiveness of our approach is validated on the VIPeR
dataset and the CUHK01 dataset. The learned mid-level
features are complementary to existing handcrafted lowlevel
features, and improve the best Rank-1 matching rate
on the VIPeR dataset by 14%
                        	
                        <br></div>
                        <div id="divBibTex2014_CVPR2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			<br>@CONFERENCE{Ouyang2014Multi,
 <br>title = {Learning Mid-level Filters for Person Re-identfiation},
 <br>author={Zhao, Rui and Ouyang, Wanli and Wang, Xiaogang},
 <br>booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 <br>year = {2014},
 <br>month = {June},
 <br>address = {Columbus, USA}
                         <br>}</code></div>
                        </td></tr>

                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/jointdeep.PNG" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Joint Deep Learning for Pedestrian Detection ", In <em>Proc. IEEE ICCV </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ouyangWiccv13Joint/index.html" target="_blank">Project & Source code</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/ouyangWiccv13.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV1" class="pub_div_abstract"><br>
                        	<b>Abstract:</b> Feature extraction, deformation handling, occlusion
handling, and classification are four important components
in pedestrian detection. Existing methods learn or design
these components either individually or sequentially. The
interaction among these components is not yet well explored.
This paper proposes that they should be jointly
learned in order to maximize their strengths through cooperation.
We formulate these four components into a joint
deep learning framework and propose a new deep network
architecture. By establishing automatic, mutual interaction
among components, the deep model achieves a 9% reduction
in the average miss rate compared with the current
best-performing pedestrian detection approaches on
the largest Caltech benchmark dataset.
                        	
                        <br></div>
                        <div id="divBibTex2013_ICCV1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang2013Joint,
			  <br>author = {W. Ouyang and X. Wang},
			  <br>title = {Joint Deep Learning for Pedestrian Detection},
			  <br>booktitle = {ICCV},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/cascade.PNG" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> Xingyu Zeng, <strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Multi-Stage Contextual Deep Learning for Pedestrian Detection ", In <em>Proc. IEEE ICCV </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV2&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV2&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="" target="_blank">Project </a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/zengOWiccv13.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV2" class="pub_div_abstract"><br><b>Abstract:</b> 
                        	Multi-Stage classifiers have been widely used in pedestrian
detection and achieved great success. However, these
classifiers are usually trained sequentially without joint optimization.
In this paper, we propose a new deep architecture
that can jointly train multiple classifiers through several
stages of back-propagation. Unsupervised pre-training
and specifically designed stage-wise supervised training are
used to regularize the optimization problem. Through a specific
design of the training scheme, this deep architecture
is able to simulate the cascaded classifiers in using hard
samples to train the network stage-by-stage. Both theoretical
analysis and experimental results show that the training
scheme helps to avoid overfitting. Experimental results
on three datasets(Caltech, ETH and TudBrussels) show that
our approach performs better than the state-of-the-art approaches.                        	
                        <br></div>
                        <div id="divBibTex2013_ICCV2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Zeng2013Multi,
			  <br>author = {X. Zeng and W. Ouyang and X. Wang},
			  <br>title = {Multi-Stage Contextual Deep Learning for Pedestrian Detection},
			  <br>booktitle = {ICCV},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="imgs/pedestrian_salience2.PNG" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> Rui Zhao, <strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Person Re-identification by Salience Matching ", In <em>Proc. IEEE ICCV </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_ICCV3&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_ICCV3&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="" target="_blank">Project</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~rzhao/papers/zhaoOWiccv13.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_ICCV3" class="pub_div_abstract"><br><b>Abstract:</b> 
Human salience is distinctive and reliable information
in matching pedestrians across disjoint camera views. In
this paper, we exploit the pairwise salience distribution relationship
between pedestrian images, and solve the person
re-identification problem by proposing a salience matching
strategy. To handle the misalignment problem in pedestrian
images, patch matching is adopted and patch salience
is estimated. Matching patches with inconsistent salience
brings penalty. Images of the same person are recognized
by minimizing the salience matching cost. Furthermore, our
salience matching is tightly integrated with patch matching
in a unified structural RankSVM learning framework.
The effectiveness of our approach is validated on the VIPeR
dataset and the CUHK Campus dataset. It outperforms the
state-of-the-art methods on both datasets.
                        <br></div>
                        <div id="divBibTex2013_ICCV3" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Zhao2013Person,
			  <br>author = {R. Zhao and W. Ouyang and X. Wang},
			  <br>title = {Person Re-identification by Salience Matching},
			  <br>booktitle = {ICCV},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>
                                                
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="two_pedestrian.PNG" width="304px" height="156px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Xiaogang Wang, 
                      	"Single-Pedestrian Detection aided by Multi-pedestrian Detection ", In <em>Proc. IEEE CVPR </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_CVPR1&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_CVPR1&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ouyangWcvpr13MultiPed/index.html" target="_blank">Project & Source code</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/Ouyang2013MultiPed.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_CVPR1" class="pub_div_abstract"><br><b>Abstract:</b> 
                        	In this paper, we address the challenging problem of detecting
pedestrians who appear in groups and have interaction.
A new approach is proposed for single-pedestrian
detection aided by multi-pedestrian detection. A mixture
model of multi-pedestrian detectors is designed to capture
the unique visual cues which are formed by nearby multiple
pedestrians but cannot be captured by single-pedestrian
detectors. A probabilistic framework is proposed to model
the relationship between the configurations estimated by
single- and multi-pedestrian detectors, and to refine the
single-pedestrian detection result with multi-pedestrian detection.
It can integrate with any single-pedestrian detector
without significantly increasing the computation load. 15
state-of-the-art single-pedestrian detection approaches are
investigated on three widely used public datasets: Caltech,
TUD-Brussels and ETH. Experimental results show that our
framework significantly improves all these approaches. The
average improvement is 9% on the Caltech-Test dataset,
11% on the TUD-Brussels dataset and 17% on the ETH
dataset in terms of average miss rate. The lowest average
miss rate is reduced from 48% to 43% on the Caltech-Test
dataset, from 55% to 50% on the TUD-Brussels dataset and
from 51% to 41% on the ETH dataset.
                        <br></div>
                        <div id="divBibTex2013_CVPR1" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang2013MultiPed,
			  <br>author = {W. Ouyang and X. Wang},
			  <br>title = {Single-Pedestrian Detection aided by Multi-pedestrian Detection},
			  <br>booktitle = {CVPR},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="mutual_visibility.PNG" width="304px" height="206px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Xingyu Zeng and Xiaogang Wang, 
                      	"Modeling Mutual Visibility Relationship in Pedestrian Detection ", In <em>Proc. IEEE CVPR </em> 2013.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2013_CVPR2&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2013_CVPR2&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ouyangZWcvpr13MutVisibility/index.html" target="_blank">Project</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/Ouyang2013MutualDBN.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2013_CVPR2" class="pub_div_abstract"><br><b>Abstract:</b> Detecting pedestrians in cluttered scenes is a challenging
problem in computer vision. The difficulty is
added when several pedestrians overlap in images and occlude
each other. We observe, however, that the occlusion/
visibility statuses of overlapping pedestrians provide
useful mutual relationship for visibility estimation - the visibility
estimation of one pedestrian facilitates the visibility
estimation of another. In this paper, we propose a mutual
visibility deep model that jointly estimates the visibility
statuses of overlapping pedestrians. The visibility relationship
among pedestrians is learned from the deep model
for recognizing co-existing pedestrians. Experimental results
show that the mutual visibility deep model effectively
improves the pedestrian detection results. Compared with
existing image-based pedestrian detection approaches, our
approach has the lowest average miss rate on the Caltech-
Train dataset, the Caltech-Test dataset and the ETH dataset.
Including mutual visibility leads to 4%−8% improvements
on multiple benchmark datasets.
                        <br></div>
                        <div id="divBibTex2013_CVPR2" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang2013MutualDBN,
			  <br>author = {Wanli Ouyang and Xingyu Zeng and Xiaogang Wang},
			  <br>title = {Modeling Mutual Visibility Relationship in Pedestrian Detection},
			  <br>booktitle = {CVPR},
			  <br>year = {2013},
                        <br>}</code></div>
                        </td></tr>

                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="pedestrian_salience.PNG" width="304px" height="256px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"> Rui Zhao <strong>Wanli Ouyang</strong>, and Xiaogang Wang, 
                      	"Unsupervised Salience Learning for Person Re-identification ", In <em>Proc. IEEE CVPR </em> 2013.
                        [<a class="aLink" title="Project webpage" href="http://mmlab.ie.cuhk.edu.hk/projects/project_salience_reid/index.html" target="_blank">Project & source code</a>]  
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/ZhaoCVPR2013Salience.pdf" target="_blank">PDF</a>]  
                        </code></div>
                        </td></tr>
                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img border="0" alt="HumanDBNDetRes" src="HumanDBNDetRes.jpg" width="304px" height="200px"> </td>
                      <td class="pub_td_text"  width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong> and Xiaogang Wang, 
                      	"A Discriminative Deep Model for Pedestrian Detection with Occlusion Handling," 
                      	In <em>Proc. IEEE CVPR </em> 2012.                      	
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2012_CVPR&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2012_CVPR&#39;)">BibTeX</a>]
                        [<a class="aLink" title="Project webpage" href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ouyangWcvpr12Occ/index.html" target="_blank">Project</a>]  
			[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/CDBN-Ped12.pdf" target="_blank">  Full Text</a>]
			[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~xgwang/CUHK_pedestrian.html" target="_blank">CUHK Occlusion Dataset</a>]
<!--                          [<a class="aLink" title="Download Poster" href="http://www.ee.cuhk.edu.hk/~wlouyang/2009_icip_poster.pdf" target="_blank">Poster</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/2009_icip.pdf" target="_blank">Full Text</a>]
                        [<a class="aLink" title="Download Errata" href="http://www.ee.cuhk.edu.hk/~wlouyang/2009_icip_errata.pdf" target="_blank">Errata</a>]     
-->                        <br>
                        <div id="divAbstract2012_CVPR" class="pub_div_abstract"><br><b>Abstract:</b> Part-based models have demonstrated their merit in object detection.
			However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions or large
			deformations. To handle the imperfectness of part detectors, this paper presents a probabilistic pedestrian detection framework.
			In this framework, a deformable part-based model is used to obtain the scores of part detectors and the visibilities of parts
			are modeled as hidden variables. Unlike previous occlusion handling approaches that assume independence among visibility probabilities of parts
			or manually define rules for the visibility relationship,
			a discriminative deep model is used in this paper for learning the visibility relationship among overlapping parts at multiple layers.
			Experimental results on three public datasets (Caltech, ETH and Daimler) and a new dataset (the new dataset will be released to the public)
			specially designed for the evaluation of occlusion handling approaches show the effectiveness of the proposed approach.
                        <br></div>
                        <div id="divBibTex2012_CVPR" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @CONFERENCE{Ouyang:DBNHuman,
			 <br>author = {Wanli Ouyang and Xiaogang Wang},
			 <br>title = {A Discriminative Deep Model for Pedestrian Detection with Occlusion
			 <br>Handling},
			 <br>booktitle = {CVPR},
			 <br>year = {2012},
			}</code></div>
                        </td></tr>
                                                                        
                    <tr class="pub_tr_2">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img alt="OHT_Complexity" border="0" src="mainpa9.gif" width="304px" height="206px"></td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle"><strong>Wanli Ouyang</strong>, Renqi Zhang and Wai-Kuen Cham, 
                      	"Fast pattern matching using orthogonal Haar transform ", In <em>Proc. IEEE CVPR </em> 2010.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2011_iscas&#39;)">Abstract</a>] 
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2011_iscas&#39;)">BibTeX</a>]
			[<a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/OHT.htm" target="_blank">Project & source code</a>]
                        [<a class="aLink" title="Download Slides" href="http://www.ee.cuhk.edu.hk/~wlouyang/PPT/OHT1.ppt" target="_blank">Slides</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/Manu_OHT.pdf" target="_blank">Full Text</a>]  
                        <br>
                        <div id="divAbstract2011_iscas" class="pub_div_abstract"><br><b>Abstract:</b> This paper introduces 
                        	strip sum on the image. The sum of pixels in a rectangle can be computed by 
                        	one addition using the strip sum. Then we propose to use the orthogonal Haar 
                        	transform (OHT) for pattern matching. Applied for pattern matching, 
                        the algorithm using strip sum requires <i>O</i>(log <i>u</i>) additions per pixel to 
                        	project input data of size <i>N</i> by <i>N</i> onto <i>u</i> 2-D OHT basis while existing fast algorithms require 
                        	<i>O</i>(<i>u</i>) additions per pixel to project the same data onto <i>u</i> 2-D WHT or GCK basis.
                        <br></div>
                        <div id="divBibTex2011_iscas" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
			@CONFERENCE{Ouyang:OHT,
			  <br>author = {W. Ouyang and R. Zhang and W.K. Cham},
			  <br>title = {Fast pattern matching using orthogonal {H}aar transform},
			  <br>booktitle = {CVPR},
			  <br>year = {2010},
                        <br>}</code></div>
                        </td></tr>
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img border="0" alt="Img_coded" src="mainpa12.jpg" width="147px" height="161px"><img border="0"  alt="Img_filtered" src="mainpa13.jpg" width="147px" height="161px"> </td>
                      <td class="pub_td_text" width="500px" style="vertical-align: middle">Renqi Zhang, <strong>Wanli Ouyang</strong> and Wai-Kuen Cham, 
                      	"Image Deblocking using Dual Adaptive FIR Wiener Filter in the DCT Transform Domain," 
                      	<em>In Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing</em>, ICASSP 2009, 
                      	Taiwan, April 19-24, 2009, pp.1181-1184.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2009_icip&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2009_icip&#39;)">BibTeX</a>]
<!--                        [<a class="aLink" title="Download Errata" href="http://www.ee.cuhk.edu.hk/~wlouyang/2009_icip_errata.pdf" target="_blank">Errata</a>]     
-->                        <br>
                        <div id="divAbstract2009_icip" class="pub_div_abstract"><br><b>Abstract:</b> Blocking artifacts exist in images and video sequences
						compressed to low bit rates using block discrete cosine
						transform (DCT) compression standards. In order to reduce
						blocking artifacts, a novel DCT domain technique is
						presented in this paper. Firstly, a new FIR Wiener filter
						which exploits the dependence of neighboring DCT
						coefficients based on the linear minimum
						mean-square-error (LMMSE) criterion is proposed. Then
						we apply the new FIR Wiener filter twice in a dual adaptive
						filtering structure to restore each quantized DCT coefficient.
						In addition, an efficient parameter estimation method is
						proposed for the designed filter. Experimental results show
						that the performance of the proposed method is comparable
						to the state-of-the-art methods but has low computational
						complexity.
                        <br></div>
                        <div id="divBibTex2009_icip" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @CONFERENCE{Zhangrq:WienerDeblkICAASP,
				  <br>author = {Renqi Zhang and Wanli Ouyang and W.K. Cham},
				  <br>title = {Image Deblocking using Dual Adaptive FIR Wiener Filter in the {DCT} Transform Domain},
				  <br>booktitle = {Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP)},
				  <br>year = {2009},
				  <br>pages = {1181-1184},
				  <br>address = {Taiwan},
			  <br>month = {April 19-24},
                        <br>}</code></div>
                        </td></tr>
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
                      <td width="304px"> <img border="0" alt="Img_ori" src="mainpa14.jpg" width="100px" height="101px"><img border="0"  alt="Img_canny" src="mainpa15.gif" width="100px" height="101px"> <img border="0"  alt="Img_our3DEdge" src="mainpa16.gif" width="100px" height="101px">  </td>
                      <td class="pub_td_text">Renqi Zhang, <strong>Wanli Ouyang</strong> and Wai-Kuen Cham, 
                      	"Image Multi-scale Edge Detection using 3-D Hidden Markov Model based on the Non-decimated Wavelet," 
                      	In <em> Proc. 2009 IEEE International Conference on Image Processing</em> (ICIP), 
                      	Cairo, Egypt, November 7-10, 2009, pp.2173-2176.
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide Abstract" onclick="JavaScript:menu(&#39;divAbstract2007_iscas&#39;)">Abstract</a>]
                        [<a style="CURSOR: pointer" class="aLink" title="Show/Hide BibTeX" onclick="JavaScript:menu(&#39;divBibTex2007_iscas&#39;)">BibTeX</a>]
<!--                        [<a class="aLink" title="Download Slides" href="http://www.ee.cuhk.edu.hk/~wlouyang/2007_iscas_lecture.pdf" target="_blank">Slides</a>]
                        [<a class="aLink" title="Download Full Text" href="http://www.ee.cuhk.edu.hk/~wlouyang/2007_iscas.pdf" target="_blank">Full Text</a>] -->
                        <br>
                        <div id="divAbstract2007_iscas" class="pub_div_abstract"><br><b>Abstract:</b> Edge detection plays an important role in digital image
			processing. Based on the non-decimated wavelet which is
			shift-invariant, in this paper, we develop a new edge
			detecting technique using 3-D Hidden Markov Model.
			Our proposed model can not only capture the relationship
			of the wavelet coefficients inter-scale, but also consider
			the intra-scale dependence. A computationally efficient
			maximum likelihood (ML) estimation algorithm is
			employed to compute parameters and the hidden state of
			each coefficient is revealed by maximum a posteriori
			(MAP) estimation. Experimental results of natural images
			are provided to evaluate the algorithm. In addition, the
			proposed model has the potential to be an efficient
			multi-scale statistical modeling tool for other image or
			video processing tasks.
                        <br></div>
                        <div id="divBibTex2007_iscas" class="pub_div_abstract"><br><b>BibTeX entry:</b><br><code>
                        @CONFERENCE{Renqi:Edge,
			  <br>author = {Renqi Zhang and Wanli Ouyang and Wai-Kuen Cham},
			  <br>title = {Image Multi-scale Edge Detection using 3-D Hidden Markov Model based
			  <br>on the Non-decimated Wavelet},
			  <br>booktitle = {Proc. 2009 IEEE International Conference on Image Processing (ICIP)},
			  <br>year = {2009},
			  <br>}</code></div>
                        </td></tr>
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
		      <td width="304px"> </td> 
                      <td class="pub_td_text"> <strong>Wanli Ouyang</strong>, D. Song, C. Xiao, and W. Ju. 
                      	The matrix decomposition representation of DCT algorithms. 
                      	In <em>IEEE midwest sym. Circuits and Syst.</em> (MWCAS), 2005.
                        <br>            
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
		      <td width="304px"> </td> 
                      <td class="pub_td_text"> <strong>Wanli Ouyang</strong>, C. Xiao, W. Ju, and D. Song. The dynamic range acquisition of DCT and IDCT algorithms. 
                      	In <em>IEEE midwest sym. Circuits and Syst. </em> (MWCAS), 2005.
                        <br>            
                    <tr class="pub_tr_1">
                      <td width="20px" class="pub_td_number"> </td>
		      <td width="304px"> </td> 
                      <td class="pub_td_text"> <strong>Wanli Ouyang</strong>, C. Xiao, W. Ju, and D. Song. Practical fast asymmetric DCT algorithm based on SIMD and VLIW. 
                      	In <em>IEEE Int. Sym. Intelligent Signal Processing </em>, 2005.
                        <br>            
                        
                    <tr class="pub_tr_1">  
                      <td width="20%" align="left" colspan="2" style="HEIGHT:10px"></td></tr>
                    <tr class="pub_tr_1">  
                      <td width="20%" align="left" colspan="2"><a class="aLink" href="http://www.ee.cuhk.edu.hk/~wlouyang/#Top">Back To Top</a></td></tr>
                    </tbody></table>
</div>
                    
</td></tr></tbody></table>
<!--================================End of Body of Windows Body====================================-->
<!--================================Begin of Footer of Windows Body====================================-->
      <center>
      <table border="0" cellspacing="0" cellpadding="0" width="900px" style="BACKGROUND-IMAGE: url(footer_bg.jpg); COLOR: #2b547e">
        <tbody>
        <tr style="VERTICAL-ALIGN: middle">
<!--          <td style="TEXT-ALIGN: right; WIDTH: 200px; HEIGHT: 30px">
          <img alt="AVSX.org" src="avsx_logo_small.gif"></td> -->
          <td class="i_td_windows_footer">
          Last Update: Apr. 2013. Copyright © 2013-. 
          <a href="http://validator.w3.org/check?uri=http://www.ee.cuhk.edu.hk/~wlouyang/saved_resource.htm">
          <img style="BORDER-BOTTOM: 0px; BORDER-LEFT: 0px; WIDTH: 45px; HEIGHT: 15px; BORDER-TOP: 0px; BORDER-RIGHT: 0px" alt="Valid HTML 4.01 Transitional" src="valid-html401-blue.png"></a> 
          <a href="http://jigsaw.w3.org/css-validator/validator?uri=http://www.ee.cuhk.edu.hk/~wlouyang/saved_resource.htm">
          <img style="BORDER-BOTTOM: 0px; BORDER-LEFT: 0px; WIDTH: 45px; HEIGHT: 15px; BORDER-TOP: 0px; BORDER-RIGHT: 0px" alt="Valid CSS!" src="valid-css-blue.png"></a>
            </td>
        </tr>
        </tbody>
      </table>
      </center>
<!--================================End of Footer of Windows Body====================================-->
	  </center>
    </td>
  </tr>
  </tbody>
</table>



</body></html>